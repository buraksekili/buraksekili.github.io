<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Let's Build a CNI Plugin! From Linux Networking to CNI | Burak Sekili</title><meta name=keywords content="Networking,Linux,Linux Networking,Kubernetes,Go"><meta name=description content="From networking theory to Kubernetes networking. This blog covers networking fundamentals, Linux networking, and writing a CNI plugin and daemon in Go."><meta name=author content="Burak Sekili"><link rel=canonical href=https://buraksekili.github.io/articles/cni/><link crossorigin=anonymous href=/assets/css/stylesheet.402aea8fcb16ea8922c350ac4593eaa9cc13e6c01b2c0f5fc06b1c73f528c213.css integrity="sha256-QCrqj8sW6okiw1CsRZPqqcwT5sAbLA9fwGscc/UowhM=" rel="preload stylesheet" as=style><link rel=stylesheet href=css/extended/custom.css><link rel=icon href=https://buraksekili.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://buraksekili.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://buraksekili.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://buraksekili.github.io/apple-touch-icon.png><link rel=mask-icon href=https://buraksekili.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://buraksekili.github.io/articles/cni/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-M6ZJTX7HVG"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-M6ZJTX7HVG")}</script><meta property="og:title" content="Let's Build a CNI Plugin! From Linux Networking to CNI"><meta property="og:description" content="From networking theory to Kubernetes networking. This blog covers networking fundamentals, Linux networking, and writing a CNI plugin and daemon in Go."><meta property="og:type" content="article"><meta property="og:url" content="https://buraksekili.github.io/articles/cni/"><meta property="article:section" content="articles"><meta property="article:published_time" content="2025-11-09T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-09T21:38:34+03:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Let's Build a CNI Plugin! From Linux Networking to CNI"><meta name=twitter:description content="From networking theory to Kubernetes networking. This blog covers networking fundamentals, Linux networking, and writing a CNI plugin and daemon in Go."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Articles","item":"https://buraksekili.github.io/articles/"},{"@type":"ListItem","position":2,"name":"Let's Build a CNI Plugin! From Linux Networking to CNI","item":"https://buraksekili.github.io/articles/cni/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Let's Build a CNI Plugin! From Linux Networking to CNI","name":"Let\u0027s Build a CNI Plugin! From Linux Networking to CNI","description":"From networking theory to Kubernetes networking. This blog covers networking fundamentals, Linux networking, and writing a CNI plugin and daemon in Go.","keywords":["Networking","Linux","Linux Networking","Kubernetes","Go"],"articleBody":"This blog post is a collection of my personal notes on networking. For a long time, I had to jump between different notebooks to connect concepts; from core networking theory, to Linux internals, all the way up to Kubernetes and CNI. This post is my attempt to combine all those notes into a single, logical document.\nWe’ll follow a step-by-step path. We’ll start with fundamental network concepts, then see how those are implemented in Linux, which is the foundation for most modern virtual networking. Finally, we’ll see how Kubernetes builds on top of it all.\nAs a quick disclaimer, I’m writing this from the perspective of a software engineer, not a network engineer. This post is the guide I wish I’d had, and I’m sharing my learning journey in the hope that it’s as helpful to you as it has been to me.\nNetworking Feel free to skip this section if you are familiar with Switch vs Router comparison at least.\nLet’s start with fundamentals. Networking in general usually involves a lot of terminologies and acronyms, which makes it a bit challenging, at least for me, to understand when I need to check something. Therefore, understanding certain fundamental concepts might be helpful even in general tech literacy, even if you are not a networking engineer.\nA “host” is almost any device connected to a network, like your phone, computer, server, or printer. So, ‘host’ is a widely used term, and it does not only mean ‘server’; it can be any device connected to a network. But what is a network? It is the underlying communication fabric, built from physical links, hardware, firmware, and software. It uses protocols to provide communication, or in other words, packet-delivery services, between these hosts. As an example, if you have multiple devices connected together with patch cables (like a router and your laptop), it creates a local area network, a LAN - in most simple setups, unless advanced isolation techniques are used.\nNow you have a local network where multiple devices (laptops, phones, etc.) are connected to each other. The next question is, how do those hosts communicate? We need a way to say, “Okay ’laptop-xyz’, send this request to my printer ‘printer-abc’.” That’s where the MAC address comes into play. A MAC address is a unique hardware ID assigned to a machine, usually by its producer. For one host to send data to another on the same LAN, it must send that data to the receiver’s MAC address.\nThat’s good, but what happens if we change our laptop? The new laptop will have a new MAC address. If we only used MAC addresses, we would have to reconfigure everything to find the new one. As you can see, this would be a hassle and is an infeasible way to maintain a network, especially since devices are always joining and leaving.\nThis brings up a new problem. The MAC address solves the ‘where’ problem: where to send the packet on the local network. But it’s not good for the ‘who’ problem: who is the device I want to talk to, regardless of its specific hardware? This is why we have IP addresses (this is not the only reason of course). An IP address is a logical address assigned to a host. It’s the stable “who” we want to communicate with.\nAlso, please note that we don’t use IP addresses instead of MAC addresses; we use them together. Your application sends a packet to an IP address (the logical ‘who’). Then, your computer uses a special protocol to find the current MAC address (the physical ‘where’) associated with that IP. This two-part system gives flexible logical addressing (IPs) while still using the hardware MAC addresses to deliver the data on the local network to correct device.\nBut then, you may be wondering: how does a host find the MAC address for an IP address on its local network? This is done using the Address Resolution Protocol (ARP). For example, if host-a wants to send a packet to host-b (IP: 10.0.0.59), host-a initially has no idea what host-b’s MAC address is. It first broadcasts an ARP request to the entire network, basically asking, “Who has the IP address 10.0.0.59? Send me your MAC”. Every host on the network receives this broadcast, but only host-b recognizes its own IP and then sends an ARP reply directly back to host-a, saying, “I have 10.0.0.59, and here is my MAC address.” Once host-a learns host-b’s MAC address, it can send its packet destined to host-b’s MAC address.\nSwitch Assume that our LAN has evolved and grown. Connecting every device directly to every other one becomes unscalable. The solution is a switch. Each device is connected to switch through ports located on the switch. Switch knows where to find those hosts based on the host’s connected port on switch. For example, if host-a and host-b are connected to port-1 and port-2 ports respectively, if host-a wants to send a packet to host-b, the switch knows that host-b is behind its port-2.\nThe most important feature of switch is that it allows communication within a network, using MAC addresses. This is quite important as we’ll need to refer to this switch feature in a lot of different places.\nRouter The switch handles traffic within our network, but what happens when a device wants to communicate with a device on a different network, like a server on the internet? The switch, looking at the destination MAC address, will see that it’s not a local device and will send the data to the network’s exit point, the router.\nRouters allow communication between networks. This is one of the crucial differences between switch and router. Inter-network traffic is usually handled by switch. But once the packet needs to reach wide area network (WAN), or in other words another network on the Internet, we need a router.\nRouter knows which networks that they can route by maintaining a routing table, allowing them to know where the corresponding IP address may be found in the WAN. When your router receives a data packet destined for Google, it doesn’t know Google’s MAC address but IP address. It consults its routing table and forwards the packet to the next router on the path, which repeats the process. This forwarding across many different networks is what allows you to reach any destination on the internet.\nSo, the key takeaways are that switching uses MAC addresses to move data within a single network, while routing uses IP addresses to move data between different networks.\nOSI (L2, L3 and L4) We are not going to dive into details of OSI layers; but it’s usually good to be familiar with certain mechanisms and terminologies related to OSI layers in general, as it helps us to understand more complex systems like container networking and Kubernetes.\nLayer 2 Layer 2, the Data Link Layer, interacts between the Layer 3 (Network) and Layer 1 (Physical) layers. Its main responsibility is to take Layer 3 packets (like IP packets) and encapsulate them into frames for transmission over a physical medium. Also, it receives incoming signals from L1, reassembles them into frames, de-encapsulates them and passes the payload (the original L3 packet) up the stack.\nL2 uses a special addressing scheme called MAC to manage the delivery. By using MAC, L2 identifies the receiver host, and sends the data to the correct host.\nYour data travels across different hosts, for instance host to switch (which is a host) then another host to router, etc etc until it reaches its destination. This process is usually done by devices like network cards (NICs), or switch. As we saw previously, switch allows communication within a network. If devices are connected by a switch, the switch forwards packets between devices, to ensure host to host communication in a network.\nThe key takeaway for L2 is that it provides a mechanism for host-to-host (or host-to-router) delivery, which is the ability to move a frame from one host to another within the same broadcast domain (e.g., host-to-host on the same LAN, or host-to-router). And switch is mainly a L2 device; though you can find L3 switches on the market as well :).\nLayer 3 Sending data between connected hosts is not our usual Internet experience. We need to send our packets to other networks to access services on those networks. L2 delivery only ensures host-to-host delivery which happens in a single network. However, usual packets need to go beyond that, across networks involving multiple host-to-host deliveries. This is where L3 comes into picture.\nL3 ensures routing of packets and end to end delivery. It uses its own addressing scheme called IP addresses which, unlike physical MAC addresses, are not tied to specific hardware. A router, the key L3 device, makes this work. When a router receives a packet, it reads the destination IP address. It then consults its routing table (a set of rules) to decide where to forward that packet next.\nThe key takeaway is that L3 provides the end-to-end logic for packet delivery across multiple networks. It does this by making a series of hop-by-hop routing decisions using IP addresses.\nIn the following example, we have two networks connected to each other via Router:\nNetwork 1 (10.0.0.0/24) Network 2 (71.0.0.0/24) ┌──────────────────────┐ ┌───────────────────────┐ │ │ IF2: │ │ │ │ MAC: DD │ │ │ │ IP: 71.0.0.1 │ │ │ HOST A │ Router │ HOST B │ │ ┌─────────────┐ │ xxxxxxxxx │ ┌─────────────┐ │ │ │MAC: AA │ │ x x │ │MAC: BB │ │ │ │ ┼───│──\u003e x x────────\u003e │ │ │ │ │ │IP: 10.0.0.50│ │ x x │ │IP: 50.0.2.19│ │ │ └─────────────┘ │ xxxxxxxxx │ └─────────────┘ │ │ │ IF1: │ │ │ │ MAC: CC │ │ │ │ IP: 10.0.0.1 │ │ └──────────────────────┘ └───────────────────────┘ Host A (10.0.0.50) wants to send a packet to Host B (71.0.0.59). Host A checks its own local routing table. It compares the destination 71.0.0.59 to its own network (subnet, 10.0.0.0/24) and determines the destination is not its own network. So, it sends the packet to router which is usually defined as default gateway. In order to create the L2 frame, Host A needs the MAC address associated with 10.0.0.1. It uses ARP (Address Resolution Protocol), broadcasting a request: “Who has 10.0.0.1? The router’s IF1 interface replies: “I have 10.0.0.1, and my MAC is CC.” Host A now builds the packet and encapsulates it in an L2 frame to send to the router:\n# Frame 1: Host A -\u003e Router (on Network 1) destination MAC: CC source MAC: AA --- (L3 Header) --- destination IP: 71.0.0.59 source IP: 10.0.0.50 --- (Payload) --- The router receives this frame on IF1 as the destination MAC matches, strips the L2 header and inspects the L3 header. Since the destination IP is 71.0.0.59, the router sends this packet to IF2 by creating again L2 header with the similar process (ARP request and then updating L2 header accordingly).\nThe router re-encapsulates the original, unchanged L3 packet into a new L2 frame, using its IF2 MAC (DD) as the source.\n# Frame 2: Router -\u003e Host B (on Network 2) destination MAC: BB source MAC: DD --- (L3 Header) --- destination IP: 71.0.0.59 source IP: 10.0.0.50 --- (Payload) --- Finally, Host B receives this frame, strips the L2 header, and inspects the L3 header. It sees its own IP as the destination and accepts the packet, passing the payload up the stack.\nAgain, the key takeaway from this example is that the L3 header (IPs) remained constant for the entire end-to-end delivery, while the L2 header (MACs) was rebuilt at each hop.\nLayer 4 We have delivered the packet to the correct host (HB), but when a packet reaches to its destination, that host might be running manny different programs, like a web server, an email server, etc. So, how do we know which program the packet is for? This is the job of Layer 4 (L4), the Transport Layer. This L4 header specifies a port number. For example, if HA wants to reach HB’s web server, it will set the destination port in the L4 header to 80. When HB receives the L3 packet and unwraps it, it looks at this L4 header, sees “port 80,” and knows to deliver the data to its web server, not its email server. This is how a single IP address can serve many different services at the same time.\nSwitching (L2 Domain) We’ve covered the basics of a switch, but how does it actually manage traffic magically? This section explains the fundamental operations a switch performs.\nAs mentioned earlier, switches have multiple ports where devices (like PCs and servers) connect. To know where to send traffic, a switch builds and maintains a MAC address table. This table is crucial: it maps the MAC address of a connected device to the specific switch port it’s on. This table is what allows the switch to intelligently deliver frames to the correct destination.\nWhen a frame arrives on any port, the switch follows a simple but powerful three-step logic: Learn, Flood, and Forward.\nA Quick Note that the “Learn, Flood, and Forward” model is the most fundamental concept of L2 switching. Please be aware that this overview explains only the basics. Real-world switches perform several other important functions, such as filtering frames (dropping a frame if the destination is on the same port it came from) and aging out (removing) old entries from the MAC table to keep it up-to-date.\nLearn Switch maintains its MAC table with the port and updates it whenever a related frame passes through the switch. For example, when Host A sends a frame to Host B, the initial MAC table state is empty. When the switch receives a traffic from Host A’s MAC address (let’s say AA) through port-a, switch learns and updates its MAC table to memorize port: port-a and MAC: AA.\nFlood Now, switch still does not know where to find Host B (which port should it use?). Therefore, the switch floods a unicast frame out all switch ports except the port it was received on. This is called flooding. Once Host B receives the frame, it responds to the message through a port that it’s connected to, let’s say port-b. Now, switch receives this acknowledged response through port-b, and learns that port: port-b and MAC: BB. Now, switch knows where to find Host B\nFORWARD The rest is a bit trivial actually. The switch knows ports and MAC addresses. It performs forwarding to deliver the frame. The Forward operation is the main job of the switch once its MAC table has information.\nAfter the switch “learns” that Host B is on port-b, the “Flood” process is no longer needed to reach it. The next time Host A sends a frame to Host B, the switch receives it, looks at the Destination MAC address (BB), and checks its MAC table. It finds the entry for it port: port-b and MAC: BB and forwards the frame only out port-b. It drastically reduces network traffic and is the primary reason switches are much more efficient than old hubs.\nRouting (L3 Domain) Routing is the process of delivering data between different networks (inter-network communication) using a device called a router. This operation occurs at Layer 3 (L3) of the OSI model. If a host on one network (like your home LAN) needs to communicate with a host on a different network (like the internet), it requires a router to forward the packet.\nRouting Table Routers maintain a map of all the networks they know about, which is known as routing table. This table is essentially a set of rules, or routes, that tell the router which path to use to reach a specific network destination.\nExample routing table contents, from https://en.wikipedia.org/wiki/Routing_table\nNetwork destination Netmask Gateway Interface Metric 0.0.0.0 0.0.0.0 192.168.0.1 192.168.0.100 10 127.0.0.0 255.0.0.0 127.0.0.1 127.0.0.1 1 192.168.0.0 255.255.255.0 192.168.0.100 192.168.0.100 10 192.168.0.100 255.255.255.255 127.0.0.1 127.0.0.1 10 192.168.0.1 255.255.255.255 192.168.0.100 192.168.0.100 10 You might wonder how this table is created. Routes are typically added in three ways:\nDirect: the router automatically adds routes for networks it is physically connected to (e.g., the 192.168.0.0 entry). Static: a fixed route is manually added. Dynamic: routers peers each other (talks to each other) using routing protocols (like OSPF or BGP) to automatically learn about and share routes. This is essential in large, complex networks. If you have checked Kubernetes networking related documents, you may realize that routing protocols, especially BGP, are usually mentioned as an option in the network fabric. BGP (Border Gateway Protocol) allows different networks to exchange routing information automatically. While it’s an advanced topic, the key idea is automated route discovery, and we will see BGP again when we discuss advanced networking patterns in Kubernetes. Also, if you are using managed services from cloud providers, as an end user, most of the time you do not need to work closely with BGP. This is because most cloud providers use an abstraction called “overlay networks” which we will also briefly see in the Kubernetes networking section. These overlays run on top of the provider’s complex underlying network, which itself may use BGP heavily, but that complexity is hidden from you.\nA key entry in many routing tables is the default gateway (shown as 0.0.0.0 in the example). This is the “catch-all” route. If the router doesn’t have a specific route for a destination, it sends the packet to the default gateway if its defined. So, the default gateway is optional. So, based on the routing table, the kernel forwards data to the route.\nNAT NAT is a process where a router modifies the source and/or destination IP addresses in a packet’s header on the fly as it passes through.\nPublicly routable IP addresses (like the ones you get from your ISP) are a limited (due to the nature of unique IPv4 addresses) and costly resource. In a typical home or office, your devices (laptops, phones) use private IP addresses (e.g., 192.168.x.x, 10.x.x.x). These addresses are not routable on the public internet. Only your main router has a single public IP address.\nThen you may ask, how can your private device access the internet? And just as importantly, if a web server sends a response, how does it get back to your specific private device, which it can’t see?\nThis is solved by two main types of NAT:\nSNAT (source NAT) SNAT is the process where source IP is changed. This is important because addresses in LAN are not reachable from WAN. Thus, even though they can send requests to internet, internet can’t respond to those hosts. So, SNAT is being used for outbound connections, like when your laptop browses a website.\nThe process begins when your laptop (private host, e.g., 192.168.0.100) sends a packet to a web server (e.g., 8.8.8.8). This packet, with its private source IP, hits your router. The router then performs SNAT by changing the packet’s Source IP from 192.168.0.100 to its own public IP (e.g., 123.45.67.89). Also, it records this translation in a state table. The web server receives the packet from the public IP and sends its response back to that same public address. When your router receives this response, it checks its state table, sees the traffic belongs to 192.168.0.100, and translates the Destination IP back to your private host before forwarding it.\nDNAT (destination NAT) This is used for inbound connections, often called port forwarding. Imagine you are hosting a web server on your private network at 192.168.0.200. Since someone on the internet can’t reach this private address, you configure a DNAT rule on your router. This rule tells the router, “Any traffic that arrives at my public IP (123.45.67.89) on a specific port (like port 80 for HTTP) should have its Destination IP changed to 192.168.0.200.” This rule forwards the external request to your internal, private server, allowing it to host services for the public internet.\nThis entire stateful process requires the router to remember which internal host initiated which external connection. This mechanism is called connection tracking (or conntrack in Linux).\nThe conntrack system maintains an in-memory table of all active connections. For SNAT, this table maps the original [private_ip:port] to the [public_ip:port] it was translated to. Because this table is stored in memory, it has a limited size. This limit (e.g., nf_conntrack_max in Linux) can be reached if you have too many simultaneous active connections. It can also be a problem with a high rate of short-lived connections, as entries are kept in the table for a short time even after the connection closes. If this table fills up, the router will start dropping new connections.\nLinux Networking (Single Host) This section covers the common Linux networking “toolbox” required to understand before jumping into Kubernetes networking. We’ll use standard Linux tools to build a complete, virtualized network inside a single host. The concepts we explore network namespaces, veth and virtual bridges that are the building blocks used by container runtimes and CNI. A grasp of these operations and concepts is key to understanding how container networking operates at scale.\nIf you are already comfortable with container networking, feel free to skip ahead.\nIsolation (network namespaces) As explained in Layer 2 section, a host uses network interfaces (NICs) for network communication. These interfaces can be physical (like the actual hardware on your laptop) or virtual, which we will discuss soon.\nWhen you start your Linux machine, the kernel runs a single, default networking stack for you to manage all networking processes. This “root” stack is for everything running on the Linux machine. If you need to isolate this stack, you should use network namespaces.\nLike other Linux namespaces, network namespaces isolate the networking stack. This allows us to spin up multiple containers on the same host while managing their networking stack differently, giving each one its own isolated network devices, firewall rules, ports, and route tables.\nThis isolation is very strong. When a network namespace is created, it comes with only a single, private loopback device (lo). It has no other interfaces, no routes, and no way to communicate with the host or the outside world. The rest of the configuration must be set by hand. To make this isolated “box” useful, we must provide it with a network connection. The most common method, and the one that powers container networking, is to use a virtual ethernet (veth) pair, which acts like a virtual patch cable.\nThe main tool we will use throughout this section is the ip command. The following is a quick demonstration of ip tool in Linux to manage network namespaces\nFor a more detailed reference, the ip command suite is vast. RedHat provides a useful cheatsheet: https://access.redhat.com/sites/default/files/attachments/rh_ip_command_cheatsheet_1214_jcs_print.pdf.\n1 2 3 4 # adding a network namespace ip netns add # executing a command in this namespace. ip netns exec $COMMAND Let’s run a quick demo to see this isolation in action. We’ll create a namespace called “container0” and run a shell inside it:\n1 2 3 4 5 6 7 8 9 10 11 ip netns add container0 ip netns list # container0 (id: 0) ip netns exec container0 /bin/bash ip netns identify # this should print container0 exit # exit from container0 namespace ip netns identify # this should print nothing as we are in the root namespace ip netns del container0 Network Device abstraction (ip link) If you have come across ip commands before (or check the cheat sheet above), you may realize that ip link is widely used. This utility provides a great configuration layer for our isolated environment, by providing network device configuration.\nIn the last section, we used the ip netns command to manage the namespace itself. Now, we’ll use ip link to manage the network devices (or interfaces) inside the namespace.\nYou’ll often hear these called network interfaces, link devices, or just links. These terms all refer to the same basic concept of a kernel object that can send or receive packets. They can have technical differences, but usually they refer to the similar functionality; sending and receiving data.\n1 2 3 $ ip netns add container0 \u0026\u0026 ip netns add container1 \u0026\u0026 ip netns container0 container1 Now, let’s use ip link show to see what network devices exist inside container0:\n1 2 3 $ ip netns exec container0 ip link show 1: lo: mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 As you can see, the namespace isn’t empty; it starts with a single device: lo, the loopback interface. This virtual device allows a host (or in this case, a namespace) to send network packets to itself.\nIf you examine this output, you’ll see the loopback device’s current state is DOWN. This means the device is not “up” or ready to function. But, what is the meaning of all of this? Why would you even care? Let’s try something simple, like pinging the loopback IP address 127.0.0.1:\n1 2 $ ip netns exec container0 ping 127.0.0.1 ping: connect: Network is unreachable It fails. We get “Network is unreachable” because even though the lo device exists, it’s turned off, and the kernel can’t use it. Let’s fix this by using the ip link set command to bring the device “up”:\n1 2 3 4 5 6 7 8 $ ip netns exec container0 ip link set lo up $ ip netns exec container0 ping -c 1 127.0.0.1 PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data. 64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.082 ms --- 127.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.082/0.082/0.082/0.000 ms It works. We have just configured our first network device. This lo interface is essential, but it only lets the namespace talk to itself. To talk to other namespaces or the outside world, we need to add new devices.\nThe cable (veth pairs) Virtual Ethernet, or veth, does what we are looking for: it connects our namespace to another namespace.\nIn our above example, let’s assume we want to allow communication between container0 and container1. What can we do? Remember the concepts mentioned in Networking section. The simplest way to connect hosts was connecting them together with patch cable. However, we now operate in the virtualized world. How could we make it? In the virtualized world, veth achieves this for us.\nA virtual ethernet device, or veth for short, is a link device that is created as a pair, just like a patch cable. When a packet is transmitted on one end of veth, it is received from the other end. Thanks to this feature, we use a veth pair to connect container0 and container1 directly.\n+------------------------------------------------+ | | | container0 container1 | | +--------------+ +--------------+ | | | | | | | | | | | | | | | | | | | | +------+-------+ +------+-------+ | | ^ ^ | | | | | | | veth | | | +-----------------------+ | | | +------------------------------------------------+ The above diagram is what we want to achieve, so that we can ping container0 to container1.\n1 ip link add veth0 type veth peer name veth1 The command may look ugly, but if we read it as:\nIt makes it a bit clearer to understand. It means we want to add a link device called veth0, whose type is veth. Since it’s going to be a veth, we need to give a name to its peer, or in other words, its other end.\nWhere should we run this command? Within the container0 or container1 namespaces, or on the host? Well, we usually create the veth pair in the parent namespace (the host, in our case) and then move each end into its corresponding namespace.\nNow, let’s list what link devices we have in the host network namespace:\n1 2 3 4 5 6 7 8 9 $ ip netns identify $ ip link show 1: lo: mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 49: veth1@veth0: mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether ea:de:62:55:a0:7a brd ff:ff:ff:ff:ff:ff 50: veth0@veth1: mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 56:10:cf:46:97:1c brd ff:ff:ff:ff:ff:ff As you can see, the veth pair is there, and both devices are state DOWN Now, we’ll move one end into container0 and the other into container1:\n1 2 3 4 5 6 7 8 9 10 $ ip link set veth0 netns container0 \u0026\u0026 ip link set veth1 netns container1 $ ip netns exec container0 ip addr show 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 50: veth0@if49: mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 56:10:cf:46:97:1c brd ff:ff:ff:ff:ff:ff link-netns container1 The IPs for network devices are visible in ip addr show output. If you check inet field, its the IPv4 address, and inet6 corresponds to IPv6 address.\nOkay, now both containers are connected. Perfect, can we now ping the container1 from container0? Well, which address should we use? There is no IP address for the containers at the moment. What should get an IP address; namespace, the veth or something else? How can we assign an IP address?\nWell, based on our discussion in the Networking section, we assumed hosts (devices) have IP addresses. So, we assign IP to network interfaces to allow communication through IP. In our case, the “device” is the veth pair. Since veth is a link device (a network interface), we can assign an IP to it and ping that IP. Assigning IPs can be done via the ip command.\n1 2 3 $ ip netns exec container0 ip addr add 10.0.1.50/24 dev veth0 $ ip netns exec container1 ip addr add 10.0.1.59/24 dev veth1 $ ip netns exec container0 ip link set dev veth0 up \u0026\u0026 ip netns exec container1 ip link set dev veth1 up This command assigns 10.0.1.50/24 to veth0 and 10.0.1.59/24 to veth1. Since both of these IPs are in the same 10.0.1.0/24 subnet, the kernel will know they can reach each other directly over the veth “cable” through L2 network, without routing.\nBy calling ip link set dev veth0 up, we are setting the link (device, network interface) up, or ready to function.\nLet’s check the IP addresses:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ ip netns exec container0 ip addr show veth0 50: veth0@if49: mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 56:10:cf:46:97:1c brd ff:ff:ff:ff:ff:ff link-netns container1 inet 10.0.1.50/24 scope global veth0 valid_lft forever preferred_lft forever inet6 fe80::5410:cfff:fe46:971c/64 scope link valid_lft forever preferred_lft forever $ ip netns exec container1 ip addr show veth1 49: veth1@if50: mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether ea:de:62:55:a0:7a brd ff:ff:ff:ff:ff:ff link-netns container0 inet 10.0.1.59/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::e8de:62ff:fe55:a07a/64 scope link valid_lft forever preferred_lft forever Based on inet output, we can see that veth devices have IP addresses as we defined above.\nNow, we should test if container0 can reach container1 by simply pinging IP addresses within the container network namespaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ ip netns exec container0 ping -c 1 10.0.1.59 # pinging container1 within container0 PING 10.0.1.59 (10.0.1.59) 56(84) bytes of data. 64 bytes from 10.0.1.59: icmp_seq=1 ttl=64 time=0.031 ms --- 10.0.1.59 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.031/0.031/0.031/0.000 ms $ ip netns exec container1 ping -c 1 10.0.1.50 # pinging container0 within container1 PING 10.0.1.50 (10.0.1.50) 56(84) bytes of data. 64 bytes from 10.0.1.50: icmp_seq=1 ttl=64 time=0.300 ms --- 10.0.1.50 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.300/0.300/0.300/0.000 ms It works. We have successfully created our first virtual network by connecting two isolated namespaces with a veth pair.\nThe Virtual Switch (bridge) Having two containers and managing a direct connection between them is not a big deal. We just need to perform a couple of commands, and the two isolated network namespaces are ready to communicate.\nHowever, think about scaling this system. If you have many containers, connecting them to each other directly will not scale. Does this issue sound familiar? Remember one of the benefits of switches. In the physical world, switches solve this exact problem. Instead of connecting hosts directly to each other, we connect all hosts to a central switch, which then handles communication between them.\nA similar solution exists in the virtualized environment, and in Linux, it’s called a bridge.\nA Linux bridge is a Layer 2 device that behaves exactly like a virtual switch. It builds a MAC address table and forwards frames between the devices connected to it. Because it’s L2, it does not know about IP addresses or routes.\nFrom the host’s point of view, the bridge is just another network device (a link). From the containers’ point of view, it’s the “switch” they are all plugged into.\nOur old setup was a direct veth “patch cable” between two containers: Our new setup will look like this, with a central br0 bridge: In this setup, introducing a new container or removing a new container (network namespaces) become quite easy. We just need to perform operations that we explained above. But instead of connecting veth pairs to host network, we connect one end of veth pair to bridge, and the another to container itself.\nTo set this up, we’ll create a br0 bridge device in the host namespace. Then, for each container, we’ll create a veth pair. One end will go inside the container namespace (and get the IP address), while the other end will be “plugged into” the bridge as a port.\nLet’s build this. First, we need to create the bridge device. We’ll use the ip command again.\n1 2 3 # create a device of type 'bridge', named 'br0' ip link add br0 type bridge ip link set br0 up That’s it. We now have a virtual switch that is on, but has nothing plugged into it. Now, let’s clean up our old setup and connect our containers to this new bridge.\nMaybe you may want to try it yourself, by following the commands in the previous section.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # clean up the old veth pair # note that deleting one end also deletes the peer ip netns exec container0 ip link del veth0 # create new veth pairs for container0 and container1 # (c0-veth, c0-br) -\u003e for container0 # (c1-veth, c1-br) -\u003e for container1 ip link add c0-veth type veth peer name c0-br ip link add c1-veth type veth peer name c1-br # move the 'veth' end into the namespaces ip link set c0-veth netns container0 ip link set c1-veth netns container1 # attach the 'br' veth end to the bridge # this is like plugging the cable into the switch ip link set c0-br master br0 ip link set c1-br master br0 # add IP addresses to veth pairs inside the containers ip netns exec container0 ip addr add 10.0.1.50/24 dev c0-veth ip netns exec container1 ip addr add 10.0.1.59/24 dev c1-veth ip netns exec container0 ip link set dev c0-veth up ip netns exec container1 ip link set dev c1-veth up # also do not forget to set 'br' veth pair ends UP ip link set c0-br up ip link set c1-br up Now the setup is complete. We expect the similar behaviour, where containers can ping each other.\n1 2 3 4 5 6 7 $ ip netns exec container1 ping -c 1 10.0.1.50 PING 10.0.1.50 (10.0.1.50) 56(84) bytes of data. 64 bytes from 10.0.1.50: icmp_seq=1 ttl=64 time=0.092 ms --- 10.0.1.50 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.092/0.092/0.092/0.000 ms It works! You may wonder the purpose of all hassle by setting up the bridge. Since bridges work like virtual switches, adding a new container to this setup becomes easier than before. We just need to add veth pairs, connect one end of the pair to container and another end to the bridge, assign IP address and set everything UP. Now the new container also can communicate with other containers, so we do not need to update every veth pair in other containers.\nWe have now built a scalable, single-host virtual network, which is the foundation of how Kubernetes and CNI work.\nL3 Gateway Our containers can communicate with each other. That’s perfect for offline environments. But most of the time, we also expect containers to reach other networks, especially the Internet.\nLet’s check whether the containers can communicate with the Internet.\n1 2 3 4 $ ip netns exec container0 ping -c 1 8.8.8.8 ping: connect: Network is unreachable $ ip netns exec container1 ping -c 1 8.8.8.8 ping: connect: Network is unreachable Even though containers can ping each other, they cannot communicate with the Internet.\nRemember, in the previous sections, we explained how packet delivery happens: L2 ensures host-to-host (or host-to-router) delivery, while L3 ensures end-to-end delivery. What are we missing here?\nThe simple answer is that the container network namespaces do not know any path to reach 8.8.8.8. Therefore, they cannot deliver the ping request (ICMP packets). We need to add a ‘route’ to instruct our system to find the path. In our host machine, we are able to ping the Internet. It seems the host namespace contains a route allowing it to access the internet, and this route does not exist in the containers.\nIn our host machine, we are able to ping the Internet. It seems the host namespace contains a route allowing it to access the internet, and this route does not exist in the containers.\nTo find out the reasons, we should look at how we reach the Internet from the host:\n1 2 3 4 $ ip netns identify \u0026\u0026 ip route get 8.8.8.8 8.8.8.8 via 192.168.215.1 dev eth0 src 192.168.215.2 uid 0 cache ip route get command allows us to check which route the host takes while reaching . In our case, we reach 8.8.8.8 via the eth0 device. If you list all routes, you’ll see why:\n1 2 3 $ ip route default via 192.168.215.1 dev eth0 192.168.215.0/24 dev eth0 proto kernel scope link src 192.168.215.2 As we mentioned in Routing Table section, the first entry is the default gateway, which is used if no other rule is matched. By using this rule, we are able to ping the Internet. We need to add a similar route inside the container namespaces.\nLet’s first check the full routing table inside container0:\n1 2 3 4 $ ip route 10.0.1.0/24 dev c0-veth proto kernel scope link src 10.0.1.50 $ ip route get 8.8.8.8 RTNETLINK answers: Network is unreachable This confirms our suspicion. The only rule it knows is the “direct” route for its local subnet. When we ask it to find 8.8.8.8, it doesn’t match this rule, and there’s no “default” to fall back on.\nSo, how do we give the containers a default route? We need to give them a gateway (a router) that they can send all non-local traffic to. The perfect candidate for this gateway is our own br0 bridge.\nYou might ask, why the bridge? In our last example, br0 was just a simple L2 switch and didn’t have an IP at all. This is the key difference: to act as an L3 gateway, a device should have an IP address. A fundamental rule of IP networking is that a host’s gateway must be on the same local subnet. This is so the container (e.g., 10.0.1.50) can use ARP to find the gateway’s MAC address. Therefore, we will turn our L2 switch into an L3 gateway by assigning it an IP address on the containers’ subnet, 10.0.1.0/24.\nIf the bridge were only acting as a simple L2 switch for a network that didn’t need to talk to the outside world, it wouldn’t need an IP.\nWe’ll use 10.0.1.1 as the gateway IP. This will be the “internal” IP of our router, reachable by both containers.\nIt is not mandatory but convention to give a.b.c.1 IP addresses to the Gateway.\n1 2 3 $ ip addr add 10.0.1.1/24 dev br0 $ ip addr show br0 | grep -e \"inet \" inet 10.0.1.1/24 scope global br0 Now, lets also instruct container network stack to route any non-local packets to bridge IP.\n1 2 3 4 5 6 7 $ ip netns exec container0 ip route add default via 10.0.1.1 $ ip netns exec container1 ip route add default via 10.0.1.1 # let's check the route table for container0 again $ ip netns exec container0 ip route default via 10.0.1.1 dev c0-veth 10.0.1.0/24 dev c0-veth proto kernel scope link src 10.0.1.50 Now container know that any traffic not destined for 10.0.1.0/24 should be sent to the gateway at 10.0.1.1. Let’s test the connection:\n1 2 3 4 5 $ ip netns exec container0 ping -c 1 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. --- 8.8.8.8 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms It still fails! Why?\nThis may be confusing, since our containers can talk to each other. That’s because the traffic between namespaces is a Layer 2 (switching) operation. The br0 bridge acts like a physical switch, and the packets never leave the bridge to be routed. The host’s main L3 (IP) routing engine isn’t involved. If you remember the switching operations (learn, flood, forward) in Switching section, you may remember that Switch actually knows which MAC addresses are connected to which ports. This operation only applies to within network traffic as we explained in the first section.\nHowever, ping 8.8.8.8 is a Layer 3 (routing) operation. The container sends the packet to its gateway (the br0 interface), meaning that we need to reach another network. As you remember, this requires routing as we need to move data between networks. The host’s kernel receives this packet on br0 and, after checking its own route table, sees it must send it out a different interface (like eth0).\nThis act of receiving a packet on one interface and forwarding it to another is called L3 forwarding. By default, the Linux kernel disables this for security, so a machine doesn’t accidentally act as a router. We must explicitly enable this by setting net.ipv4.ip_forward=1.\nThat’s the first problem. The second problem is NAT. The packet from container0 has a source IP of 10.0.1.50. The internet doesn’t know how to send a reply to this private IP.\nWe must use iptables to “masquerade” the packet, rewriting its source IP to the host’s public IP (192.168.215.2).\nOkay, you might think, what is iptables since we never mentioned it yet.\nIn simple words, iptables is a program that allows us to set rules for packets. You can create iptables rules to filter, modify, or redirect packets. It’s a powerful tool for building firewalls, and it’s also commonly used to implement NAT because its rules allow “mangling” (updating) the packet itself.\niptables is a complex tool, and while it’s now being replaced by modern alternatives, its concepts are still fundamental to Kubernetes networking.\nSo, let’s update our host machine to enable IP forwarding:\n1 sysctl -w net.ipv4.ip_forward=1 And, we need to add an iptables NAT rule. This command is a bit long, but what it means is that “For any packet in the nat table, in the POSTROUTING (after-routing) chain, if it’s from our container subnet (-s 10.0.1.0/24) and going out the -o eth0 interface, then -j MASQUERADE (change its source IP to eth0’s IP).”\n1 iptables -t nat -A POSTROUTING -s 10.0.1.0/24 -o eth0 -j MASQUERADE Now, all the pieces are in place. Let’s try one last time:\n1 2 3 4 5 6 7 $ ip netns exec container0 ping -c 1 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=106 time=29.4 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 29.362/29.362/29.362/0.000 ms Finally it works! We have successfully connected our isolated containers to the internet.\nKubernetes Networking \u0026 CNI It’s been a long journey but here we are, Kubernetes networking. We are not going to dive into too much details about technical requirements of the Kubernetes networking. Kubernetes’ official documentation clearly explains the problems that we need to address, as follows:\nHighly-coupled container-to-container communications: this is solved by Pods and localhost communications. Pod-to-Pod communications: this is the primary focus of this document. Pod-to-Service communications: this is covered by Services. External-to-Service communications: this is also covered by Services.” https://kubernetes.io/docs/concepts/cluster-administration/networking/\nMotivation and Automation of Linux networking In the last section, we built a single-host virtual network. We manually created namespaces, veth pairs, a bridge, and configured routing. The goal of Kubernetes networking is to automate this entire process at a massive, multi-node scale.\nOne of the core principle of Kubernetes networking is a unique IP per pod model. To understand why this is so important, imagine scaling a web application. You need three replicas, and they all want to listen on port 80. On a single machine, this is impossible, as only one process can bind to a port at a time. This creates a problem of coordinating ports for all your microservices.\nKubernetes solves this by giving every Pod its own isolated network namespace and a unique IP address. Because each Pod has its own network stack (through a dedicated network namespace), there are no port conflicts. Your three replicas can all bind to port 80, each on its own IP (e.g., 10.244.1.10:80, 10.244.2.20:80, 10.244.3.30:80). This simplifies application development and management, as you no longer need to manage port assignments.\nNote: A Pod can also use hostNetwork: true, which skips this process, doesn’t get its own IP, and just shares the node’s network, but this is for special workloads.\nTo make this model work, Kubernetes requires an L3 network. This means all Pod IPs must be reachable from all other Pod IPs, no matter which node they are on. Therefore, this is quite important and technical aspect that we didn’t cover it yet.\nThe first problem Kubernetes networking solves is connectivity: “How do we give a Pod its own network namespace and a unique IP, and then make it reachable by other Pods?”.\nAnother problem is discovery of Pods. Pods are ephemeral; they can be destroyed and replaced at any time, getting a new IP. This means you can’t rely on a Pod’s IP address. This is solved by a Kubernetes Service. A Service provides a single, stable virtual IP and a DNS name (e.g., my-service.prod.svc.cluster.local). There is an agent running on each node to manage host level networking and watches the Kubernetes API and updates rules on the host (using iptables, IPVS, or nftables) to map the Service IP to the real Pod IPs. Other tools like Cilium can also do this as kube-proxy replacement, by using eBPF.\nWe also have more problems, for instance reachability, which will be discussed in the next chapter\nCNI (Container Networking Interface) CNI is a specification that tells the container runtime to call a “plugin” (or program) to handle the network setup for containers.\nIt solves the problem of “giving a Pod its own network namespace and a unique IP, and then making it reachable by the cluster”. Therefore, we can say that CNI helps to create an isolated network for pod, to allocate the Pod IP, and make the IP reachable by the cluster.\nOur demo in Part 2 where we manually created veth pairs, a bridge, and iptables rules is a good illustration of how one of the most common CNI plugins (the bridge plugin) works under the hood. Other CNI plugins might solve this problem in completely different ways, such as using eBPF, IPVLAN, or different routing techniques.\nWe should indicate that CNI is not only for Kubernetes, it’s a generic specification for container networking. Kubernetes in this scenario is just an orchestrator or runtime, which triggers CNI plugins, to set up a network environment for the Pod.\nCNI Plugin According to CNI specification, “plugin is a program that applies a specified network configuration”. Roughly, the container runtime calls CNI plugin, meaning CNI program, to prepare the container networking. So, if we move the codes that we use in the demo into a bash script and make it executable, it can be good starting point for us to write our own first “plugin”.\nThough i still do not understand the motivation behind ’executable’ plugins, instead of RPC based plugins like other Kubernetes out-tree interfaces (CRI or CSI).\nThe good part about CNI specification is that it allows chaining the plugins. Meaning that you can develop and use existing CNI plugins, and combine them to come up with your own container networking solutions. For example, you can use “ipam” plugin along with “bridge” plugin, so that “bridge” plugin can create the demo environment that we set and “ipam” plugin can manage the IP address assignment to those resources.\nSince Kubernetes expects more than Pod connectivity, CNI Plugins are usually shipped with additional component(s).\nThe binary is usually the one that we call the plugin. So, its main purpose is configuring the pod network interface, which is for “connectivity”. Daemon is managing the routing. So, its mainly for “reachability” There are various ways to ensure “connectivity” and “reachability”, and we saw one possible solution for “connectivity”, in our bridge example.\n“Reachability” though require more work. The CNI specification itself does not involve reachability; it covers the container networking (connectivity) and some runtime (e.g, Kubernetes) rules while calling CNI plugins. Therefore, reachability is a bit specific to Kubernetes networking requirements.\nThe purpose of the reachability is ensuring every Pod is accessible from every node. So, it’s a bit related to route lifecycle; thus, we should somehow announce the routes to each pod to each node. Therefore, Kubernetes CNI solutions ship with the ‘daemon’ running on each node to ensure that the network is configured in a way that all Pods are accessible across nodes.\nHow could we make it possible? As we mentioned in previous section, if nodes are in the same L2 network, we could utilize Switch + Routers. But the common ways to solve this are ‘Overlay Networks’ (e.g, VXLAN or IP-in-IP) or ‘Routing Protocols’ (BGP or OSPF).\nThere might be other hacky solutions as well; but these are the most common ones I faced.\nHow CNI Plugins Called The CNI (Container Network Interface) specification defines how a container runtime, like Kubernetes, interacts with network plugins. This guide covers the fundamental operations and concepts you need to know. If you are looking for more details, please take a look at the actual specification (https://github.com/containernetworking/cni/blob/main/SPEC.md).\nA runtime calls a CNI plugin by providing two key inputs: a JSON network configuration (via STDIN) and a set of environment variables.\nNetwork Configuration File The runtime finds network configurations by searching a dedicated directory, typically /etc/cni/net.d. Kubernetes, for example, monitors this directory periodically for configuration files. This directory can be configured; therefore, in your cluster, it might be located somewhere else.\nThis JSON file contains settings for the runtime and for any plugins it needs to call, such as bridge or ipam.\nHere is an example of a simple network configuration:\n1 2 3 4 5 6 7 8 9 10 11 { \"cniVersion\": \"1.1.0\", \"name\": \"test\", \"type\": \"bridge\", \"isDefaultGateway\": true, \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.244.0.0/24\" } } When the runtime reads this file, it identifies the main plugin to run from the “type” field. In this case, it’s bridge. The runtime will then look for an executable file named bridge in its plugin search path (usually /opt/cni/bin).\nIt is critical that network configuration files remain static while in use. CNI operations are not designed to handle configuration changes between an ADD and a DEL command.\nFor example, assume your configuration uses CIDR-A when a pod is created. If you modify the file to use CIDR-B before that pod is deleted, the CNI plugin will not know about the original CIDR-A. This will likely cause the plugin to fail when it tries to clean up the old network interfaces during the DEL operation.\nCNI Operations The JSON configuration defines what the network should look like, but it doesn’t specify what to do. The runtime tells the plugin which action to perform by setting environment variables.\nThe most important variable is CNI_COMMAND, which defines the operation:\nADD: Add container to network, or apply modifications DEL: Remove container from network, or un-apply modifications CHECK: Check container’s networking is as expected STATUS: Check plugin status VERSION: probe plugin version support GC: Clean up any stale resources Reference CNI spec\nOther variables provide the context for the command:\nCNI_NETNS: A path to the network namespace (e.g, /var/run/netns/{ns}) CNI_IFNAME: Interface name to create ‘inside’ the container. CNI_PATH: List of paths in the system to search CNI plugin executables. CNI_CONTAINERID: Container ID for the container. CNI_ARGS: Extra args provided by runtime. Example: An ADD Operation Let’s put this all together. Imagine we want to add a container with ID c123 to the network namespace container0, using the configuration file /etc/cni/net.d/10-mynet.conflist (which contains our JSON example).\nThe runtime would execute a command similar to this:\n1 2 3 4 5 6 7 8 9 10 # runtimes get these information via # Network configuration JSON. PLUGIN_NAME=\"bridge\" CNI_PATH=\"/opt/cni/bin\" CNI_COMMAND=\"ADD\" \\ CNI_CONTAINERID=\"c123\" \\ CNI_NETNS=\"/var/run/netns/container0\" \\ CNI_IFNAME=\"eth0\" \\ cat /etc/cni/net.d/10-mynet.conflist | $CNI_PATH/$PLUGIN_NAME In this command, the runtime pipes the JSON configuration to the bridge plugin’s STDIN and sets the environment variables. The bridge plugin executes and reads the config.\nIf you remember our quick demo, this configuration instructs the bash commands we run to:\nCreate eth0 interface in container (instead of c0-veth) The network namespace you need to operate is container0, which means that ip netns exec $operations You may also see CNI_NETNS_OVERRIDE. This variable is not part of the official CNI spec but is used by the libcni Go package. It acts as a guard-rail to stop the plugin from modifying the wrong network namespace if the namespace path changes unexpectedly during the operation. This should ideally not happen with modern container runtimes, as they do not rely on process ID based paths for network namespaces.\nhttps://github.com/containernetworking/plugins/issues/714\nKubernetes and CNI But how does Kubernetes use CNI? We mentioned that we may have multiple “plugins” available. We should somehow instruct Kubernetes to use correct plugins.\nKubernetes manages Pods with CRI (Container Runtime Interface), which is triggered by kubelet. So, kubelet calls CRI via gRPC API, and your cluster can use various CRI implementations (RPC implementation). While setting up a Pod, kubelet calls “rpc RunPodSandbox(RunPodSandboxRequest)”, which in turn calls underlying CRI. While CRI is setting up the Pod, it internally calls CNI to make sure that Pod’s network environment is ready. That’s actually how Kubernetes calls CNI. It’s a bit of a chain of calls but to simply put: kubelet -\u003e CRI -\u003e CNI\nFor example, how container-d calls CNI in RunPodSandbox: https://github.com/containerd/containerd/blob/1c4457e00facac03ce1d75f7b6777a7a851e5c41/internal/cri/server/sandbox_run.go#L261-L263\nCRI (Container Runtime Interface) calls CNI to set up the pod network, by following the above example. Then, based on CNI configurations and arguments, the CNI performs operations to set up the pod network.\nWriting CNI Plugin in Go The “bridge” demo we created is actually quite close to being a CNI plugin. We just need to make it aware of the CNI environment variables (like CNI_COMMAND) and make it read from STDIN and write to STDOUT, as defined in the CNI specification. However, if you’ve ever checked the official plugins, they are usually not shell scripts. We need to handle JSON, error reporting, and complex networking operations, which is a better job for a language like Go.\nThis section will go through my notes on what I learned from the official “bridge” plugin while writing my own.\nThe source code for the CNI maintained “bridge” plugin: https://github.com/containernetworking/plugins/tree/v1.8.0/plugins/main/bridge\nCNI maintainers provide official Go packages to handle the common boilerplate. This includes reading CNI environment variables, parsing the network configuration from STDIN, validating them, and formatting the result as JSON to STDOUT. Building all of this from scratch would be tedious.\nThe libcni package (https://pkg.go.dev/github.com/containernetworking/cni/libcni) and especially the skel package (https://pkg.go.dev/github.com/containernetworking/cni/pkg/skel) are designed to solve this.\nThe skel package provides a skeleton for a CNI plugin. It implements all the argument parsing and validation, like checking if all required environment variables are defined for a given CNI_COMMAND.\nhttps://github.com/containernetworking/plugins/blob/0e648479e11c2c6d9109b14fc0c9ac64c677861b/plugins/main/bridge/bridge.go#L837-L843\n1 2 3 4 5 6 7 8 func main() { skel.PluginMainFuncs(skel.CNIFuncs{ Add: cmdAdd, Check: cmdCheck, Del: cmdDel, Status: cmdStatus, }, version.All, bv.BuildString(\"bridge\")) } As a plugin developer, you just register your functions for the Add, Check, and Del commands. The skel package handles calling the right function and provides a struct containing all the parsed arguments and network configuration. The rest is just implementing the networking logic based on the provided information.\nLet’s skip the configuration parsing and dive into a Go-specific trick you’ll need.\nGo Scheduler and the Problem One thing I realized is that most CNI plugins use runtime.LockOSThread() before they change network namespaces. This isn’t a function I use in my typical day-to-day work. Understanding why this is necessary led me to learn much more about how Go’s scheduler works with OS threads.\nThe Go scheduler’s job is to run these N goroutines on M OS threads (this is called an M:N scheduler).\nGOMAXPROCS limits the number of OS threads that can execute user-level Go code simultaneously. By default, this is set to the number of available CPU cores. Note that Go can create additional OS threads beyond this limit when threads block on system calls, but those blocked threads don’t count against the GOMAXPROCS limit. Only threads actively executing Go code are constrained by GOMAXPROCS.\nThe Go scheduler maintains multiple runqueues to manage goroutines efficiently. There is a global runqueue shared by all threads, and each thread has its own local runqueue. This design minimizes lock contention. When a thread needs work, it first checks its local queue, and if empty, it can steal work from other threads’ queues or check the global queue.\nThe Go scheduler uses asynchronous preemption. This means that if a goroutine runs for too long (e.g., more than 10ms), the scheduler can forcefully pause it and schedule it to run again later. This is great for fairness, as it prevents one CPU-heavy goroutine from starving all others.\nFor example, if you have a goroutine that blocks the CPU and not being able to voluntarily give up its control to CPU back (due to some maybe hardware flaw), then other goroutines in the OS thread’s runqueue needs to wait for this long running to finish.\nThis preemption has a critical side effect: a goroutine is not guaranteed to stay on the same OS thread. It might run on Thread 1, get preempted, and later resume its work on Thread 2. For most code, this doesn’t matter. But for a CNI plugin, it’s a huge problem.\nIssues with Go and Network Namespaces You might be wondering, “What does this have to do with CNI?”. The problem lies in how we change network namespaces.\nWhen we create a network namespace for a pod, we need to run commands inside it. We don’t use the ip netns exec command; instead, we use a Linux system call: setns(2) under the hood.\nThe setns(2) syscall is very specific: it changes the namespace of the calling thread, not the whole process. So, this Linux system call associates a thread with a namespace, not a process with a namespace. In Go, this might be a bit problematic due to Go scheduler behavior.\nGo scheduler can preempt the goroutine and move it to another OS thread, due to motivations explained in the previous section. For example, assume that cmdAdd goroutine is running on OS Thread A (which is in the host netns). The goroutine calls setns() to enter the pod’s netns, which changes OS Thread A’s state. Now, the thread is in the pod’s network namespace. Your goroutine continues, preparing to create the veth pair. BAM! The Go scheduler preempts your goroutine because your time is up, the turn is for the next scheduled goroutine. A moment later, the scheduler decides to resume your goroutine. It picks OS Thread B (which is still in the host netns) to run it. Your goroutine, unaware it has been moved, continues where it left off. It tries to create the veth pair, but it’s now running on OS Thread B, so it incorrectly modifies the host network instead of the pod’s network. This is a subtle and dangerous bug :(.\ngoroutine -\u003e OS Thread A (namespace: host) | | (preemption, goscheduler moves goroutine to the new thread) v OS Thread B (namespace: somethingelse) Solution: runtime.LockOSThread() This is precisely what runtime.LockOSThread() solves.\nWhen you call runtime.LockOSThread(), you are telling the Go scheduler: “Pin this current goroutine to the current OS thread”. It does not stop the scheduler. Your goroutine can still be preempted. But when the scheduler resumes it, it is forced to resume it on the exact same OS thread it was pinned to. When we lock the thread, it also prevents the Go scheduler from running other goroutines on that thread. This is useful if your goroutine modifies the thread’s namespace state (like network namespace operations). It ensures that no other goroutine that might also try to update thread state (like changing the network namespace) gets scheduled on that thread.\nFor example, when a goroutine calls runtime.LockOSThread(), it is now pinned to OS Thread A. When this goroutine running on Thread A enters the pod network namespace and gets preempted, the scheduler will resume the goroutine on the same OS Thread A. In the meantime, no other goroutines will be scheduled on that thread.\nThis is why you see most of the namespace-switching logic in CNI plugins using a LockOSThread/UnlockOSThread block.\nhttps://github.com/containernetworking/plugins/blob/372953dfb89fe5c17a29a865b502a2eabb31a195/pkg/ns/ns_linux.go#L27-L35\nOne final remark is that this thread lock is not inherited by new goroutines. If you spawn a new goroutine from your locked one, the new goroutine can run on any thread.\n1 2 3 4 5 runtime.LockOSThread() defer runtime.UnlockOSThread() go func() { // this new goroutine may run on a different thread }() Therefore, do not spawn a new goroutine from a locked one if the new goroutine is expected to run on the same thread.\nLockOSThread() works like a taint. If you do not unlock the thread, the thread will not be used for scheduling other goroutines anymore. When the locked goroutine exits without unlocking, the thread itself will be terminated. So, do not forget to unlock the thread (unless you know what you are doing) to return it to the scheduler pool.\nTo obtain the current ns of the thread, you can simply use /proc/${os.getPid()}/task/${unix.Gettid()}/ns/net\nBridge Plugin The official Bridge plugin roughly performs the following operations:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 br, brInterface, err := setupBridge(n) // ... netns, err := ns.GetNS(args.Netns) // ... hostInterface, containerInterface, err := setupVeth(netns, br, args.IfName) // run IPAM plugin for IP assignment ipam.ExecAdd(n.IPAM.Type, args.StdinData) netns.Do(func(_ ns.NetNS) error { // Add the IP to the interface return ipam.ConfigureIface(args.IfName, result) }, ) // ... // add IP to bridge as well. err = ensureAddr(br, \u0026gw) So, what we are doing is as follows:\nsetup bridge setup veth pairs run IPAM plugin and configure veth and bridge to ensure they have IP How does it create links (devices)? For that, it uses https://github.com/vishvananda/netlink package which is an API to perform operations similar to ip link add by communicating with the kernel. And this package is the Go binding of this kernel communication.\nThe Do method is quite powerful method, which is defined as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 type NetNS interface { // Executes the passed closure in this object's network namespace, // attempting to restore the original namespace before returning. // However, since each OS thread can have a different network namespace, // and Go's thread scheduling is highly variable, callers cannot // guarantee any specific namespace is set unless operations that // require that namespace are wrapped with Do(). Also, no code called // from Do() should call runtime.UnlockOSThread(), or the risk // of executing code in an incorrect namespace will be greater. See // https://github.com/golang/go/wiki/LockOSThread for further details. Do(toRun func(NetNS) error) error // ... other methods } https://github.com/containernetworking/plugins/blob/0e648479e11c2c6d9109b14fc0c9ac64c677861b/pkg/ns/ns_linux.go#L74-L83\nIt is a powerful method that achieves what we are looking for. It locks the thread, runs our Go code in that thread to safely manipulate the thread, and takes the network namespace where the operation has started (like host).\nSo, Do ensures the user operation is safe by locking the goroutine to a specific OS thread, performing the namespace switch, running the code, and carefully switching back.\nNote: The following is a simplified, pseudocode representation of Do method. It omits error handling, handle closing (.Close()), and other details to focus purely on the core logic and execution flow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 // Do executes the 'toRun' function within the network namespace 'ns'. func (ns *netNS) Do(toRun func(hostNS NetNS)) { // Check if the target namespace handle 'ns' is already closed. if err := ns.errorIfClosed(); err != nil { return err } // Get a handle to the current (host) namespace before we do anything. // This 'hostNS' handle is what we'll pass to the user's function. // So that if user needs to perform operations in the initial network namespace // it can use this `hostNS` to checkout to the correct network namespace. hostNS := getCurrentNS() // We need a WaitGroup to wait for the goroutine to finish. var wg sync.WaitGroup wg.Add(1) var innerError error go func() { defer wg.Done() // Lock this goroutine to its current OS thread, as discussed runtime.LockOSThread() // Get a handle to this specific thread's original namespace. threadOriginalNS := getCurrentNS() // Switch to the target namespace, like `netns(2)` system call. // this will affect the thread's state. ns.Set() defer func() { // Switch back to the thread's original namespace. switchBackErr := threadOriginalNS.Set() // - ONLY unlock the OS thread if we successfully switched back. // - If 'switchBackErr' is not nil, we are in a bad state. // - We leave the thread locked. The Go runtime will see // this and discard the OS thread entirely, preventing // a \"dirty\" thread (stuck in the wrong ns) from // being reused by the scheduler, as explained in the prev section. if switchBackErr == nil { runtime.UnlockOSThread() } }() // Now that we are inside the target namespace 'ns', execute the user's function. innerError = toRun(hostNS) }() wg.Wait() return innerError } This is quite useful, especially while setting up veth pairs.\nIf you remember, we were running ip commands in both host and container namespaces while setting up the veth pairs. With this Do method, we can actually achieve something similar. The following is the pseudocode for “bridge” plugin while setting up the veth pairs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // - First, obtain CNI_IFNAME and CNI_NETNS to understand // what interface name should be created in which container namespace. // These are embedded in the `args` struct, which is initialized by `skel` boilerplate // from `libcni` package. // - And then, in host namespace, checkout the container namespace // which is created by runtime (k8s). // - Then, use Do method to perform operations required to create veth pairs. containerNs, _ := ns.GetNS(args.Netns) defer netns.Close() // Note that we run this code in the host namespace. // That's why the argument in caller is 'hostNs'. // By using the Do function, we run the operations in containerNs with locked OS thread. containerNs.Do(func(hostNs ns.NetNS) error { // The operations in this caller is running in the `containerNs`. // First create a veth device. veth:= netlink.Veth{ LinkAttrs: netlink.LinkAttrs{Name: args.Ifname}, PeerName: hostVethPeerName, // - PeerNamespace is the interface of the peer (host). // - It can take the namespace as FD (file descriptor), // or PID (process ID) for the namespace. // - Since the file descriptor approach is the convenient one, // as explained with CNI_NETNS_OVERRIDE above, // use helpers of `ns.NetNS` struct to obtain file descriptor // of the host network namespace. // - Since the veth pair that we create here is going to be located // in the container its peer needs to be in host, // so that we can connect to it to the bridge. PeerNamespace: netlink.NsFd(int(hostNs.Fd())) /*...*/ } // Then, `add` this link device; similar to `ip link add` netlink.LinkAdd(\u0026veth) // Set this new link device `up`, similar to `ip link set up` netlink.LinkSetUp(netlink.LinkByName(args.Ifname)) // Now, run the following code in \"host\" namespace. hostNs.Do(func(_ ns.NetNS) error { // Set the veth pair in the host namespace UP. hostVeth, _ := netlinksafe.LinkByName(hostVethPeer) netlink.LinkSetUp(hostVeth) return nil }) }) Kernel Knobs The official “bridge” plugin uses more kernel knobs that we see so far. We only saw “ip_forward” but the official plugin uses more than that to provide better experience the users. This section will go through some of them that I noticed to mention.\n/net/ipv4/conf//arp-notify This knob controls Gratuitous ARP (GARP) behavior when the interface’s state becomes UP. A Gratuitous ARP is a broadcast packet where a host announces its own IP-to-MAC mapping, in order to update the caches of other devices on the same network. By default (0), the kernel does not send a GARP when an interface comes up. When enabled (1), the kernel will send GARP packets when this interface is brought to an UP state or when an IPv4 address is added to it.\nThis setting directly addresses the problem of stale ARP caches on peer devices. Especially in high-availability (HA) or IP failover scenarios, such as a Kubernetes VIP moving between nodes, a client’s ARP cache may still point to the MAC address of the old, failed node. This creates issues until that client’s ARP entry times out.\nEnabling arp_notify solves this by having the kernel proactively announce the new “IP-to-MAC” mapping. For ex, when a standby node’s interface comes UP to take over a VIP, the arp_notify mechanism triggers a GARP, forcing all listeners on the L2 segment to immediately update their ARP caches. This preempts the cache timeout and makes the failover near-instantaneous.\n/net/ipv6/conf//keep_addr_on_down This configuration prevents the kernel from flushing static global IPv6 addresses when an interface’s link state goes DOWN. This is a critical difference from IPv4, where addresses are retained by default.\nWith the default setting (0), the kernel flushes all global IPv6 addresses when the link state goes DOWN. By setting this knob to 1, the kernel preserves the IPv6 address during the link-down event.\nMulti-Node reachability Besides from the implementation-wise differences, the official “bridge” plugin in Go is similar to what we have built. We’ve walked through the logic of building a CNI plugin, from using CNI Go packages to safely managing network namespaces with LockOSThread. We have a complete solution for local pod connectivity. Our plugin can successfully get a cmdAdd call, create a veth pair, connect a pod to the host’s bridge, and get it an IP address. This setup works perfectly for any pods that need to communicate on the same node.\nBut what happens when Pod A on Node 1 tries to send a packet to Pod B on Node 2? As we’ve established, our bridge plugin alone can’t solve this. The packet gets sent to the cni0 bridge and then dropped by the host, which has no route to the other node’s pod network.\nThis is the central challenge of Kubernetes networking: multi-node reachability. Since the CNI specification itself only covers ‘connectivity’, it’s up to the CNI daemon to solve this. This next section will explore the common strategies used to make this cluster-wide communication possible.\nLet’s first see why the ‘bridge’ approach that we explained and developed does not provide reachability in multi-node Kubernetes clusters.\nThe bridge plugin (like the cni0 bridge it creates) is a purely host-specific construct. The cni0 virtual switch on Node 1 is completely separate from the cni0 virtual switch on Node 2. They have no knowledge of each other and are not connected in any way. Think of it like two switches located in two networks having two switches respectively. You cannot expect a switch on one network to communicate with the switch on the other without extra configuration.\nThis separation creates a “reachability” problem.\nFor example, consider when Pod A on Node 1 (10.244.1.5) tries to send a packet to Pod B on Node 2 (10.244.2.8):\nThe packet leaves Pod A and arrives at the cni0 bridge on Node 1. The bridge inspects the destination (10.244.2.8) and finds it isn’t connected to any of its local pod interfaces, as it only knows about its own 10.244.1.x pods. The packet is then passed up to the host, Node 1. However, Node 1’s kernel also has no route for the 10.244.2.0/24 subnet; it has no idea that this network “lives” on Node 2, thus the packet is unroutable and dropped. This is precisely the problem that the CNI daemon (the second component we discussed) is designed to solve. Its entire job is to create the “routes” that Node 1 is missing, making Node 2’s pods reachable. The strategy this daemon uses depends entirely on the underlying physical network. We will now explore the three most common strategies for solving this multi-node reachability problem.\nBroadly, your cluster’s nodes live in one of two physical network scenarios\nNodes are on the same L2 Network: This means all your nodes are in the same subnet (e.g., all have IPs like 192.168.1.x). They can find and send packets to each other directly, like computers plugged into the same home router. So, no routing is needed.\nNodes are on different L2 Networks: This is the most common scenario. Your nodes are in different subnets (e.g., 192.168.1.x and 192.168.2.x, or in different Availability Zones). They cannot reach each other directly and must send packets through at least one router. So, routing is needed.\nThese two network designs require completely different solutions to make pods reachable. We will now explore the three most common strategies, though there are different solutions as well.\nSimple Routing in Same Subnet This is a straightforward approach that works only when the nodes are on the same L2 network. The CNI daemon on each node acts as a controller, watching the Kubernetes API. When a new node joins the cluster, the daemon on every other node gets an update.\nFor example, when Node 2 joins, the CNI daemon on Node 1 sees the new Node 2 object. It reads two key pieces of information: Node 2’s IP (e.g., 192.168.1.11) and the pod network assigned to it, known as its podCIDR (e.g., 10.244.2.0/24). The daemon then adds a route to Node 1’s local routing table, typically using a netlink library, such as ip route add 10.244.2.0/24 via 192.168.1.11. Now, when Pod A on Node 1 sends a packet to Pod B (10.244.2.5), Node 1’s kernel sees this route and knows to forward the packet directly to Node 2’s IP. This is very high-performance because there is no encapsulation.\nWe’ll develop an example controller to perform this as well.\nDynamic Routing Protocols These protocols allow configuring routers to exchange route information between each other. BGP is one of the popular protocol used for this.\nBGP, in general, forms neighourship across routers, where neighbourship is declared statically. Once you register your peers, routing information is shared across routers, through a TCP connection.\nIn Kubernetes, this usually works by having the CNI daemon on each node run a lightweight BGP agent. This agent “peers” (forms a BGP neighborship). Calico is a famous CNI plugin that supports BGP.\nThis is quite useful because if you have multiple large clusters, you can use BGP or something similar to achieve route sharing across your cluster’s outer router. The routers are all taught where the pod networks are. Although this is a very powerful and scalable solution, it’s also the most complex. It requires access to and expertise in configuring your physical network fabric (your routers and switches) to peer with your nodes, which may not be possible all the time.\nOverlay Networks An overlay network is a virtual network built on top of an existing physical network, like a cloud provider’s network. This concept isn’t specific to Kubernetes but very useful in managed Kubernetes offerings, because your cluster nodes (like VMs) might be on different underlying networks, such as in different cloud availability zones.\nIn modern data centers or clouds, your cluster nodes (VMs) are often in different locations. They might be on different racks or even in different “availability zones”. They are not all plugged into one giant switch, instead they live on a complex, routed network.\nThis creates a problem: the physical network only knows how to send packets between nodes (using node IPs). It has no idea what a pod IP is or how to find it. So, even though nodes are connected by L3 connectivity, each nodes get its own subnet. Assume that Node 1 and Node 2 are VMs located in different servers and if Node 1 just sent a packet addressed to Pod IP located in Node 2, the physical network would drop it, not knowing where to send it even though Node 1 and Node 2 can communicate with each other (L3 connectivity).\nTo solve this, the overlay network hides the original pod packet. It does this by “wrapping” it inside a new, “outer” packet. This process is called encapsulation. The new outer packet is addressed to the node that the pod lives on (e.g., Node 2’s IP), which the physical network does understand.\nOne of the most common overlay networking technique is VXLAN (Virtual Extensible LAN) which uses the similar idea of wrapping packets. Each node has a virtual device called VTEP (Virtual Tunnel Endpoint), where packets are sent to this tunnel endpoint before they are going into other nodes.\nFor example, assume that we have Pod A on Node 1 and Pod B on Node 2. When Pod A on Node 1 sends a packet to Pod B, the packet targets Pod B’s IP address. The kernel recognizes that this Pod is not in our subnet, so the packet is destined to VTEP, because since Pod B’s IP address is local to Node 2, if the physical network receives it, it does not know how to send it to the actual place (similar to NAT problem).\nThe VTEP encapsulates this packet inside a new UDP packet, which is destined to Node 2’s IP.\n+-----------------------------------------------------+ | Outer IP Header (Source: Node 1, Dest: Node 2) | +-----------------------------------------------------+ | Outer UDP Header (Dest Port: 4789 for VXLAN) | +-----------------------------------------------------+ | VXLAN Header (VNI) | +-----------------------------------------------------+ | +---------------------------------------------+ | | | Inner IP Header (Source: Pod A, Dest: Pod B)| | | +---------------------------------------------+ | | | Payload (Data) | | | +---------------------------------------------+ | +-----------------------------------------------------+ Note: This is a simplified diagram. A real packet would also include L2 (Ethernet) headers for both the inner and outer packets, as well as a specific VXLAN header. But this illustrates the main idea of wrapping the pod-to-pod packet inside a node-to-node packet.\nThe physical network sees a normal UDP packet from Node 1 to Node 2 and routes it. Node 2’s kernel receives the UDP packet, sees it’s for VXLAN, and hands it to its VTEP. The VTEP de-encapsulates it, removing the outer UDP packet and what’s left is the original packet addressed to Pod B. Since Node 2 knows that the Pod B is on its system, it delivers it.\nThis is a popular approach in most cloud providers because it decouples the pod network from the physical network. It allows Kubernetes to create its own flat, virtual network for pods without having to ask the cloud provider to make any special changes to its physical routers. As long as the nodes can send UDP packets (L3 connectivity) to each other, the pod network just works.\nWriting CNI Daemon in Go Finally, in this section, we are going to write a simple daemon utilizing the principles defined in the Routing in Same Subnet section. We’ll create a Kubernetes controller that modifies each node’s route table to ensure networking works as expected.\nOur controller will run as a DaemonSet, meaning one copy (one pod) will run on each node in the cluster. This daemon watches for changes to Node objects. Its job is to ensure that the correct routes are set up on the host it’s running on.\nIt adds a route for every other node in the cluster, telling the host’s kernel: “To reach pods on Node B (with PodCIDR 10.244.1.0/24), send the traffic directly to Node B’s internal IP (e.g., 192.168.1.101).”\nHow Can a Pod Change the Host’s Routes? Before we look at the Go code, we should understand how a pod, which is normally an isolated sandbox, is allowed to modify its host. Our DaemonSet’s pod needs several high-privilege settings:\nhostNetwork: true: This is the most important setting. It tells Kubernetes to not create a separate network namespace for this pod. The pod will run directly in the host’s network namespace. This gives our controller access to the host’s eth0 and, crucially, its main routing table. NET_ADMIN Capability: Even in the host’s namespace, a process needs permission to change network settings. This capability grants our controller the right to add and delete routes. priorityClassName: system-node-critical: This is a best practice for CNIs. It tells Kubernetes this pod is essential for the node to function, so it should be scheduled first and be the last to get evicted if the node is under resource pressure. tolerations: - operator: Exists: This ensures our DaemonSet pod will run on all nodes, including control-plane nodes that might have taints. This is necessary for full cluster connectivity. You may also see an initContainer in CNI DaemonSets. This init container often has privileged: true and mounts hostPath volumes for /opt/cni/bin and /etc/cni/net.d. Its job is different: it’s responsible for installing the CNI plugin binary (like our bridge plugin) onto the host’s filesystem, so the kubelet can call it.\nController Note: The following is a simplified pseudocode representation. It omits error handling, logging, and other production-level details to focus purely on the core logic and execution flow. The source code can be found here for reference: https://github.com/buraksekili/hirs-cni/blob/main/cmd/controllers/node/main.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 func main() { currentNodeName := os.Getenv(\"NODE_NAME\") mgr := CreateControllerManager() reconciler := \u0026NodeReconciler{ Client: mgr.GetClient(), NodeName: currentNodeName, } // Set up the controller with the manager. // - We watch for \"Node\" objects. // - We filter out events for our *own* node, // since we don't need to add a route to ourselves. isRemoteNode := predicate.NewPredicateFuncs(func(obj client.Object) bool { return obj.GetName() != r.NodeName }) ctrl.NewControllerManagedBy(mgr). For(\u0026corev1.Node{}). WithEventFilter(predicate.Filter(isRemoteNode)). Complete(reconciler) mgr.Start() } func (r *NodeReconciler) Reconcile(ctx, req) { // We already filtered out our own node, so this is always a remote node. node := r.Get(ctx, req.NamespacedName) if node.IsBeingDeleted() { if controllerutil.ContainsFinalizer(node, \"hirscni.io/route-cleanup\") { r.deleteRoute(node) controllerutil.RemoveFinalizer(node, \"hirscni.io/route-cleanup\") return r.Update(ctx, node) } return } // add finalizer if it does not exist. if controllerutil.AddFinalizer(node, \"hirscni.io/route-cleanup\") { r.Update(ctx, node) return } return r.reconcileRoute(ctx, node) } func (r *NodeReconciler) reconcileRoute(ctx, node) { podCIDR := node.Spec.PodCIDR if podCIDR == \"\" { return } // Get that node's internal IP (e.g., \"192.168.1.101\"). nodeInternalIP := getNodeInternalIP(node) if nodeInternalIP == \"\" { return } r.addOrUpdateRoute(podCIDR, nodeInternalIP) } func (r *NodeReconciler) addOrUpdateRoute(podCIDR, gatewayIP) { // Parse the destination CIDR. dst := net.ParseCIDR(podCIDR) // Parse the gateway (the remote node's) IP. gw := net.ParseIP(gatewayIP) // Find the local interface (link) to reach that gateway. // If you remember, we were using `ip route get ` command // to see which route the host takes while reaching . // `netlink.RouteGet` runs a similar request. routes := netlink.RouteGet(gw) linkIndex := routes[0].LinkIndex // The following route means that, to reach the address `dst`, // forward the packet to `gw` through `linkIndex` device. route := \u0026netlink.Route{ LinkIndex: linkIndex, // e.g., 'eth0' Dst: dst, // e.g., 10.244.1.0/24 Gw: gw, // e.g., 192.168.1.101 } // just a tip: `ip route replace` is the idempotent call // to manage routes. netlink.RouteReplace(route) } func (r *NodeReconciler) deleteRoute(node) { podCIDR := node.Spec.PodCIDR nodeInternalIP := getNodeInternalIP(node) dst := net.ParseCIDR(podCIDR) gw := net.ParseIP(nodeInternalIP) routes := netlink.RouteGet(gw) linkIndex := routes[0].LinkIndex route := \u0026netlink.Route{ LinkIndex: linkIndex, Dst: dst, Gw: gw, } netlink.RouteDel(route) } func getNodeInternalIP(node) string { // Loop through the node's addresses and find the one labeled \"InternalIP\" for _, addr := range node.Status.Addresses { if addr.Type == corev1.NodeInternalIP { return addr.Address } } return \"\" } Conclusion We’ve covered a lot of ground in this post. We started with the very basics of networking theory, then saw how those ideas are implemented in Linux with tools like network namespaces, veth pairs, and bridges.\nFrom that foundation, we explored the CNI specification and wrote our own CNI plugin in Go, even tackling tricky concurrency issues like runtime.LockOSThread. We also learned that a plugin is only half the story and went on to build a complete CNI daemon as a Kubernetes controller, teaching it to manage routes across multiple nodes.\nWhile we’ve built a functional CNI, the Kubernetes networking world is vast. We didn’t get to critical features like Network Policy (for security), Service Routing (for ClusterIPs), or IP Address Management (IPAM).\nI hope this journey has helped demystify what’s happening under the hood. The next time you deploy a pod and see it instantly get an IP, you’ll know the whole story, from the kubelet calling the plugin for local connectivity, to the CNI daemon ensuring that pod is reachable by the rest of the cluster.\nAs I mentioned, this is all based on my personal notes. If you find any mistakes or have suggestions, feel free to edit this post through GitHub and raise a PR or contact me via X.\nReferences Go Scheduler Kubernetes and the CNI: Where We Are and What’s Next - Casey Callendrello, CoreOS Linux Namespaces and Go Don’t Mix Linux Namespaces Part 4 Namespaces in Operation, part 4: more on PID namespaces Networking and Kubernetes: A Layered Approach Runtime Package Documentation Tutorial: Communication Is Key - Understanding Kubernetes Networking - Jeff Poole, Vivint Smart Home ","wordCount":"14701","inLanguage":"en","datePublished":"2025-11-09T00:00:00Z","dateModified":"2025-11-09T21:38:34+03:00","author":{"@type":"Person","name":"Burak Sekili"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://buraksekili.github.io/articles/cni/"},"publisher":{"@type":"Organization","name":"Burak Sekili","logo":{"@type":"ImageObject","url":"https://buraksekili.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://buraksekili.github.io/ accesskey=h title="Burak Sekili (Alt + H)">Burak Sekili</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://buraksekili.github.io/archives/ title=Articles><span>Articles</span></a></li><li><a href=https://buraksekili.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://buraksekili.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://buraksekili.github.io/articles/>Articles</a></div><h1 class=post-title>Let's Build a CNI Plugin! From Linux Networking to CNI</h1><div class=post-description>From networking theory to Kubernetes networking. This blog covers networking fundamentals, Linux networking, and writing a CNI plugin and daemon in Go.</div><div class=post-meta><span title='2025-11-09 00:00:00 +0000 UTC'>November 9, 2025</span>&nbsp;·&nbsp;70 min&nbsp;·&nbsp;Burak Sekili</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#networking aria-label=Networking>Networking</a><ul><li><a href=#switch aria-label=Switch>Switch</a></li><li><a href=#router aria-label=Router>Router</a></li><li><a href=#osi-l2-l3-and-l4 aria-label="OSI (L2, L3 and L4)">OSI (L2, L3 and L4)</a><ul><li><a href=#layer-2 aria-label="Layer 2">Layer 2</a></li><li><a href=#layer-3 aria-label="Layer 3">Layer 3</a></li><li><a href=#layer-4 aria-label="Layer 4">Layer 4</a></li></ul></li><li><a href=#switching-l2-domain aria-label="Switching (L2 Domain)">Switching (L2 Domain)</a><ul><li><a href=#learn aria-label=Learn>Learn</a></li><li><a href=#flood aria-label=Flood>Flood</a></li><li><a href=#forward aria-label=FORWARD>FORWARD</a></li></ul></li><li><a href=#routing-l3-domain aria-label="Routing (L3 Domain)">Routing (L3 Domain)</a><ul><li><a href=#routing-table aria-label="Routing Table">Routing Table</a></li><li><a href=#nat aria-label=NAT>NAT</a><ul><li><a href=#snat-source-nat aria-label="SNAT (source NAT)">SNAT (source NAT)</a></li><li><a href=#dnat-destination-nat aria-label="DNAT (destination NAT)">DNAT (destination NAT)</a></li></ul></li></ul></li></ul></li><li><a href=#linux-networking-single-host aria-label="Linux Networking (Single Host)">Linux Networking (Single Host)</a><ul><li><a href=#isolation-network-namespaces aria-label="Isolation (network namespaces)">Isolation (network namespaces)</a></li><li><a href=#network-device-abstraction-ip-link aria-label="Network Device abstraction (ip link)">Network Device abstraction (ip link)</a></li><li><a href=#the-cable-veth-pairs aria-label="The cable (veth pairs)">The cable (veth pairs)</a></li><li><a href=#the-virtual-switch-bridge aria-label="The Virtual Switch (bridge)">The Virtual Switch (bridge)</a></li><li><a href=#l3-gateway aria-label="L3 Gateway">L3 Gateway</a></li></ul></li><li><a href=#kubernetes-networking--cni aria-label="Kubernetes Networking & CNI">Kubernetes Networking & CNI</a><ul><li><a href=#motivation-and-automation-of-linux-networking aria-label="Motivation and Automation of Linux networking">Motivation and Automation of Linux networking</a></li><li><a href=#cni-container-networking-interface aria-label="CNI (Container Networking Interface)">CNI (Container Networking Interface)</a><ul><li><a href=#cni-plugin aria-label="CNI Plugin">CNI Plugin</a></li></ul></li><li><a href=#how-cni-plugins-called aria-label="How CNI Plugins Called">How CNI Plugins Called</a><ul><li><a href=#network-configuration-file aria-label="Network Configuration File">Network Configuration File</a></li><li><a href=#cni-operations aria-label="CNI Operations">CNI Operations</a></li><li><a href=#example-an-add-operation aria-label="Example: An ADD Operation">Example: An ADD Operation</a></li><li><a href=#kubernetes-and-cni aria-label="Kubernetes and CNI">Kubernetes and CNI</a></li></ul></li><li><a href=#writing-cni-plugin-in-go aria-label="Writing CNI Plugin in Go">Writing CNI Plugin in Go</a><ul><li><a href=#go-scheduler-and-the-problem aria-label="Go Scheduler and the Problem">Go Scheduler and the Problem</a></li><li><a href=#issues-with-go-and-network-namespaces aria-label="Issues with Go and Network Namespaces">Issues with Go and Network Namespaces</a></li><li><a href=#solution-runtimelockosthread aria-label="Solution: runtime.LockOSThread()">Solution: runtime.LockOSThread()</a></li></ul></li><li><a href=#bridge-plugin aria-label="Bridge Plugin">Bridge Plugin</a><ul><li><a href=#kernel-knobs aria-label="Kernel Knobs">Kernel Knobs</a><ul><li><a href=#netipv4confinterfacearp-notify aria-label="/net/ipv4/conf/<interface>/arp-notify"><code>/net/ipv4/conf/&lt;interface>/arp-notify</code></a></li><li><a href=#netipv6confinterfacekeep_addr_on_down aria-label="/net/ipv6/conf/<interface>/keep_addr_on_down"><code>/net/ipv6/conf/&lt;interface>/keep_addr_on_down</code></a></li></ul></li></ul></li><li><a href=#multi-node-reachability aria-label="Multi-Node reachability">Multi-Node reachability</a><ul><li><a href=#simple-routing-in-same-subnet aria-label="Simple Routing in Same Subnet">Simple Routing in Same Subnet</a></li><li><a href=#dynamic-routing-protocols aria-label="Dynamic Routing Protocols">Dynamic Routing Protocols</a></li><li><a href=#overlay-networks aria-label="Overlay Networks">Overlay Networks</a></li></ul></li><li><a href=#writing-cni-daemon-in-go aria-label="Writing CNI Daemon in Go">Writing CNI Daemon in Go</a><ul><li><a href=#how-can-a-pod-change-the-hosts-routes aria-label="How Can a Pod Change the Host&rsquo;s Routes?">How Can a Pod Change the Host&rsquo;s Routes?</a></li><li><a href=#controller aria-label=Controller>Controller</a></li></ul></li></ul></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>This blog post is a collection of my personal notes on networking. For a long time, I had to jump between different notebooks to connect concepts; from core networking theory, to Linux internals, all the way up to Kubernetes and CNI.
This post is my attempt to combine all those notes into a single, logical document.</p><p>We&rsquo;ll follow a step-by-step path. We&rsquo;ll start with fundamental network concepts, then see how those are implemented in Linux, which is the foundation for most modern virtual networking.
Finally, we&rsquo;ll see how Kubernetes builds on top of it all.</p><p>As a quick disclaimer, I&rsquo;m writing this from the perspective of a software engineer, not a network engineer.
This post is the guide I wish I&rsquo;d had, and I&rsquo;m sharing my learning journey in the hope that it&rsquo;s as helpful to you as it has been to me.</p><h2 id=networking>Networking<a hidden class=anchor aria-hidden=true href=#networking>#</a></h2><blockquote><p>Feel free to skip this section if you are familiar with Switch vs Router comparison at least.</p></blockquote><p>Let&rsquo;s start with fundamentals. Networking in general usually involves a lot of terminologies and acronyms, which makes it a bit challenging, at least for me, to understand when I need to check something. Therefore, understanding certain fundamental concepts might be helpful even in general tech literacy, even if you are not a networking engineer.</p><p>A &ldquo;host&rdquo; is almost any device connected to a network, like your phone, computer, server, or printer. So, &lsquo;host&rsquo; is a widely used term, and it does not only mean &lsquo;server&rsquo;; it can be any device connected to a network. But what is a network? It is the underlying communication fabric, built from physical links, hardware, firmware, and software. It uses protocols to provide communication, or in other words, packet-delivery services, between these hosts.
As an example, if you have multiple devices connected together with patch cables (like a router and your laptop), it creates a local area network, a LAN - in most simple setups, unless advanced isolation techniques are used.</p><p>Now you have a local network where multiple devices (laptops, phones, etc.) are connected to each other. The next question is, how do those hosts communicate? We need a way to say, &ldquo;Okay &rsquo;laptop-xyz&rsquo;, send this request to my printer &lsquo;printer-abc&rsquo;.&rdquo; That&rsquo;s where the MAC address comes into play. A MAC address is a unique hardware ID assigned to a machine, usually by its producer. For one host to send data to another on the same LAN, it must send that data to the receiver&rsquo;s MAC address.</p><p>That&rsquo;s good, but what happens if we change our laptop? The new laptop will have a new MAC address. If we only used MAC addresses, we would have to reconfigure everything to find the new one. As you can see, this would be a hassle and is an infeasible way to maintain a network, especially since devices are always joining and leaving.</p><p>This brings up a new problem. The MAC address solves the &lsquo;where&rsquo; problem: where to send the packet on the local network. But it&rsquo;s not good for the &lsquo;who&rsquo; problem: who is the device I want to talk to, regardless of its specific hardware? This is why we have IP addresses (this is not the only reason of course). An IP address is a logical address assigned to a host. It&rsquo;s the stable &ldquo;who&rdquo; we want to communicate with.</p><p>Also, please note that we don&rsquo;t use IP addresses instead of MAC addresses; we use them together. Your application sends a packet to an IP address (the logical &lsquo;who&rsquo;). Then, your computer uses a special protocol to find the current MAC address (the physical &lsquo;where&rsquo;) associated with that IP. This two-part system gives flexible logical addressing (IPs) while still using the hardware MAC addresses to deliver the data on the local network to correct device.</p><p>But then, you may be wondering: how does a host find the MAC address for an IP address on its local network? This is done using the Address Resolution Protocol (ARP). For example, if host-a wants to send a packet to host-b (IP: 10.0.0.59), host-a initially has no idea what host-b&rsquo;s MAC address is. It first broadcasts an ARP request to the entire network, basically asking, &ldquo;Who has the IP address 10.0.0.59? Send me your MAC&rdquo;.
Every host on the network receives this broadcast, but only host-b recognizes its own IP and then sends an ARP reply directly back to host-a, saying, &ldquo;I have 10.0.0.59, and here is my MAC address.&rdquo; Once host-a learns host-b&rsquo;s MAC address, it can send its packet destined to host-b&rsquo;s MAC address.</p><h3 id=switch>Switch<a hidden class=anchor aria-hidden=true href=#switch>#</a></h3><p>Assume that our LAN has evolved and grown. Connecting every device directly to every other one becomes unscalable. The solution is a switch. Each device is connected to switch through ports located on the switch.
Switch knows where to find those hosts based on the host&rsquo;s connected port on switch. For example, if host-a and host-b are connected to port-1 and port-2 ports respectively, if host-a wants to send a packet to host-b, the switch knows that host-b is behind its port-2.</p><p>The most important feature of switch is that it allows communication <strong><em>within</em></strong> a network, using MAC addresses. This is quite important as we&rsquo;ll need to refer to this switch feature in a lot of different places.</p><h3 id=router>Router<a hidden class=anchor aria-hidden=true href=#router>#</a></h3><p>The switch handles traffic within our network, but what happens when a device wants to communicate with a device on a different network, like a server on the internet? The switch, looking at the destination MAC address, will see that it&rsquo;s not a local device and will send the data to the network&rsquo;s exit point, the router.</p><p>Routers allow communication between networks. This is one of the crucial differences between switch and router. Inter-network traffic is usually handled by switch. But once the packet needs to reach wide area network (WAN), or in other words another network on the Internet, we need a router.</p><p>Router knows which networks that they can route by maintaining a routing table, allowing them to
know where the corresponding IP address may be found in the WAN. When your router receives a data packet destined for Google, it doesn&rsquo;t know Google&rsquo;s MAC address but IP address. It consults its routing table and forwards the packet to the next router on the path, which repeats the process. This forwarding across many different networks is what allows you to reach any destination on the internet.</p><p>So, the key takeaways are that switching uses MAC addresses to move data within a single network, while routing uses IP addresses to move data between different networks.</p><h3 id=osi-l2-l3-and-l4>OSI (L2, L3 and L4)<a hidden class=anchor aria-hidden=true href=#osi-l2-l3-and-l4>#</a></h3><p>We are not going to dive into details of OSI layers; but it&rsquo;s usually good to be familiar with certain mechanisms and terminologies related to OSI layers in general, as it helps us to understand more complex systems like container networking and Kubernetes.</p><h4 id=layer-2>Layer 2<a hidden class=anchor aria-hidden=true href=#layer-2>#</a></h4><p>Layer 2, the Data Link Layer, interacts between the Layer 3 (Network) and Layer 1 (Physical) layers. Its main responsibility is to take Layer 3 packets (like IP packets) and encapsulate them into frames for transmission over a physical medium. Also, it receives incoming signals from L1, reassembles them into frames, de-encapsulates them and passes the payload (the original L3 packet) up the stack.</p><p>L2 uses a special addressing scheme called MAC to manage the delivery. By using MAC, L2 identifies the receiver host, and sends the data to the correct host.</p><p>Your data travels across different hosts, for instance host to switch (which is a host) then another host to router, etc etc until it reaches its destination. This process is usually done by devices like network cards (NICs), or switch. As we saw previously, switch allows communication within a network. If devices are connected by a switch, the switch forwards packets between devices, to ensure host to host communication in a network.</p><p>The key takeaway for L2 is that it provides a mechanism for host-to-host (or host-to-router) delivery, which is the ability to move a frame from one host to another within the same broadcast domain (e.g., host-to-host on the same LAN, or host-to-router). And switch is mainly a L2 device; though you can find L3 switches on the market as well :).</p><h4 id=layer-3>Layer 3<a hidden class=anchor aria-hidden=true href=#layer-3>#</a></h4><p>Sending data between connected hosts is not our usual Internet experience. We need to send our packets to other networks to access services on those networks. L2 delivery only ensures host-to-host delivery which happens in a single network. However, usual packets need to go beyond that, across networks involving multiple host-to-host deliveries. This is where L3 comes into picture.</p><p>L3 ensures routing of packets and end to end delivery. It uses its own addressing scheme called IP addresses which, unlike physical MAC addresses, are not tied to specific hardware. A router, the key L3 device, makes this work. When a router receives a packet, it reads the destination IP address. It then consults its routing table (a set of rules) to decide where to forward that packet next.</p><p>The key takeaway is that L3 provides the end-to-end logic for packet delivery across multiple networks. It does this by making a series of hop-by-hop routing decisions using IP addresses.</p><p>In the following example, we have two networks connected to each other via Router:</p><pre tabindex=0><code> Network 1 (10.0.0.0/24)                              Network 2 (71.0.0.0/24)
┌──────────────────────┐                          ┌───────────────────────┐ 
│                      │            IF2:          │                       │ 
│                      │            MAC: DD       │                       │ 
│                      │            IP: 71.0.0.1  │                       │ 
│        HOST A        │       Router             │        HOST B         │ 
│    ┌─────────────┐   │      xxxxxxxxx           │    ┌─────────────┐    │ 
│    │MAC: AA      │   │     x         x          │    │MAC: BB      │    │ 
│    │             ┼───│──&gt;  x         x────────&gt; │    │             │    │ 
│    │IP: 10.0.0.50│   │     x         x          │    │IP: 50.0.2.19│    │ 
│    └─────────────┘   │      xxxxxxxxx           │    └─────────────┘    │ 
│                      │   IF1:                   │                       │ 
│                      │   MAC: CC                │                       │ 
│                      │   IP: 10.0.0.1           │                       │ 
└──────────────────────┘                          └───────────────────────┘ 
</code></pre><p>Host A (10.0.0.50) wants to send a packet to Host B (71.0.0.59). Host A checks its own local routing table. It compares the destination 71.0.0.59 to its own network (subnet, 10.0.0.0/24) and determines the destination is not its own network. So, it sends the packet to router which is usually defined as default gateway.
In order to create the L2 frame, Host A needs the MAC address associated with 10.0.0.1. It uses ARP (Address Resolution Protocol), broadcasting a request: &ldquo;Who has 10.0.0.1? The router&rsquo;s IF1 interface replies: &ldquo;I have 10.0.0.1, and my MAC is CC.&rdquo;
Host A now builds the packet and encapsulates it in an L2 frame to send to the router:</p><pre tabindex=0><code># Frame 1: Host A -&gt; Router (on Network 1)

destination MAC: CC
source MAC: AA
--- (L3 Header) ---
destination IP: 71.0.0.59
source IP: 10.0.0.50
--- (Payload) ---
</code></pre><p>The router receives this frame on IF1 as the destination MAC matches, strips the L2 header and inspects the L3 header. Since the destination IP is 71.0.0.59, the router sends this packet to IF2 by creating again L2 header with the similar process (ARP request and then updating L2 header accordingly).</p><p>The router re-encapsulates the original, unchanged L3 packet into a new L2 frame, using its IF2 MAC (DD) as the source.</p><pre tabindex=0><code># Frame 2: Router -&gt; Host B (on Network 2)

destination MAC: BB
source MAC: DD
--- (L3 Header) ---
destination IP: 71.0.0.59
source IP: 10.0.0.50
--- (Payload) ---
</code></pre><p>Finally, Host B receives this frame, strips the L2 header, and inspects the L3 header. It sees its own IP as the destination and accepts the packet, passing the payload up the stack.</p><p>Again, the key takeaway from this example is that the L3 header (IPs) remained constant for the entire end-to-end delivery, while the L2 header (MACs) was rebuilt at each hop.</p><h4 id=layer-4>Layer 4<a hidden class=anchor aria-hidden=true href=#layer-4>#</a></h4><p>We have delivered the packet to the correct host (HB), but when a packet reaches to its destination, that host might be running manny different programs, like a web server, an email server, etc. So, how do we know which program the packet is for?
This is the job of Layer 4 (L4), the Transport Layer. This L4 header specifies a port number. For example, if HA wants to reach HB&rsquo;s web server, it will set the destination port in the L4 header to 80.
When HB receives the L3 packet and unwraps it, it looks at this L4 header, sees &ldquo;port 80,&rdquo; and knows to deliver the data to its web server, not its email server. This is how a single IP address can serve many different services at the same time.</p><h3 id=switching-l2-domain>Switching (L2 Domain)<a hidden class=anchor aria-hidden=true href=#switching-l2-domain>#</a></h3><p>We&rsquo;ve covered the basics of a switch, but how does it actually manage traffic magically? This section explains the fundamental operations a switch performs.</p><p>As mentioned earlier, switches have multiple ports where devices (like PCs and servers) connect. To know where to send traffic, a switch builds and maintains a MAC address table. This table is crucial: it maps the MAC address of a connected device to the specific switch port it&rsquo;s on. This table is what allows the switch to intelligently deliver frames to the correct destination.</p><p>When a frame arrives on any port, the switch follows a simple but powerful three-step logic: Learn, Flood, and Forward.</p><blockquote><p><em>A Quick Note</em> that the &ldquo;Learn, Flood, and Forward&rdquo; model is the most fundamental concept of L2 switching. Please be aware that this overview explains only the basics. Real-world switches perform several other important functions, such as filtering frames (dropping a frame if the destination is on the same port it came from) and aging out (removing) old entries from the MAC table to keep it up-to-date.</p></blockquote><h4 id=learn>Learn<a hidden class=anchor aria-hidden=true href=#learn>#</a></h4><p>Switch maintains its MAC table with the port and updates it whenever a related frame passes through the switch.
For example, when Host A sends a frame to Host B, the initial MAC table state is empty. When the switch receives a traffic from Host A&rsquo;s MAC address (let&rsquo;s say AA) through port-a, switch learns and updates its MAC table to memorize <code>port: port-a and MAC: AA</code>.</p><h4 id=flood>Flood<a hidden class=anchor aria-hidden=true href=#flood>#</a></h4><p>Now, switch still does not know where to find Host B (which port should it use?). Therefore, the switch floods a unicast frame out all switch ports except the port it was received on. This is called flooding.
Once Host B receives the frame, it responds to the message through a port that it&rsquo;s connected to, let&rsquo;s say port-b. Now, switch receives this acknowledged response through port-b, and learns that <code>port: port-b and MAC: BB</code>.
Now, switch knows where to find Host B</p><h4 id=forward>FORWARD<a hidden class=anchor aria-hidden=true href=#forward>#</a></h4><p>The rest is a bit trivial actually. The switch knows ports and MAC addresses. It performs forwarding to deliver the frame. The Forward operation is the main job of the switch once its MAC table has information.</p><p>After the switch &ldquo;learns&rdquo; that Host B is on port-b, the &ldquo;Flood&rdquo; process is no longer needed to reach it. The next time Host A sends a frame to Host B, the switch receives it, looks at the Destination MAC address (BB), and checks its MAC table. It finds the entry for it <code>port: port-b and MAC: BB</code> and forwards the frame only out port-b. It drastically reduces network traffic and is the primary reason switches are much more efficient than old hubs.</p><h3 id=routing-l3-domain>Routing (L3 Domain)<a hidden class=anchor aria-hidden=true href=#routing-l3-domain>#</a></h3><p>Routing is the process of delivering data between different networks (inter-network communication) using a device called a router. This operation occurs at Layer 3 (L3) of the OSI model.
If a host on one network (like your home LAN) needs to communicate with a host on a different network (like the internet), it requires a router to forward the packet.</p><h4 id=routing-table>Routing Table<a hidden class=anchor aria-hidden=true href=#routing-table>#</a></h4><p>Routers maintain a map of all the networks they know about, which is known as routing table.
This table is essentially a set of rules, or routes, that tell the router which path to use to reach a specific network destination.</p><p>Example routing table contents, from <a href=https://en.wikipedia.org/wiki/Routing_table>https://en.wikipedia.org/wiki/Routing_table</a></p><table><thead><tr><th>Network destination</th><th>Netmask</th><th>Gateway</th><th>Interface</th><th>Metric</th></tr></thead><tbody><tr><td>0.0.0.0</td><td>0.0.0.0</td><td>192.168.0.1</td><td>192.168.0.100</td><td>10</td></tr><tr><td>127.0.0.0</td><td>255.0.0.0</td><td>127.0.0.1</td><td>127.0.0.1</td><td>1</td></tr><tr><td>192.168.0.0</td><td>255.255.255.0</td><td>192.168.0.100</td><td>192.168.0.100</td><td>10</td></tr><tr><td>192.168.0.100</td><td>255.255.255.255</td><td>127.0.0.1</td><td>127.0.0.1</td><td>10</td></tr><tr><td>192.168.0.1</td><td>255.255.255.255</td><td>192.168.0.100</td><td>192.168.0.100</td><td>10</td></tr></tbody></table><p>You might wonder how this table is created. Routes are typically added in three ways:</p><ul><li>Direct: the router automatically adds routes for networks it is physically connected to (e.g., the 192.168.0.0 entry).</li><li>Static: a fixed route is manually added.</li><li>Dynamic: routers peers each other (talks to each other) using routing protocols (like OSPF or BGP) to automatically learn about and share routes. This is essential in large, complex networks.</li></ul><p>If you have checked Kubernetes networking related documents, you may realize that routing protocols, especially BGP, are usually mentioned as an option in the network fabric. BGP (Border Gateway Protocol) allows different networks to exchange routing information automatically.
While it&rsquo;s an advanced topic, the key idea is automated route discovery, and we will see BGP again when we discuss advanced networking patterns in Kubernetes.
Also, if you are using managed services from cloud providers, as an end user, most of the time you do not need to work closely with BGP. This is because most cloud providers use an abstraction called &ldquo;overlay networks&rdquo; which we will also briefly see in the Kubernetes networking section. These overlays run on top of the provider&rsquo;s complex underlying network, which itself may use BGP heavily, but that complexity is hidden from you.</p><p>A key entry in many routing tables is the default gateway (shown as 0.0.0.0 in the example). This is the &ldquo;catch-all&rdquo; route. If the router doesn&rsquo;t have a specific route for a destination, it sends the packet to the default gateway if its defined. So, the default gateway is optional. So, based on the routing table, the kernel forwards data to the route.</p><h4 id=nat>NAT<a hidden class=anchor aria-hidden=true href=#nat>#</a></h4><p>NAT is a process where a router modifies the source and/or destination IP addresses in a packet&rsquo;s header on the fly as it passes through.</p><p>Publicly routable IP addresses (like the ones you get from your ISP) are a limited (due to the nature of unique IPv4 addresses) and costly resource.
In a typical home or office, your devices (laptops, phones) use private IP addresses (e.g., 192.168.x.x, 10.x.x.x). These addresses are not routable on the public internet. Only your main router has a single public IP address.</p><p>Then you may ask, how can your private device access the internet? And just as importantly, if a web server sends a response, how does it get back to your specific private device, which it can&rsquo;t see?</p><p>This is solved by two main types of NAT:</p><h5 id=snat-source-nat>SNAT (source NAT)<a hidden class=anchor aria-hidden=true href=#snat-source-nat>#</a></h5><p>SNAT is the process where source IP is changed. This is important because addresses in LAN are not reachable from WAN. Thus, even though they can send requests to internet, internet can&rsquo;t respond to those hosts. So, SNAT is being used for outbound connections, like when your laptop browses a website.</p><p>The process begins when your laptop (private host, e.g., 192.168.0.100) sends a packet to a web server (e.g., 8.8.8.8). This packet, with its private source IP, hits your router.
The router then performs SNAT by changing the packet&rsquo;s Source IP from 192.168.0.100 to its own public IP (e.g., 123.45.67.89). Also, it records this translation in a state table. The web server receives the packet from the public IP and sends its response back to that same public address. When your router receives this response, it checks its state table, sees the traffic belongs to 192.168.0.100, and translates the Destination IP back to your private host before forwarding it.</p><h5 id=dnat-destination-nat>DNAT (destination NAT)<a hidden class=anchor aria-hidden=true href=#dnat-destination-nat>#</a></h5><p>This is used for inbound connections, often called port forwarding.
Imagine you are hosting a web server on your private network at 192.168.0.200. Since someone on the internet can&rsquo;t reach this private address, you configure a DNAT rule on your router. This rule tells the router, &ldquo;Any traffic that arrives at my public IP (123.45.67.89) on a specific port (like port 80 for HTTP) should have its Destination IP changed to 192.168.0.200.&rdquo; This rule forwards the external request to your internal, private server, allowing it to host services for the public internet.</p><p>This entire stateful process requires the router to remember which internal host initiated which external connection. This mechanism is called connection tracking (or <code>conntrack</code> in Linux).</p><p>The <code>conntrack</code> system maintains an in-memory table of all active connections.
For SNAT, this table maps the original <code>[private_ip:port]</code> to the <code>[public_ip:port]</code> it was translated to.
Because this table is stored in memory, it has a limited size. This limit (e.g., <code>nf_conntrack_max</code> in Linux) can be reached if you have too many simultaneous active connections. It can also be a problem with a high rate of short-lived connections, as entries are kept in the table for a short time even after the connection closes.
If this table fills up, the router will start dropping new connections.</p><h2 id=linux-networking-single-host>Linux Networking (Single Host)<a hidden class=anchor aria-hidden=true href=#linux-networking-single-host>#</a></h2><p>This section covers the common Linux networking &ldquo;toolbox&rdquo; required to understand before jumping into Kubernetes networking.
We&rsquo;ll use standard Linux tools to build a complete, virtualized network inside a single host.
The concepts we explore network namespaces, veth and virtual bridges that are the building blocks used by container runtimes and CNI. A grasp of these operations and concepts is key to understanding how container networking operates at scale.</p><p>If you are already comfortable with container networking, feel free to skip ahead.</p><h3 id=isolation-network-namespaces>Isolation (network namespaces)<a hidden class=anchor aria-hidden=true href=#isolation-network-namespaces>#</a></h3><p>As explained in <a href=#layer-2>Layer 2</a> section, a host uses network interfaces (NICs) for network communication. These interfaces can be physical (like the actual hardware on your laptop) or virtual, which we will discuss soon.</p><p>When you start your Linux machine, the kernel runs a single, default networking stack for you to manage all networking processes. This &ldquo;root&rdquo; stack is for everything running on the Linux machine. If you need to isolate this stack, you should use network namespaces.</p><p>Like other Linux namespaces, network namespaces isolate the networking stack.
This allows us to spin up multiple containers on the same host while managing their networking stack differently, giving each one its own isolated network devices, firewall rules, ports, and route tables.</p><p>This isolation is very strong. When a network namespace is created, it comes with only a single, private loopback device (lo). It has no other interfaces, no routes, and no way to communicate with the host or the outside world. The rest of the configuration must be set by hand. To make this isolated &ldquo;box&rdquo; useful, we must provide it with a network connection. The most common method, and the one that powers container networking, is to use a virtual ethernet (veth) pair, which acts like a virtual patch cable.</p><p>The main tool we will use throughout this section is the <code>ip</code> command. The following is a quick demonstration of <code>ip</code> tool in Linux to manage network namespaces</p><blockquote><p>For a more detailed reference, the <code>ip</code> command suite is vast. RedHat provides a useful cheatsheet: <a href=https://access.redhat.com/sites/default/files/attachments/rh_ip_command_cheatsheet_1214_jcs_print.pdf>https://access.redhat.com/sites/default/files/attachments/rh_ip_command_cheatsheet_1214_jcs_print.pdf</a>.</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># adding a network namespace</span>
</span></span><span class=line><span class=cl>ip netns add &lt;namespace_name&gt;
</span></span><span class=line><span class=cl><span class=c1># executing a command in this namespace.</span>
</span></span><span class=line><span class=cl>ip netns <span class=nb>exec</span> &lt;namespace_name&gt; <span class=nv>$COMMAND</span>
</span></span></code></pre></td></tr></table></div></div><p>Let&rsquo;s run a quick demo to see this isolation in action. We&rsquo;ll create a namespace called &ldquo;container0&rdquo; and run a shell inside it:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ip netns add container0
</span></span><span class=line><span class=cl>ip netns list
</span></span><span class=line><span class=cl><span class=c1># container0 (id: 0)</span>
</span></span><span class=line><span class=cl>ip netns <span class=nb>exec</span> container0 /bin/bash
</span></span><span class=line><span class=cl>ip netns identify
</span></span><span class=line><span class=cl><span class=c1># this should print container0</span>
</span></span><span class=line><span class=cl><span class=nb>exit</span>
</span></span><span class=line><span class=cl><span class=c1># exit from container0 namespace</span>
</span></span><span class=line><span class=cl>ip netns identify
</span></span><span class=line><span class=cl><span class=c1># this should print nothing as we are in the root namespace</span>
</span></span><span class=line><span class=cl>ip netns del container0
</span></span></code></pre></td></tr></table></div></div><h3 id=network-device-abstraction-ip-link>Network Device abstraction (ip link)<a hidden class=anchor aria-hidden=true href=#network-device-abstraction-ip-link>#</a></h3><p>If you have come across <code>ip</code> commands before (or check the cheat sheet above), you may realize that <code>ip link</code> is widely used. This utility provides a great configuration layer for our isolated environment, by providing network device configuration.</p><p>In the last section, we used the <code>ip netns</code> command to manage the namespace itself.
Now, we&rsquo;ll use <code>ip link</code> to manage the network devices (or interfaces) inside the namespace.</p><blockquote><p>You&rsquo;ll often hear these called network interfaces, link devices, or just links.
These terms all refer to the same basic concept of a kernel object that can send or receive packets. They can have technical differences, but usually they refer to the similar functionality; sending and receiving data.</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns add container0 <span class=o>&amp;&amp;</span> ip netns add container1 <span class=o>&amp;&amp;</span> ip netns
</span></span><span class=line><span class=cl>container0
</span></span><span class=line><span class=cl>container1
</span></span></code></pre></td></tr></table></div></div><p>Now, let&rsquo;s use <code>ip link show</code> to see what network devices exist inside <code>container0</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip link show
</span></span><span class=line><span class=cl>1: lo: &lt;LOOPBACK&gt; mtu <span class=m>65536</span> qdisc noop state DOWN mode DEFAULT group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span></code></pre></td></tr></table></div></div><p>As you can see, the namespace isn&rsquo;t empty; it starts with a single device: <code>lo</code>, the loopback interface. This virtual device allows a host (or in this case, a namespace) to send network packets to itself.</p><p>If you examine this output, you&rsquo;ll see the loopback device&rsquo;s current state is <code>DOWN</code>. This means the device is not &ldquo;up&rdquo; or ready to function. But, what is the meaning of all of this? Why would you even care? Let&rsquo;s try something simple, like pinging the loopback IP address 127.0.0.1:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ping 127.0.0.1
</span></span><span class=line><span class=cl>ping: connect: Network is unreachable
</span></span></code></pre></td></tr></table></div></div><p>It fails. We get &ldquo;Network is unreachable&rdquo; because even though the <code>lo</code> device exists, it&rsquo;s turned off, and the kernel can&rsquo;t use it. Let&rsquo;s fix this by using the <code>ip link set</code> command to bring the device &ldquo;up&rdquo;:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip link <span class=nb>set</span> lo up
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ping -c <span class=m>1</span> 127.0.0.1
</span></span><span class=line><span class=cl>PING 127.0.0.1 <span class=o>(</span>127.0.0.1<span class=o>)</span> 56<span class=o>(</span>84<span class=o>)</span> bytes of data.
</span></span><span class=line><span class=cl><span class=m>64</span> bytes from 127.0.0.1: <span class=nv>icmp_seq</span><span class=o>=</span><span class=m>1</span> <span class=nv>ttl</span><span class=o>=</span><span class=m>64</span> <span class=nv>time</span><span class=o>=</span>0.082 ms
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- 127.0.0.1 ping statistics ---
</span></span><span class=line><span class=cl><span class=m>1</span> packets transmitted, <span class=m>1</span> received, 0% packet loss, <span class=nb>time</span> 0ms
</span></span><span class=line><span class=cl>rtt min/avg/max/mdev <span class=o>=</span> 0.082/0.082/0.082/0.000 ms
</span></span></code></pre></td></tr></table></div></div><p>It works. We have just configured our first network device. This <code>lo</code> interface is essential, but it only lets the namespace talk to itself. To talk to other namespaces or the outside world, we need to add new devices.</p><h3 id=the-cable-veth-pairs>The cable (veth pairs)<a hidden class=anchor aria-hidden=true href=#the-cable-veth-pairs>#</a></h3><p>Virtual Ethernet, or veth, does what we are looking for: it connects our namespace to another namespace.</p><p>In our above example, let&rsquo;s assume we want to allow communication between <code>container0</code> and <code>container1</code>. What can we do?
Remember the concepts mentioned in <a href=#networking>Networking</a> section.
The simplest way to connect hosts was connecting them together with patch cable.
However, we now operate in the virtualized world. How could we make it? In the virtualized world, <code>veth</code> achieves this for us.</p><p>A virtual ethernet device, or veth for short, is a link device that is created as a pair, just like a patch cable.
When a packet is transmitted on one end of veth, it is received from the other end.
Thanks to this feature, we use a veth pair to connect container0 and container1 directly.</p><pre tabindex=0><code>+------------------------------------------------+
|                                                |
|       container0              container1       |
|    +--------------+        +--------------+    |
|    |              |        |              |    |
|    |              |        |              |    |
|    |              |        |              |    |
|    +------+-------+        +------+-------+    |
|           ^                       ^            |
|           |                       |            |
|           |         veth          |            |
|           +-----------------------+            |
|                                                |
+------------------------------------------------+
</code></pre><p>The above diagram is what we want to achieve, so that we can ping container0 to container1.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ip link add veth0 <span class=nb>type</span> veth peer name veth1
</span></span></code></pre></td></tr></table></div></div><p>The command may look ugly, but if we read it as:</p><pre tabindex=0><code>&lt;ip link add veth0&gt; &lt;type veth&gt; &lt;peer name veth1&gt;
</code></pre><p>It makes it a bit clearer to understand. It means we want to add a link device called veth0, whose type is veth.
Since it&rsquo;s going to be a veth, we need to give a name to its peer, or in other words, its other end.</p><p>Where should we run this command? Within the container0 or container1 namespaces, or on the host?
Well, we usually create the veth pair in the parent namespace (the host, in our case) and then move each end into its corresponding namespace.</p><p>Now, let&rsquo;s list what link devices we have in the host network namespace:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns identify
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ ip link show
</span></span><span class=line><span class=cl>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class=m>65536</span> qdisc noqueue state UNKNOWN mode DEFAULT group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span class=line><span class=cl>49: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu <span class=m>1500</span> qdisc noop state DOWN mode DEFAULT group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/ether ea:de:62:55:a0:7a brd ff:ff:ff:ff:ff:ff
</span></span><span class=line><span class=cl>50: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu <span class=m>1500</span> qdisc noop state DOWN mode DEFAULT group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/ether 56:10:cf:46:97:1c brd ff:ff:ff:ff:ff:ff
</span></span></code></pre></td></tr></table></div></div><p>As you can see, the veth pair is there, and both devices are state <code>DOWN</code>
Now, we&rsquo;ll move one end into <code>container0</code> and the other into <code>container1</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip link <span class=nb>set</span> veth0 netns container0 <span class=o>&amp;&amp;</span> ip link <span class=nb>set</span> veth1 netns container1
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip addr show
</span></span><span class=line><span class=cl>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class=m>65536</span> qdisc noqueue state UNKNOWN group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span class=line><span class=cl>    inet 127.0.0.1/8 scope host lo
</span></span><span class=line><span class=cl>       valid_lft forever preferred_lft forever
</span></span><span class=line><span class=cl>    inet6 ::1/128 scope host
</span></span><span class=line><span class=cl>       valid_lft forever preferred_lft forever
</span></span><span class=line><span class=cl>50: veth0@if49: &lt;BROADCAST,MULTICAST&gt; mtu <span class=m>1500</span> qdisc noop state DOWN group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/ether 56:10:cf:46:97:1c brd ff:ff:ff:ff:ff:ff link-netns container1
</span></span></code></pre></td></tr></table></div></div><blockquote><p>The IPs for network devices are visible in <code>ip addr show</code> output. If you check <code>inet</code> field, its the IPv4 address, and <code>inet6</code> corresponds to IPv6 address.</p></blockquote><p>Okay, now both containers are connected. Perfect, can we now ping the container1 from container0?
Well, which address should we use? There is no IP address for the containers at the moment.
What should get an IP address; namespace, the veth or something else?
How can we assign an IP address?</p><p>Well, based on our discussion in the <a href=#networking>Networking</a> section, we assumed hosts (devices) have IP addresses. So, we assign IP to network interfaces to allow communication through IP.
In our case, the &ldquo;device&rdquo; is the veth pair.
Since <code>veth</code> is a link device (a network interface), we can assign an IP to it and ping that IP.
Assigning IPs can be done via the <code>ip</code> command.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip addr add 10.0.1.50/24 dev veth0 
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container1 ip addr add 10.0.1.59/24 dev veth1
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip link <span class=nb>set</span> dev veth0 up <span class=o>&amp;&amp;</span> ip netns <span class=nb>exec</span> container1 ip link <span class=nb>set</span> dev veth1 up
</span></span></code></pre></td></tr></table></div></div><p>This command assigns 10.0.1.50/24 to <code>veth0</code> and 10.0.1.59/24 to <code>veth1</code>.
Since both of these IPs are in the same 10.0.1.0/24 subnet, the kernel will know they can reach each other directly over the <code>veth</code> &ldquo;cable&rdquo; through L2 network, without routing.</p><p>By calling <code>ip link set dev veth0 up</code>, we are setting the link (device, network interface) up, or ready to function.</p><p>Let&rsquo;s check the IP addresses:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip addr show veth0
</span></span><span class=line><span class=cl>50: veth0@if49: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class=m>1500</span> qdisc noqueue state UP group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/ether 56:10:cf:46:97:1c brd ff:ff:ff:ff:ff:ff link-netns container1
</span></span><span class=line><span class=cl>    inet 10.0.1.50/24 scope global veth0
</span></span><span class=line><span class=cl>       valid_lft forever preferred_lft forever
</span></span><span class=line><span class=cl>    inet6 fe80::5410:cfff:fe46:971c/64 scope link
</span></span><span class=line><span class=cl>       valid_lft forever preferred_lft forever
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container1 ip addr show veth1
</span></span><span class=line><span class=cl>49: veth1@if50: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class=m>1500</span> qdisc noqueue state UP group default qlen <span class=m>1000</span>
</span></span><span class=line><span class=cl>    link/ether ea:de:62:55:a0:7a brd ff:ff:ff:ff:ff:ff link-netns container0
</span></span><span class=line><span class=cl>    inet 10.0.1.59/24 scope global veth1
</span></span><span class=line><span class=cl>       valid_lft forever preferred_lft forever
</span></span><span class=line><span class=cl>    inet6 fe80::e8de:62ff:fe55:a07a/64 scope link
</span></span><span class=line><span class=cl>       valid_lft forever preferred_lft forever
</span></span></code></pre></td></tr></table></div></div><p>Based on <code>inet</code> output, we can see that <code>veth</code> devices have IP addresses as we defined above.</p><p>Now, we should test if container0 can reach container1 by simply pinging IP addresses within the container network namespaces:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ping -c <span class=m>1</span> 10.0.1.59 <span class=c1># pinging container1 within container0</span>
</span></span><span class=line><span class=cl>PING 10.0.1.59 <span class=o>(</span>10.0.1.59<span class=o>)</span> 56<span class=o>(</span>84<span class=o>)</span> bytes of data.
</span></span><span class=line><span class=cl><span class=m>64</span> bytes from 10.0.1.59: <span class=nv>icmp_seq</span><span class=o>=</span><span class=m>1</span> <span class=nv>ttl</span><span class=o>=</span><span class=m>64</span> <span class=nv>time</span><span class=o>=</span>0.031 ms
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- 10.0.1.59 ping statistics ---
</span></span><span class=line><span class=cl><span class=m>1</span> packets transmitted, <span class=m>1</span> received, 0% packet loss, <span class=nb>time</span> 0ms
</span></span><span class=line><span class=cl>rtt min/avg/max/mdev <span class=o>=</span> 0.031/0.031/0.031/0.000 ms
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container1 ping -c <span class=m>1</span> 10.0.1.50 <span class=c1># pinging container0 within container1</span>
</span></span><span class=line><span class=cl>PING 10.0.1.50 <span class=o>(</span>10.0.1.50<span class=o>)</span> 56<span class=o>(</span>84<span class=o>)</span> bytes of data.
</span></span><span class=line><span class=cl><span class=m>64</span> bytes from 10.0.1.50: <span class=nv>icmp_seq</span><span class=o>=</span><span class=m>1</span> <span class=nv>ttl</span><span class=o>=</span><span class=m>64</span> <span class=nv>time</span><span class=o>=</span>0.300 ms
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- 10.0.1.50 ping statistics ---
</span></span><span class=line><span class=cl><span class=m>1</span> packets transmitted, <span class=m>1</span> received, 0% packet loss, <span class=nb>time</span> 0ms
</span></span><span class=line><span class=cl>rtt min/avg/max/mdev <span class=o>=</span> 0.300/0.300/0.300/0.000 ms
</span></span></code></pre></td></tr></table></div></div><p>It works. We have successfully created our first virtual network by connecting two isolated namespaces with a veth pair.</p><h3 id=the-virtual-switch-bridge>The Virtual Switch (bridge)<a hidden class=anchor aria-hidden=true href=#the-virtual-switch-bridge>#</a></h3><p>Having two containers and managing a direct connection between them is not a big deal. We just need to perform a couple of commands, and the two isolated network namespaces are ready to communicate.</p><p>However, think about scaling this system. If you have many containers, connecting them to each other directly will not scale. Does this issue sound familiar? Remember one of the benefits of switches. In the physical world, switches solve this exact problem. Instead of connecting hosts directly to each other, we connect all hosts to a central switch, which then handles communication between them.</p><p>A similar solution exists in the virtualized environment, and in Linux, it&rsquo;s called a bridge.</p><p>A Linux bridge is a Layer 2 device that behaves exactly like a virtual switch. It builds a MAC address table and forwards frames between the devices connected to it. Because it&rsquo;s L2, it does not know about IP addresses or routes.</p><p>From the host&rsquo;s point of view, the bridge is just another network device (a link). From the containers&rsquo; point of view, it&rsquo;s the &ldquo;switch&rdquo; they are all plugged into.</p><p>Our old setup was a direct veth &ldquo;patch cable&rdquo; between two containers:
<img loading=lazy src=/images/cni-host-veth.png#center alt=veth-pair></p><p>Our new setup will look like this, with a central br0 bridge:
<img loading=lazy src=/images/cni-containers-bridge.png#center alt=linux-bridge></p><p>In this setup, introducing a new container or removing a new container (network namespaces) become quite easy. We just need to perform operations that we explained above.
But instead of connecting veth pairs to host network, we connect one end of veth pair to bridge, and the another to container itself.</p><p>To set this up, we&rsquo;ll create a br0 bridge device in the host namespace.
Then, for each container, we&rsquo;ll create a veth pair.
One end will go inside the container namespace (and get the IP address), while the other end will be &ldquo;plugged into&rdquo; the bridge as a port.</p><p>Let&rsquo;s build this. First, we need to create the bridge device. We&rsquo;ll use the ip command again.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># create a device of type &#39;bridge&#39;, named &#39;br0&#39;</span>
</span></span><span class=line><span class=cl>ip link add br0 <span class=nb>type</span> bridge
</span></span><span class=line><span class=cl>ip link <span class=nb>set</span> br0 up
</span></span></code></pre></td></tr></table></div></div><p>That&rsquo;s it. We now have a virtual switch that is on, but has nothing plugged into it.
Now, let&rsquo;s clean up our old setup and connect our containers to this new bridge.</p><blockquote><p>Maybe you may want to try it yourself, by following the commands in the previous section.</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># clean up the old veth pair</span>
</span></span><span class=line><span class=cl><span class=c1># note that deleting one end also deletes the peer</span>
</span></span><span class=line><span class=cl>ip netns <span class=nb>exec</span> container0 ip link del veth0
</span></span><span class=line><span class=cl><span class=c1># create new veth pairs for container0 and container1</span>
</span></span><span class=line><span class=cl><span class=c1># (c0-veth, c0-br) -&gt; for container0</span>
</span></span><span class=line><span class=cl><span class=c1># (c1-veth, c1-br) -&gt; for container1</span>
</span></span><span class=line><span class=cl>ip link add c0-veth <span class=nb>type</span> veth peer name c0-br
</span></span><span class=line><span class=cl>ip link add c1-veth <span class=nb>type</span> veth peer name c1-br
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># move the &#39;veth&#39; end into the namespaces</span>
</span></span><span class=line><span class=cl>ip link <span class=nb>set</span> c0-veth netns container0
</span></span><span class=line><span class=cl>ip link <span class=nb>set</span> c1-veth netns container1
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># attach the &#39;br&#39; veth end to the bridge</span>
</span></span><span class=line><span class=cl><span class=c1># this is like plugging the cable into the switch</span>
</span></span><span class=line><span class=cl>ip link <span class=nb>set</span> c0-br master br0
</span></span><span class=line><span class=cl>ip link <span class=nb>set</span> c1-br master br0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># add IP addresses to veth pairs inside the containers</span>
</span></span><span class=line><span class=cl>ip netns <span class=nb>exec</span> container0 ip addr add 10.0.1.50/24 dev c0-veth
</span></span><span class=line><span class=cl>ip netns <span class=nb>exec</span> container1 ip addr add 10.0.1.59/24 dev c1-veth
</span></span><span class=line><span class=cl>ip netns <span class=nb>exec</span> container0 ip link <span class=nb>set</span> dev c0-veth up
</span></span><span class=line><span class=cl>ip netns <span class=nb>exec</span> container1 ip link <span class=nb>set</span> dev c1-veth up
</span></span><span class=line><span class=cl><span class=c1># also do not forget to set &#39;br&#39; veth pair ends UP</span>
</span></span><span class=line><span class=cl>ip link <span class=nb>set</span> c0-br up
</span></span><span class=line><span class=cl>ip link <span class=nb>set</span> c1-br up
</span></span></code></pre></td></tr></table></div></div><p>Now the setup is complete. We expect the similar behaviour, where containers can ping each other.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container1 ping -c <span class=m>1</span> 10.0.1.50
</span></span><span class=line><span class=cl>PING 10.0.1.50 <span class=o>(</span>10.0.1.50<span class=o>)</span> 56<span class=o>(</span>84<span class=o>)</span> bytes of data.
</span></span><span class=line><span class=cl><span class=m>64</span> bytes from 10.0.1.50: <span class=nv>icmp_seq</span><span class=o>=</span><span class=m>1</span> <span class=nv>ttl</span><span class=o>=</span><span class=m>64</span> <span class=nv>time</span><span class=o>=</span>0.092 ms
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- 10.0.1.50 ping statistics ---
</span></span><span class=line><span class=cl><span class=m>1</span> packets transmitted, <span class=m>1</span> received, 0% packet loss, <span class=nb>time</span> 0ms
</span></span><span class=line><span class=cl>rtt min/avg/max/mdev <span class=o>=</span> 0.092/0.092/0.092/0.000 ms
</span></span></code></pre></td></tr></table></div></div><p>It works! You may wonder the purpose of all hassle by setting up the bridge.
Since bridges work like virtual switches, adding a new container to this setup becomes easier than before.
We just need to add <code>veth</code> pairs, connect one end of the pair to container and another end to the bridge, assign IP address and set everything UP. Now the new container also can communicate with other containers, so we do not need to update every <code>veth</code> pair in other containers.</p><p>We have now built a scalable, single-host virtual network, which is the foundation of how Kubernetes and CNI work.</p><h3 id=l3-gateway>L3 Gateway<a hidden class=anchor aria-hidden=true href=#l3-gateway>#</a></h3><p>Our containers can communicate with each other. That&rsquo;s perfect for offline environments. But most of the time, we also expect containers to reach other networks, especially the Internet.</p><p>Let&rsquo;s check whether the containers can communicate with the Internet.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ping -c <span class=m>1</span> 8.8.8.8
</span></span><span class=line><span class=cl>ping: connect: Network is unreachable
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container1 ping -c <span class=m>1</span> 8.8.8.8
</span></span><span class=line><span class=cl>ping: connect: Network is unreachable
</span></span></code></pre></td></tr></table></div></div><p>Even though containers can ping each other, they cannot communicate with the Internet.</p><p>Remember, in the previous sections, we explained how packet delivery happens: L2 ensures host-to-host (or host-to-router) delivery, while L3 ensures end-to-end delivery.
What are we missing here?</p><p>The simple answer is that the container network namespaces do not know any path to reach 8.8.8.8. Therefore, they cannot deliver the ping request (ICMP packets). We need to add a &lsquo;route&rsquo; to instruct our system to find the path. In our host machine, we are able to ping the Internet. It seems the host namespace contains a route allowing it to access the internet, and this route does not exist in the containers.</p><p>In our host machine, we are able to ping the Internet. It seems the host namespace contains a route allowing it to access the internet, and this route does not exist in the containers.</p><p>To find out the reasons, we should look at how we reach the Internet from the host:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns identify <span class=o>&amp;&amp;</span> ip route get 8.8.8.8
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>8.8.8.8 via 192.168.215.1 dev eth0 src 192.168.215.2 uid <span class=m>0</span>
</span></span><span class=line><span class=cl>    cache
</span></span></code></pre></td></tr></table></div></div><p><code>ip route get &lt;address></code> command allows us to check which route the host takes while reaching &lt;address>.
In our case, we reach 8.8.8.8 via the <code>eth0</code> device. If you list all routes, you&rsquo;ll see why:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip route
</span></span><span class=line><span class=cl>default via 192.168.215.1 dev eth0
</span></span><span class=line><span class=cl>192.168.215.0/24 dev eth0 proto kernel scope link src 192.168.215.2
</span></span></code></pre></td></tr></table></div></div><p>As we mentioned in <a href=#routing-table>Routing Table</a> section, the first entry is the default gateway, which is used if no other rule is matched.
By using this rule, we are able to ping the Internet. We need to add a similar route inside the container namespaces.</p><p>Let&rsquo;s first check the full routing table inside container0:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip route
</span></span><span class=line><span class=cl>10.0.1.0/24 dev c0-veth proto kernel scope link src 10.0.1.50
</span></span><span class=line><span class=cl>$ ip route get 8.8.8.8
</span></span><span class=line><span class=cl>RTNETLINK answers: Network is unreachable
</span></span></code></pre></td></tr></table></div></div><p>This confirms our suspicion. The only rule it knows is the &ldquo;direct&rdquo; route for its local subnet. When we ask it to find 8.8.8.8, it doesn&rsquo;t match this rule, and there&rsquo;s no &ldquo;default&rdquo; to fall back on.</p><p>So, how do we give the containers a default route? We need to give them a gateway (a router) that they can send all non-local traffic to. The perfect candidate for this gateway is our own <code>br0</code> bridge.</p><p>You might ask, why the bridge? In our last example, <code>br0</code> was just a simple L2 switch and didn&rsquo;t have an IP at all.
This is the key difference: to act as an L3 gateway, a device should have an IP address.
A fundamental rule of IP networking is that a host&rsquo;s gateway must be on the same local subnet. This is so the container (e.g., 10.0.1.50) can use ARP to find the gateway&rsquo;s MAC address.
Therefore, we will turn our L2 switch into an L3 gateway by assigning it an IP address on the containers&rsquo; subnet, 10.0.1.0/24.</p><blockquote><p>If the bridge were only acting as a simple L2 switch for a network that didn&rsquo;t need to talk to the outside world, it wouldn&rsquo;t need an IP.</p></blockquote><p>We&rsquo;ll use 10.0.1.1 as the gateway IP. This will be the &ldquo;internal&rdquo; IP of our router, reachable by both containers.</p><blockquote><p>It is not mandatory but convention to give <code>a.b.c.1</code> IP addresses to the Gateway.</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip addr add 10.0.1.1/24 dev br0
</span></span><span class=line><span class=cl>$ ip addr show br0 <span class=p>|</span> grep -e <span class=s2>&#34;inet &#34;</span>
</span></span><span class=line><span class=cl>    inet 10.0.1.1/24 scope global br0
</span></span></code></pre></td></tr></table></div></div><p>Now, lets also instruct container network stack to route any non-local packets to bridge IP.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip route add default via 10.0.1.1
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container1 ip route add default via 10.0.1.1
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># let&#39;s check the route table for container0 again</span>
</span></span><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ip route
</span></span><span class=line><span class=cl>default via 10.0.1.1 dev c0-veth
</span></span><span class=line><span class=cl>10.0.1.0/24 dev c0-veth proto kernel scope link src 10.0.1.50
</span></span></code></pre></td></tr></table></div></div><p>Now container know that any traffic not destined for 10.0.1.0/24 should be sent to the gateway at 10.0.1.1. Let&rsquo;s test the connection:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ping -c <span class=m>1</span> 8.8.8.8
</span></span><span class=line><span class=cl>PING 8.8.8.8 <span class=o>(</span>8.8.8.8<span class=o>)</span> 56<span class=o>(</span>84<span class=o>)</span> bytes of data.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- 8.8.8.8 ping statistics ---
</span></span><span class=line><span class=cl><span class=m>1</span> packets transmitted, <span class=m>0</span> received, 100% packet loss, <span class=nb>time</span> 0ms
</span></span></code></pre></td></tr></table></div></div><p>It still fails! Why?</p><p>This may be confusing, since our containers can talk to each other.
That&rsquo;s because the traffic between namespaces is a Layer 2 (switching) operation. The <code>br0</code> bridge acts like a physical switch, and the packets never leave the bridge to be routed. The host&rsquo;s main L3 (IP) routing engine isn&rsquo;t involved.
If you remember the switching operations (learn, flood, forward) in <a href=#switching-l2-domain>Switching</a> section, you may remember that Switch actually knows which MAC addresses are connected to which ports.
This operation only applies to within network traffic as we explained in the first section.</p><p>However, ping 8.8.8.8 is a Layer 3 (routing) operation. The container sends the packet to its gateway (the <code>br0</code> interface), meaning that we need to reach another network. As you remember, this requires <a href=#routing-l3-domain>routing</a> as we need to move data between networks.
The host&rsquo;s kernel receives this packet on <code>br0</code> and, after checking its own route table, sees it must send it out a different interface (like <code>eth0</code>).</p><p>This act of receiving a packet on one interface and forwarding it to another is called L3 forwarding. By default, the Linux kernel disables this for security, so a machine doesn&rsquo;t accidentally act as a router. We must explicitly enable this by setting <code>net.ipv4.ip_forward=1</code>.</p><p>That&rsquo;s the first problem.
The second problem is <a href=#nat>NAT</a>. The packet from <code>container0</code> has a source IP of 10.0.1.50.
The internet doesn&rsquo;t know how to send a reply to this private IP.</p><p>We must use <code>iptables</code> to &ldquo;masquerade&rdquo; the packet, rewriting its source IP to the host&rsquo;s public IP (192.168.215.2).</p><p>Okay, you might think, what is <code>iptables</code> since we never mentioned it yet.</p><p>In simple words, <code>iptables</code> is a program that allows us to set rules for packets.
You can create <code>iptables</code> rules to filter, modify, or redirect packets.
It&rsquo;s a powerful tool for building firewalls, and it&rsquo;s also commonly used to implement NAT because its rules allow &ldquo;mangling&rdquo; (updating) the packet itself.</p><blockquote><p><code>iptables</code> is a complex tool, and while it&rsquo;s now being replaced by modern alternatives, its concepts are still fundamental to Kubernetes networking.</p></blockquote><p>So, let&rsquo;s update our host machine to enable IP forwarding:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sysctl -w net.ipv4.ip_forward<span class=o>=</span><span class=m>1</span>
</span></span></code></pre></td></tr></table></div></div><p>And, we need to add an <code>iptables</code> NAT rule.
This command is a bit long, but what it means is that &ldquo;For any packet in the <code>nat</code> table, in the <code>POSTROUTING</code> (after-routing) chain, if it&rsquo;s from our container subnet (<code>-s 10.0.1.0/24</code>)
and going out the <code>-o eth0</code> interface, then <code>-j MASQUERADE</code> (change its source IP to <code>eth0</code>&rsquo;s IP).&rdquo;</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>iptables -t nat -A POSTROUTING -s 10.0.1.0/24 -o eth0 -j MASQUERADE
</span></span></code></pre></td></tr></table></div></div><p>Now, all the pieces are in place. Let&rsquo;s try one last time:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ip netns <span class=nb>exec</span> container0 ping -c <span class=m>1</span> 8.8.8.8
</span></span><span class=line><span class=cl>PING 8.8.8.8 <span class=o>(</span>8.8.8.8<span class=o>)</span> 56<span class=o>(</span>84<span class=o>)</span> bytes of data.
</span></span><span class=line><span class=cl><span class=m>64</span> bytes from 8.8.8.8: <span class=nv>icmp_seq</span><span class=o>=</span><span class=m>1</span> <span class=nv>ttl</span><span class=o>=</span><span class=m>106</span> <span class=nv>time</span><span class=o>=</span>29.4 ms
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- 8.8.8.8 ping statistics ---
</span></span><span class=line><span class=cl><span class=m>1</span> packets transmitted, <span class=m>1</span> received, 0% packet loss, <span class=nb>time</span> 0ms
</span></span><span class=line><span class=cl>rtt min/avg/max/mdev <span class=o>=</span> 29.362/29.362/29.362/0.000 ms
</span></span></code></pre></td></tr></table></div></div><p>Finally it works! We have successfully connected our isolated containers to the internet.</p><h2 id=kubernetes-networking--cni>Kubernetes Networking & CNI<a hidden class=anchor aria-hidden=true href=#kubernetes-networking--cni>#</a></h2><p>It&rsquo;s been a long journey but here we are, Kubernetes networking.
We are not going to dive into too much details about technical requirements of the Kubernetes networking.
Kubernetes&rsquo; official documentation clearly explains the problems that we need to address, as follows:</p><blockquote><ol><li>Highly-coupled container-to-container communications: this is solved by <a href=https://kubernetes.io/docs/concepts/workloads/pods/>Pods</a> and <code>localhost</code> communications.</li><li>Pod-to-Pod communications: this is the primary focus of this document.</li><li>Pod-to-Service communications: this is covered by <a href=https://kubernetes.io/docs/concepts/services-networking/service/>Services</a>.</li><li>External-to-Service communications: this is also covered by Services.&rdquo;</li></ol><p><a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>https://kubernetes.io/docs/concepts/cluster-administration/networking/</a></p></blockquote><h3 id=motivation-and-automation-of-linux-networking>Motivation and Automation of Linux networking<a hidden class=anchor aria-hidden=true href=#motivation-and-automation-of-linux-networking>#</a></h3><p>In the last section, we built a single-host virtual network.
We manually created namespaces, veth pairs, a bridge, and configured routing.
The goal of Kubernetes networking is to automate this entire process at a massive, multi-node scale.</p><p>One of the core principle of Kubernetes networking is a unique IP per pod model.
To understand why this is so important, imagine scaling a web application.
You need three replicas, and they all want to listen on port 80.
On a single machine, this is impossible, as only one process can bind to a port at a time.
This creates a problem of coordinating ports for all your microservices.</p><p>Kubernetes solves this by giving every Pod its own isolated network namespace and a unique IP address.
Because each Pod has its own network stack (through a dedicated network namespace),
there are no port conflicts.
Your three replicas can all bind to port 80, each on its own IP (e.g., 10.244.1.10:80, 10.244.2.20:80, 10.244.3.30:80).
This simplifies application development and management, as you no longer need to manage port assignments.</p><blockquote><p><em>Note</em>: A Pod can also use <code>hostNetwork: true</code>, which skips this process, doesn&rsquo;t get its own IP, and just shares the node&rsquo;s network, but this is for special workloads.</p></blockquote><p>To make this model work, Kubernetes requires an L3 network.
This means all Pod IPs must be reachable from all other Pod IPs, no matter which node they are on.
Therefore, this is quite important and technical aspect that we didn&rsquo;t cover it yet.</p><p>The first problem Kubernetes networking solves is connectivity:
&ldquo;How do we give a Pod its own network namespace and a unique IP, and then make it reachable by other Pods?&rdquo;.</p><p>Another problem is discovery of Pods. Pods are ephemeral; they can be destroyed and replaced at any time, getting a new IP.
This means you can&rsquo;t rely on a Pod&rsquo;s IP address. This is solved by a Kubernetes Service.
A Service provides a single, stable virtual IP and a DNS name (e.g., my-service.prod.svc.cluster.local).
There is an agent running on each node to manage host level networking and watches the Kubernetes API and updates rules on the host (using <code>iptables</code>, IPVS, or nftables) to map the Service IP to the real Pod IPs. Other tools like Cilium can also do this as kube-proxy replacement, by using eBPF.</p><p>We also have more problems, for instance reachability, which will be discussed in the next chapter</p><h3 id=cni-container-networking-interface>CNI (Container Networking Interface)<a hidden class=anchor aria-hidden=true href=#cni-container-networking-interface>#</a></h3><p>CNI is a specification that tells the container runtime to call a &ldquo;plugin&rdquo; (or program) to handle the network setup for containers.</p><p>It solves the problem of &ldquo;giving a Pod its own network namespace and a unique IP, and then making it reachable by the cluster&rdquo;.
Therefore, we can say that CNI helps to create an isolated network for pod, to allocate the Pod IP, and make the IP reachable by the cluster.</p><p>Our demo in Part 2 where we manually created <code>veth</code> pairs, a bridge, and <code>iptables</code> rules is a good illustration of how one of the most common CNI plugins (the <code>bridge</code> plugin) works under the hood.
Other CNI plugins might solve this problem in completely different ways, such as using eBPF, IPVLAN, or different routing techniques.</p><blockquote><p>We should indicate that CNI is not only for Kubernetes, it&rsquo;s a generic specification for container networking. Kubernetes in this scenario is just an orchestrator or runtime, which triggers CNI plugins, to set up a network environment for the Pod.</p></blockquote><h4 id=cni-plugin>CNI Plugin<a hidden class=anchor aria-hidden=true href=#cni-plugin>#</a></h4><p>According to CNI specification, &ldquo;plugin is a program that applies a specified network configuration&rdquo;. Roughly, the container runtime calls CNI plugin, meaning CNI program, to prepare the container networking.
So, if we move the codes that we use in the demo into a bash script and make it executable, it can be
good starting point for us to write our own first &ldquo;plugin&rdquo;.</p><blockquote><p>Though i still do not understand the motivation behind &rsquo;executable&rsquo; plugins, instead of RPC based plugins like other Kubernetes out-tree interfaces (CRI or CSI).</p></blockquote><p>The good part about CNI specification is that it allows chaining the plugins. Meaning that you can develop and use existing CNI plugins, and combine them to come up with your own container networking solutions.
For example, you can use &ldquo;ipam&rdquo; plugin along with &ldquo;bridge&rdquo; plugin, so that &ldquo;bridge&rdquo; plugin can create
the demo environment that we set and &ldquo;ipam&rdquo; plugin can manage the IP address assignment to those resources.</p><p>Since Kubernetes expects more than Pod connectivity, CNI Plugins are usually shipped with additional component(s).</p><ul><li>The <em>binary</em> is usually the one that we call the plugin. So, its main purpose is configuring the pod network interface, which is for &ldquo;connectivity&rdquo;.</li><li><em>Daemon</em> is managing the routing. So, its mainly for &ldquo;reachability&rdquo;</li></ul><p>There are various ways to ensure &ldquo;connectivity&rdquo; and &ldquo;reachability&rdquo;, and we saw one possible solution for &ldquo;connectivity&rdquo;, in our bridge example.</p><p>&ldquo;Reachability&rdquo; though require more work. The CNI specification itself does not involve reachability; it covers the container networking (connectivity) and some runtime (e.g, Kubernetes) rules while calling CNI plugins.
Therefore, reachability is a bit specific to Kubernetes networking requirements.</p><p>The purpose of the reachability is ensuring every Pod is accessible from every node. So, it&rsquo;s a bit related to route lifecycle; thus, we should somehow announce the routes to each pod to each node.
Therefore, Kubernetes CNI solutions ship with the &lsquo;daemon&rsquo; running on each node to ensure that the network is configured in a way that all Pods are accessible across nodes.</p><p>How could we make it possible? As we mentioned in previous section, if nodes are in the same L2 network,
we could utilize Switch + Routers. But the common ways to solve this are &lsquo;Overlay Networks&rsquo; (e.g, VXLAN or IP-in-IP) or &lsquo;Routing Protocols&rsquo; (BGP or OSPF).</p><blockquote><p>There might be other hacky solutions as well; but these are the most common ones I faced.</p></blockquote><h3 id=how-cni-plugins-called>How CNI Plugins Called<a hidden class=anchor aria-hidden=true href=#how-cni-plugins-called>#</a></h3><p>The CNI (Container Network Interface) specification defines how a container runtime, like Kubernetes, interacts with network plugins. This guide covers the fundamental operations and concepts you need to know.
If you are looking for more details, please take a look at the actual specification (<a href=https://github.com/containernetworking/cni/blob/main/SPEC.md)>https://github.com/containernetworking/cni/blob/main/SPEC.md)</a>.</p><p>A runtime calls a CNI plugin by providing two key inputs: a JSON network configuration (via STDIN) and a set of environment variables.</p><h4 id=network-configuration-file>Network Configuration File<a hidden class=anchor aria-hidden=true href=#network-configuration-file>#</a></h4><p>The runtime finds network configurations by searching a dedicated directory, typically <code>/etc/cni/net.d</code>. Kubernetes, for example, monitors this directory periodically for configuration files.
This directory can be configured; therefore, in your cluster, it might be located somewhere else.</p><p>This JSON file contains settings for the runtime and for any plugins it needs to call, such as <code>bridge</code> or <code>ipam</code>.</p><p>Here is an example of a simple network configuration:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;cniVersion&#34;</span><span class=p>:</span> <span class=s2>&#34;1.1.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;test&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;bridge&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;isDefaultGateway&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;ipMasq&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;ipam&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;host-local&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;subnet&#34;</span><span class=p>:</span> <span class=s2>&#34;10.244.0.0/24&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>When the runtime reads this file, it identifies the main plugin to run from the &ldquo;type&rdquo; field.
In this case, it&rsquo;s <code>bridge</code>. The runtime will then look for an executable file named <code>bridge</code> in its plugin search path (usually <code>/opt/cni/bin</code>).</p><p>It is critical that network configuration files remain static while in use. CNI operations are not designed to handle configuration changes between an <code>ADD</code> and a <code>DEL</code> command.</p><p>For example, assume your configuration uses CIDR-A when a pod is created. If you modify the file to use CIDR-B before that pod is deleted, the CNI plugin will not know about the original CIDR-A. This will likely cause the plugin to fail when it tries to clean up the old network interfaces during the <code>DEL</code> operation.</p><h4 id=cni-operations>CNI Operations<a hidden class=anchor aria-hidden=true href=#cni-operations>#</a></h4><p>The JSON configuration defines what the network should look like, but it doesn&rsquo;t specify what to do. The runtime tells the plugin which action to perform by setting environment variables.</p><p>The most important variable is <code>CNI_COMMAND</code>, which defines the operation:</p><blockquote><ul><li><code>ADD</code>: Add container to network, or apply modifications</li><li><code>DEL</code>: Remove container from network, or un-apply modifications</li><li><code>CHECK</code>: Check container&rsquo;s networking is as expected</li><li><code>STATUS</code>: Check plugin status</li><li><code>VERSION</code>: probe plugin version support</li><li><code>GC</code>: Clean up any stale resources</li></ul><p>Reference CNI spec</p></blockquote><p>Other variables provide the context for the command:</p><ul><li><code>CNI_NETNS</code>: A path to the network namespace (e.g, <code>/var/run/netns/{ns}</code>)</li><li><code>CNI_IFNAME</code>: Interface name to create &lsquo;inside&rsquo; the container.</li><li><code>CNI_PATH</code>: List of paths in the system to search CNI plugin executables.</li><li><code>CNI_CONTAINERID</code>: Container ID for the container.</li><li><code>CNI_ARGS</code>: Extra args provided by runtime.</li></ul><h4 id=example-an-add-operation>Example: An ADD Operation<a hidden class=anchor aria-hidden=true href=#example-an-add-operation>#</a></h4><p>Let&rsquo;s put this all together.
Imagine we want to add a container with ID <code>c123</code> to the network namespace <code>container0</code>, using the configuration file <code>/etc/cni/net.d/10-mynet.conflist</code> (which contains our JSON example).</p><p>The runtime would execute a command similar to this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># runtimes get these information via</span>
</span></span><span class=line><span class=cl><span class=c1># Network configuration JSON.</span>
</span></span><span class=line><span class=cl><span class=nv>PLUGIN_NAME</span><span class=o>=</span><span class=s2>&#34;bridge&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>CNI_PATH</span><span class=o>=</span><span class=s2>&#34;/opt/cni/bin&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>CNI_COMMAND</span><span class=o>=</span><span class=s2>&#34;ADD&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=nv>CNI_CONTAINERID</span><span class=o>=</span><span class=s2>&#34;c123&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=nv>CNI_NETNS</span><span class=o>=</span><span class=s2>&#34;/var/run/netns/container0&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=nv>CNI_IFNAME</span><span class=o>=</span><span class=s2>&#34;eth0&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>cat /etc/cni/net.d/10-mynet.conflist <span class=p>|</span> <span class=nv>$CNI_PATH</span>/<span class=nv>$PLUGIN_NAME</span>
</span></span></code></pre></td></tr></table></div></div><p>In this command, the runtime pipes the JSON configuration to the <code>bridge</code> plugin&rsquo;s STDIN and sets the environment variables. The <code>bridge</code> plugin executes and reads the config.</p><p>If you remember our quick demo, this configuration instructs the bash commands we run to:</p><ul><li>Create <code>eth0</code> interface in container (instead of <code>c0-veth</code>)</li><li>The network namespace you need to operate is <code>container0</code>, which means that <code>ip netns exec &lt;given_namespace> $operations</code></li></ul><blockquote><p>You may also see <code>CNI_NETNS_OVERRIDE</code>.
This variable is not part of the official CNI spec but is used by the <code>libcni</code> Go package.
It acts as a guard-rail to stop the plugin from modifying the wrong network namespace if the namespace path changes unexpectedly during the operation.
This should ideally not happen with modern container runtimes, as they do not rely on
process ID based paths for network namespaces.</p><p><a href=https://github.com/containernetworking/plugins/issues/714>https://github.com/containernetworking/plugins/issues/714</a></p></blockquote><h4 id=kubernetes-and-cni>Kubernetes and CNI<a hidden class=anchor aria-hidden=true href=#kubernetes-and-cni>#</a></h4><p>But how does Kubernetes use CNI? We mentioned that we may have multiple &ldquo;plugins&rdquo; available. We should somehow instruct Kubernetes to use correct plugins.</p><p>Kubernetes manages Pods with CRI (Container Runtime Interface), which is triggered by <code>kubelet</code>.
So, <code>kubelet</code> calls CRI via gRPC API, and your cluster can use various CRI implementations (RPC implementation).
While setting up a Pod, <code>kubelet</code> calls <a href=https://github.com/kubernetes/cri-api/blob/15d088979b4574be15436b5b9772f934874a27ad/pkg/apis/runtime/v1/api.proto#L40C5-L40C44>&ldquo;rpc RunPodSandbox(RunPodSandboxRequest)&rdquo;</a>,
which in turn calls underlying CRI.
While CRI is setting up the Pod, it internally calls CNI to make sure that Pod&rsquo;s network environment is ready.
That&rsquo;s actually how Kubernetes calls CNI. It&rsquo;s a bit of a chain of calls but to simply put: <code>kubelet -> CRI -> CNI</code></p><blockquote><p>For example, how container-d calls CNI in RunPodSandbox: <a href=https://github.com/containerd/containerd/blob/1c4457e00facac03ce1d75f7b6777a7a851e5c41/internal/cri/server/sandbox_run.go#L261-L263>https://github.com/containerd/containerd/blob/1c4457e00facac03ce1d75f7b6777a7a851e5c41/internal/cri/server/sandbox_run.go#L261-L263</a></p></blockquote><p>CRI (Container Runtime Interface) calls CNI to set up the pod network, by following the above example.
Then, based on CNI configurations and arguments, the CNI performs operations to set up the pod network.</p><h3 id=writing-cni-plugin-in-go>Writing CNI Plugin in Go<a hidden class=anchor aria-hidden=true href=#writing-cni-plugin-in-go>#</a></h3><p>The &ldquo;bridge&rdquo; demo we created is actually quite close to being a CNI plugin. We just need to make it aware of the CNI environment variables (like <code>CNI_COMMAND</code>) and
make it read from STDIN and write to STDOUT, as defined in the CNI specification.
However, if you&rsquo;ve ever checked the official plugins, they are usually not shell scripts.
We need to handle JSON, error reporting, and complex networking operations, which is a better job for a language like Go.</p><p>This section will go through my notes on what I learned from the official &ldquo;bridge&rdquo; plugin while writing my own.</p><blockquote><p>The source code for the CNI maintained &ldquo;bridge&rdquo; plugin: <a href=https://github.com/containernetworking/plugins/tree/v1.8.0/plugins/main/bridge>https://github.com/containernetworking/plugins/tree/v1.8.0/plugins/main/bridge</a></p></blockquote><p>CNI maintainers provide official Go packages to handle the common boilerplate. This includes reading CNI environment variables, parsing the network configuration from STDIN,
validating them, and formatting the result as JSON to STDOUT.
Building all of this from scratch would be tedious.</p><p>The <code>libcni</code> package (<a href=https://pkg.go.dev/github.com/containernetworking/cni/libcni>https://pkg.go.dev/github.com/containernetworking/cni/libcni</a>) and especially the <code>skel</code> package (<a href=https://pkg.go.dev/github.com/containernetworking/cni/pkg/skel>https://pkg.go.dev/github.com/containernetworking/cni/pkg/skel</a>) are designed to solve this.</p><p>The <code>skel</code> package provides a skeleton for a CNI plugin. It implements all the argument parsing and validation, like checking if all required environment variables are defined for a given <code>CNI_COMMAND</code>.</p><blockquote><p><a href=https://github.com/containernetworking/plugins/blob/0e648479e11c2c6d9109b14fc0c9ac64c677861b/plugins/main/bridge/bridge.go#L837-L843>https://github.com/containernetworking/plugins/blob/0e648479e11c2c6d9109b14fc0c9ac64c677861b/plugins/main/bridge/bridge.go#L837-L843</a></p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>main</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>skel</span><span class=p>.</span><span class=nf>PluginMainFuncs</span><span class=p>(</span><span class=nx>skel</span><span class=p>.</span><span class=nx>CNIFuncs</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>Add</span><span class=p>:</span><span class=w>    </span><span class=nx>cmdAdd</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>Check</span><span class=p>:</span><span class=w>  </span><span class=nx>cmdCheck</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>Del</span><span class=p>:</span><span class=w>    </span><span class=nx>cmdDel</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>Status</span><span class=p>:</span><span class=w> </span><span class=nx>cmdStatus</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=p>},</span><span class=w> </span><span class=nx>version</span><span class=p>.</span><span class=nx>All</span><span class=p>,</span><span class=w> </span><span class=nx>bv</span><span class=p>.</span><span class=nf>BuildString</span><span class=p>(</span><span class=s>&#34;bridge&#34;</span><span class=p>))</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>As a plugin developer, you just register your functions for the <code>Add</code>, <code>Check</code>, and <code>Del</code> commands.
The <code>skel</code> package handles calling the right function and provides a struct containing all the parsed arguments and network configuration.
The rest is just implementing the networking logic based on the provided information.</p><p>Let&rsquo;s skip the configuration parsing and dive into a Go-specific trick you&rsquo;ll need.</p><h4 id=go-scheduler-and-the-problem>Go Scheduler and the Problem<a hidden class=anchor aria-hidden=true href=#go-scheduler-and-the-problem>#</a></h4><p>One thing I realized is that most CNI plugins use <code>runtime.LockOSThread()</code> before they change network namespaces.
This isn&rsquo;t a function I use in my typical day-to-day work.
Understanding why this is necessary led me to learn much more about how Go&rsquo;s scheduler works with OS threads.</p><p>The Go scheduler&rsquo;s job is to run these N goroutines on M OS threads (this is called an M:N scheduler).</p><p><code>GOMAXPROCS</code> limits the number of OS threads that can execute user-level Go code simultaneously. By default, this is set to the number of available CPU cores. Note that Go can create additional OS threads beyond this limit when threads block on system calls, but those blocked threads don&rsquo;t count against the <code>GOMAXPROCS</code> limit. Only threads actively executing Go code are constrained by <code>GOMAXPROCS</code>.</p><p>The Go scheduler maintains multiple runqueues to manage goroutines efficiently. There is a global runqueue shared by all threads, and each thread has its own local runqueue. This design minimizes lock contention. When a thread needs work, it first checks its local queue, and if empty, it can steal work from other threads&rsquo; queues or check the global queue.</p><img src=/images/cni-runqueue.png alt="Go Runqueue" style="width:150%;max-width:900px;display:block;margin:10px auto"><p>The Go scheduler uses asynchronous preemption.
This means that if a goroutine runs for too long (e.g., more than 10ms), the scheduler can forcefully pause it
and schedule it to run again later.
This is great for fairness, as it prevents one CPU-heavy goroutine from starving all others.</p><p>For example, if you have a goroutine that blocks the CPU and not being able to voluntarily give up its control to CPU back (due to some maybe hardware flaw),
then other goroutines in the OS thread&rsquo;s runqueue needs to wait for this long running to finish.</p><p>This preemption has a critical side effect: a goroutine is not guaranteed to stay on the same OS thread.
It might run on Thread 1, get preempted, and later resume its work on Thread 2.
For most code, this doesn&rsquo;t matter. But for a CNI plugin, it&rsquo;s a huge problem.</p><h4 id=issues-with-go-and-network-namespaces>Issues with Go and Network Namespaces<a hidden class=anchor aria-hidden=true href=#issues-with-go-and-network-namespaces>#</a></h4><p>You might be wondering, &ldquo;What does this have to do with CNI?&rdquo;. The problem lies in how we change network namespaces.</p><p>When we create a network namespace for a pod, we need to run commands inside it.
We don&rsquo;t use the <code>ip netns exec</code> command; instead, we use a Linux system call: <code>setns(2)</code> under the hood.</p><p>The <code>setns(2)</code> syscall is very specific: it changes the namespace of the calling thread, not the whole process. So, this Linux system call associates a thread with a namespace, not a process with a namespace.
In Go, this might be a bit problematic due to Go scheduler behavior.</p><p>Go scheduler can preempt the goroutine and move it to another OS thread, due to motivations explained in the previous section.
For example, assume that <code>cmdAdd</code> goroutine is running on OS Thread A (which is in the host <code>netns</code>).
The goroutine calls <code>setns()</code> to enter the pod&rsquo;s <code>netns</code>, which changes OS Thread A&rsquo;s state. Now, the thread is in the pod&rsquo;s network namespace. Your goroutine continues, preparing to create the <code>veth</code> pair.
BAM! The Go scheduler preempts your goroutine because your time is up, the turn is for the next scheduled
goroutine. A moment later, the scheduler decides to resume your goroutine. It picks OS Thread B (which is still in the host <code>netns</code>) to run it.
Your goroutine, unaware it has been moved, continues where it left off. It tries to create the <code>veth</code> pair, but it&rsquo;s now running on OS Thread B, so it incorrectly modifies the host network instead of the pod&rsquo;s network. This is a subtle and dangerous bug :(.</p><pre tabindex=0><code>goroutine -&gt; OS Thread A (namespace: host)
                 |
                 | (preemption, goscheduler moves goroutine to the new thread)
                 v
             OS Thread B (namespace: somethingelse)
</code></pre><h4 id=solution-runtimelockosthread>Solution: runtime.LockOSThread()<a hidden class=anchor aria-hidden=true href=#solution-runtimelockosthread>#</a></h4><p>This is precisely what <code>runtime.LockOSThread()</code> solves.</p><p>When you call <code>runtime.LockOSThread()</code>, you are telling the Go scheduler: &ldquo;Pin this current goroutine to the current OS thread&rdquo;. It does not stop the scheduler. Your goroutine can still be preempted. But when the scheduler resumes it, it is forced to resume it on the exact same OS thread it was pinned to.
When we lock the thread, it also prevents the Go scheduler from running other goroutines on that thread.
This is useful if your goroutine modifies the thread&rsquo;s namespace state (like network namespace operations). It ensures that no other goroutine that might also try to update thread state (like changing the network namespace) gets scheduled on that thread.</p><p>For example, when a goroutine calls <code>runtime.LockOSThread()</code>, it is now pinned to OS Thread A.
When this goroutine running on Thread A enters the pod network namespace and gets preempted, the scheduler will resume the goroutine on the same OS Thread A. In the meantime, no other goroutines will be scheduled on that thread.</p><p>This is why you see most of the namespace-switching logic in CNI plugins using a <code>LockOSThread</code>/<code>UnlockOSThread</code> block.</p><blockquote><p><a href=https://github.com/containernetworking/plugins/blob/372953dfb89fe5c17a29a865b502a2eabb31a195/pkg/ns/ns_linux.go#L27-L35>https://github.com/containernetworking/plugins/blob/372953dfb89fe5c17a29a865b502a2eabb31a195/pkg/ns/ns_linux.go#L27-L35</a></p></blockquote><p>One final remark is that this thread lock is not inherited by new goroutines.
If you spawn a new goroutine from your locked one, the new goroutine can run on any thread.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>runtime</span><span class=p>.</span><span class=nf>LockOSThread</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>defer</span><span class=w> </span><span class=nx>runtime</span><span class=p>.</span><span class=nf>UnlockOSThread</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>go</span><span class=w> </span><span class=kd>func</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1>// this new goroutine may run on a different thread</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}()</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>Therefore, do not spawn a new goroutine from a locked one if the new goroutine is expected to run on the same thread.</p><p><code>LockOSThread()</code> works like a taint. If you do not unlock the thread, the thread will not be used for scheduling other goroutines anymore.
When the locked goroutine exits without unlocking, the thread itself will be terminated.
So, do not forget to unlock the thread (unless you know what you are doing) to return it to the scheduler pool.</p><p>To obtain the current ns of the thread, you can simply use
<code>/proc/${os.getPid()}/task/${unix.Gettid()}/ns/net</code></p><h3 id=bridge-plugin>Bridge Plugin<a hidden class=anchor aria-hidden=true href=#bridge-plugin>#</a></h3><p>The official Bridge plugin roughly performs the following operations:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>br</span><span class=p>,</span><span class=w> </span><span class=nx>brInterface</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nf>setupBridge</span><span class=p>(</span><span class=nx>n</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>netns</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>ns</span><span class=p>.</span><span class=nf>GetNS</span><span class=p>(</span><span class=nx>args</span><span class=p>.</span><span class=nx>Netns</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>hostInterface</span><span class=p>,</span><span class=w> </span><span class=nx>containerInterface</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nf>setupVeth</span><span class=p>(</span><span class=nx>netns</span><span class=p>,</span><span class=w> </span><span class=nx>br</span><span class=p>,</span><span class=w> </span><span class=nx>args</span><span class=p>.</span><span class=nx>IfName</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// run IPAM plugin for IP assignment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>ipam</span><span class=p>.</span><span class=nf>ExecAdd</span><span class=p>(</span><span class=nx>n</span><span class=p>.</span><span class=nx>IPAM</span><span class=p>.</span><span class=nx>Type</span><span class=p>,</span><span class=w> </span><span class=nx>args</span><span class=p>.</span><span class=nx>StdinData</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>netns</span><span class=p>.</span><span class=nf>Do</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>_</span><span class=w> </span><span class=nx>ns</span><span class=p>.</span><span class=nx>NetNS</span><span class=p>)</span><span class=w> </span><span class=kt>error</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Add the IP to the interface</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>return</span><span class=w> </span><span class=nx>ipam</span><span class=p>.</span><span class=nf>ConfigureIface</span><span class=p>(</span><span class=nx>args</span><span class=p>.</span><span class=nx>IfName</span><span class=p>,</span><span class=w> </span><span class=nx>result</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=p>},</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// add IP to bridge as well.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>err</span><span class=w> </span><span class=p>=</span><span class=w> </span><span class=nf>ensureAddr</span><span class=p>(</span><span class=nx>br</span><span class=p>,</span><span class=w> </span><span class=o>&amp;</span><span class=nx>gw</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>So, what we are doing is as follows:</p><ul><li>setup bridge</li><li>setup veth pairs</li><li>run IPAM plugin and configure veth and bridge to ensure they have IP</li></ul><p>How does it create links (devices)? For that, it uses <a href=https://github.com/vishvananda/netlink>https://github.com/vishvananda/netlink</a> package
which is an API to perform operations similar to <code>ip link add</code> by communicating with the kernel.
And this package is the Go binding of this kernel communication.</p><p>The <code>Do</code> method is quite powerful method, which is defined as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>type</span><span class=w> </span><span class=nx>NetNS</span><span class=w> </span><span class=kd>interface</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// Executes the passed closure in this object&#39;s network namespace,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// attempting to restore the original namespace before returning.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// However, since each OS thread can have a different network namespace,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// and Go&#39;s thread scheduling is highly variable, callers cannot</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// guarantee any specific namespace is set unless operations that</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// require that namespace are wrapped with Do().  Also, no code called</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// from Do() should call runtime.UnlockOSThread(), or the risk</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// of executing code in an incorrect namespace will be greater.  See</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=c1>// https://github.com/golang/go/wiki/LockOSThread for further details.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=nf>Do</span><span class=p>(</span><span class=nx>toRun</span><span class=w> </span><span class=kd>func</span><span class=p>(</span><span class=nx>NetNS</span><span class=p>)</span><span class=w> </span><span class=kt>error</span><span class=p>)</span><span class=w> </span><span class=kt>error</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1>// ... other methods</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><blockquote><p><a href=https://github.com/containernetworking/plugins/blob/0e648479e11c2c6d9109b14fc0c9ac64c677861b/pkg/ns/ns_linux.go#L74-L83>https://github.com/containernetworking/plugins/blob/0e648479e11c2c6d9109b14fc0c9ac64c677861b/pkg/ns/ns_linux.go#L74-L83</a></p></blockquote><p>It is a powerful method that achieves what we are looking for. It locks the thread, runs our
Go code in that thread to safely manipulate the thread, and takes the network namespace where
the operation has started (like host).</p><p>So, <code>Do</code> ensures the user operation is safe by locking the goroutine to a specific OS thread,
performing the namespace switch, running the code, and carefully switching back.</p><blockquote><p><strong>Note:</strong> The following is a simplified, pseudocode representation of <code>Do</code> method.
It omits error handling, handle closing (<code>.Close()</code>), and other details to focus purely on the core logic and execution flow.</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Do executes the &#39;toRun&#39; function within the network namespace &#39;ns&#39;.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>func</span><span class=w> </span><span class=p>(</span><span class=nx>ns</span><span class=w> </span><span class=o>*</span><span class=nx>netNS</span><span class=p>)</span><span class=w> </span><span class=nf>Do</span><span class=p>(</span><span class=nx>toRun</span><span class=w> </span><span class=kd>func</span><span class=p>(</span><span class=nx>hostNS</span><span class=w> </span><span class=nx>NetNS</span><span class=p>))</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Check if the target namespace handle &#39;ns&#39; is already closed.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>ns</span><span class=p>.</span><span class=nf>errorIfClosed</span><span class=p>();</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>		  </span><span class=k>return</span><span class=w> </span><span class=nx>err</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	  </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Get a handle to the current (host) namespace before we do anything.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// This &#39;hostNS&#39; handle is what we&#39;ll pass to the user&#39;s function.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// So that if user needs to perform operations in the initial network namespace</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// it can use this `hostNS` to checkout to the correct network namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>hostNS</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nf>getCurrentNS</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// We need a WaitGroup to wait for the goroutine to finish.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>var</span><span class=w> </span><span class=nx>wg</span><span class=w> </span><span class=nx>sync</span><span class=p>.</span><span class=nx>WaitGroup</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>wg</span><span class=p>.</span><span class=nf>Add</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>var</span><span class=w> </span><span class=nx>innerError</span><span class=w> </span><span class=kt>error</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>go</span><span class=w> </span><span class=kd>func</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>defer</span><span class=w> </span><span class=nx>wg</span><span class=p>.</span><span class=nf>Done</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c1>// Lock this goroutine to its current OS thread, as discussed</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>runtime</span><span class=p>.</span><span class=nf>LockOSThread</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c1>// Get a handle to this specific thread&#39;s original namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>threadOriginalNS</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nf>getCurrentNS</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c1>// Switch to the target namespace, like `netns(2)` system call.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c1>// this will affect the thread&#39;s state.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>ns</span><span class=p>.</span><span class=nf>Set</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>defer</span><span class=w> </span><span class=kd>func</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// Switch back to the thread&#39;s original namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nx>switchBackErr</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>threadOriginalNS</span><span class=p>.</span><span class=nf>Set</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// - ONLY unlock the OS thread if we successfully switched back.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// - If &#39;switchBackErr&#39; is not nil, we are in a bad state.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>// - We leave the thread locked. The Go runtime will see</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>//  this and discard the OS thread entirely, preventing</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>//  a &#34;dirty&#34; thread (stuck in the wrong ns) from</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=c1>//  being reused by the scheduler, as explained in the prev section.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>if</span><span class=w> </span><span class=nx>switchBackErr</span><span class=w> </span><span class=o>==</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=nx>runtime</span><span class=p>.</span><span class=nf>UnlockOSThread</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c1>// Now that we are inside the target namespace &#39;ns&#39;, execute the user&#39;s function.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>innerError</span><span class=w> </span><span class=p>=</span><span class=w> </span><span class=nf>toRun</span><span class=p>(</span><span class=nx>hostNS</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>wg</span><span class=p>.</span><span class=nf>Wait</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>return</span><span class=w> </span><span class=nx>innerError</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>This is quite useful, especially while setting up veth pairs.</p><p>If you remember, we were running <code>ip</code> commands in both <code>host</code> and <code>container</code> namespaces while setting up the veth pairs.
With this <code>Do</code> method, we can actually achieve something similar.
The following is the pseudocode for &ldquo;bridge&rdquo; plugin while setting up the veth pairs.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// - First, obtain CNI_IFNAME and CNI_NETNS to understand </span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>//  what interface name should be created in which container namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>//  These are embedded in the `args` struct, which is initialized by `skel` boilerplate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>//  from `libcni` package.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// - And then, in host namespace, checkout the container namespace</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>//  which is created by runtime (k8s).</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// - Then, use Do method to perform operations required to create veth pairs.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>containerNs</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>ns</span><span class=p>.</span><span class=nf>GetNS</span><span class=p>(</span><span class=nx>args</span><span class=p>.</span><span class=nx>Netns</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>defer</span><span class=w> </span><span class=nx>netns</span><span class=p>.</span><span class=nf>Close</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Note that we run this code in the host namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// That&#39;s why the argument in caller is &#39;hostNs&#39;.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// By using the Do function, we run the operations in containerNs with locked OS thread.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>containerNs</span><span class=p>.</span><span class=nf>Do</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>hostNs</span><span class=w> </span><span class=nx>ns</span><span class=p>.</span><span class=nx>NetNS</span><span class=p>)</span><span class=w> </span><span class=kt>error</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1>// The operations in this caller is running in the `containerNs`.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1>// First create a veth device.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nx>veth</span><span class=o>:=</span><span class=w> </span><span class=nx>netlink</span><span class=p>.</span><span class=nx>Veth</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>LinkAttrs</span><span class=p>:</span><span class=w> </span><span class=nx>netlink</span><span class=p>.</span><span class=nx>LinkAttrs</span><span class=p>{</span><span class=nx>Name</span><span class=p>:</span><span class=w> </span><span class=nx>args</span><span class=p>.</span><span class=nx>Ifname</span><span class=p>},</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>PeerName</span><span class=p>:</span><span class=w> </span><span class=nx>hostVethPeerName</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// - PeerNamespace is the interface of the peer (host).</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// - It can take the namespace as FD (file descriptor),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//  or PID (process ID) for the namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// - Since the file descriptor approach is the convenient one,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//  as explained with CNI_NETNS_OVERRIDE above,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//  use helpers of `ns.NetNS` struct to obtain file descriptor</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//  of the host network namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// - Since the veth pair that we create here is going to be located</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//  in the container its peer needs to be in host,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//  so that we can connect to it to the bridge.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>PeerNamespace</span><span class=p>:</span><span class=w> </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>NsFd</span><span class=p>(</span><span class=nb>int</span><span class=p>(</span><span class=nx>hostNs</span><span class=p>.</span><span class=nf>Fd</span><span class=p>()))</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=cm>/*...*/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1>// Then, `add` this link device; similar to `ip link add`</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>LinkAdd</span><span class=p>(</span><span class=o>&amp;</span><span class=nx>veth</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1>// Set this new link device `up`, similar to `ip link set up`</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>LinkSetUp</span><span class=p>(</span><span class=nx>netlink</span><span class=p>.</span><span class=nf>LinkByName</span><span class=p>(</span><span class=nx>args</span><span class=p>.</span><span class=nx>Ifname</span><span class=p>))</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1>// Now, run the following code in &#34;host&#34; namespace.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nx>hostNs</span><span class=p>.</span><span class=nf>Do</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>_</span><span class=w> </span><span class=nx>ns</span><span class=p>.</span><span class=nx>NetNS</span><span class=p>)</span><span class=w> </span><span class=kt>error</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Set the veth pair in the host namespace UP.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>hostVeth</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>netlinksafe</span><span class=p>.</span><span class=nf>LinkByName</span><span class=p>(</span><span class=nx>hostVethPeer</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>LinkSetUp</span><span class=p>(</span><span class=nx>hostVeth</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>return</span><span class=w> </span><span class=kc>nil</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=p>})</span><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><h4 id=kernel-knobs>Kernel Knobs<a hidden class=anchor aria-hidden=true href=#kernel-knobs>#</a></h4><p>The official &ldquo;bridge&rdquo; plugin uses more kernel knobs that we see so far.
We only saw &ldquo;ip_forward&rdquo; but the official plugin uses more than that to provide
better experience the users.
This section will go through some of them that I noticed to mention.</p><h5 id=netipv4confinterfacearp-notify><code>/net/ipv4/conf/&lt;interface>/arp-notify</code><a hidden class=anchor aria-hidden=true href=#netipv4confinterfacearp-notify>#</a></h5><p>This knob controls Gratuitous ARP (GARP) behavior when the interface&rsquo;s state becomes UP.
A Gratuitous ARP is a broadcast packet where a host announces its own IP-to-MAC mapping, in order to update the caches of other devices on the same network.
By default (0), the kernel does not send a GARP when an interface comes up.
When enabled (1), the kernel will send GARP packets when this interface is brought to an UP state or when an IPv4 address is added to it.</p><p>This setting directly addresses the problem of stale ARP caches on peer devices.
Especially in high-availability (HA) or IP failover scenarios, such as a Kubernetes VIP moving between nodes,
a client&rsquo;s ARP cache may still point to the MAC address of the old, failed node.
This creates issues until that client&rsquo;s ARP entry times out.</p><p>Enabling arp_notify solves this by having the kernel proactively announce the new &ldquo;IP-to-MAC&rdquo; mapping.
For ex, when a standby node&rsquo;s interface comes UP to take over a VIP, the arp_notify mechanism triggers a GARP, forcing all listeners on the L2 segment to immediately update their ARP caches.
This preempts the cache timeout and makes the failover near-instantaneous.</p><h5 id=netipv6confinterfacekeep_addr_on_down><code>/net/ipv6/conf/&lt;interface>/keep_addr_on_down</code><a hidden class=anchor aria-hidden=true href=#netipv6confinterfacekeep_addr_on_down>#</a></h5><p>This configuration prevents the kernel from flushing static global IPv6 addresses when an interface&rsquo;s link state goes DOWN.
This is a critical difference from IPv4, where addresses are retained by default.</p><p>With the default setting (0), the kernel flushes all global IPv6 addresses when the link state goes DOWN.
By setting this knob to 1, the kernel preserves the IPv6 address during the link-down event.</p><h3 id=multi-node-reachability>Multi-Node reachability<a hidden class=anchor aria-hidden=true href=#multi-node-reachability>#</a></h3><p>Besides from the implementation-wise differences, the official &ldquo;bridge&rdquo; plugin in Go is similar to what we have built.
We&rsquo;ve walked through the logic of building a CNI plugin, from using CNI Go packages to safely managing network namespaces with <code>LockOSThread</code>.
We have a complete solution for local pod connectivity.
Our plugin can successfully get a <code>cmdAdd</code> call, create a <code>veth</code> pair, connect a pod to the host&rsquo;s bridge, and get it an IP address. This setup works perfectly for any pods that need to communicate on the same node.</p><p>But what happens when Pod A on Node 1 tries to send a packet to Pod B on Node 2?
As we&rsquo;ve established, our <code>bridge</code> plugin alone can&rsquo;t solve this.
The packet gets sent to the <code>cni0</code> bridge and then dropped by the host, which has no route to the other node&rsquo;s pod network.</p><p>This is the central challenge of Kubernetes networking: multi-node reachability.
Since the CNI specification itself only covers &lsquo;connectivity&rsquo;, it&rsquo;s up to the CNI daemon to solve this.
This next section will explore the common strategies used to make this cluster-wide communication possible.</p><p>Let&rsquo;s first see why the &lsquo;bridge&rsquo; approach that we explained and developed does not provide reachability in multi-node Kubernetes clusters.</p><p>The <code>bridge</code> plugin (like the <code>cni0</code> bridge it creates) is a purely host-specific construct. The <code>cni0</code> virtual switch on Node 1 is completely separate from the <code>cni0</code> virtual switch on Node 2. They have no knowledge of each other and are not connected in any way.
Think of it like two switches located in two networks having two switches respectively.
You cannot expect a switch on one network to communicate with the switch on the other without extra configuration.</p><p>This separation creates a &ldquo;reachability&rdquo; problem.</p><p>For example, consider when Pod A on Node 1 (10.244.1.5) tries to send a packet to Pod B on Node 2 (10.244.2.8):</p><ul><li>The packet leaves Pod A and arrives at the <code>cni0</code> bridge on Node 1.</li><li>The bridge inspects the destination (10.244.2.8) and finds it isn&rsquo;t connected to any of its local pod interfaces, as it only knows about its own <code>10.244.1.x</code> pods.</li><li>The packet is then passed up to the host, Node 1.</li><li>However, Node 1&rsquo;s kernel also has no route for the 10.244.2.0/24 subnet; it has no idea that this network &ldquo;lives&rdquo; on Node 2, thus the packet is unroutable and dropped.</li></ul><p>This is precisely the problem that the CNI daemon (the second component we discussed) is designed to solve. Its entire job is to create the &ldquo;routes&rdquo; that Node 1 is missing, making Node 2&rsquo;s pods reachable.
The strategy this daemon uses depends entirely on the underlying physical network. We will now explore the three most common strategies for solving this multi-node reachability problem.</p><p>Broadly, your cluster&rsquo;s nodes live in one of two physical network scenarios</p><ul><li><p>Nodes are on the same L2 Network: This means all your nodes are in the same subnet (e.g., all have IPs like 192.168.1.x).
They can find and send packets to each other directly, like computers plugged into the same home router. So, no routing is needed.</p></li><li><p>Nodes are on different L2 Networks: This is the most common scenario.
Your nodes are in different subnets (e.g., 192.168.1.x and 192.168.2.x, or in different Availability Zones).
They cannot reach each other directly and must send packets through at least one router.
So, routing is needed.</p></li></ul><p>These two network designs require completely different solutions to make pods reachable.
We will now explore the three most common strategies, though there are different solutions as well.</p><h4 id=simple-routing-in-same-subnet>Simple Routing in Same Subnet<a hidden class=anchor aria-hidden=true href=#simple-routing-in-same-subnet>#</a></h4><p>This is a straightforward approach that works only when the nodes are on the same L2 network.
The CNI daemon on each node acts as a controller, watching the Kubernetes API. When a new node joins the cluster, the daemon on every other node gets an update.</p><p>For example, when Node 2 joins, the CNI daemon on Node 1 sees the new Node 2 object.
It reads two key pieces of information: Node 2&rsquo;s IP (e.g., 192.168.1.11) and the pod network assigned to it, known as its podCIDR (e.g., 10.244.2.0/24).
The daemon then adds a route to Node 1&rsquo;s local routing table, typically using a netlink library, such as ip route add 10.244.2.0/24 via 192.168.1.11.
Now, when Pod A on Node 1 sends a packet to Pod B (10.244.2.5), Node 1&rsquo;s kernel sees this route and knows to forward the packet directly to Node 2&rsquo;s IP.
This is very high-performance because there is no encapsulation.</p><p>We&rsquo;ll develop an example controller to perform this as well.</p><h4 id=dynamic-routing-protocols>Dynamic Routing Protocols<a hidden class=anchor aria-hidden=true href=#dynamic-routing-protocols>#</a></h4><p>These protocols allow configuring routers to exchange route information between each other.
BGP is one of the popular protocol used for this.</p><p>BGP, in general, forms neighourship across routers, where neighbourship is declared statically.
Once you register your peers, routing information is shared across routers, through a TCP connection.</p><p>In Kubernetes, this usually works by having the CNI daemon on each node run a lightweight BGP agent.
This agent &ldquo;peers&rdquo; (forms a BGP neighborship). Calico is a famous CNI plugin that supports BGP.</p><p>This is quite useful because if you have multiple large clusters, you can use BGP or something similar
to achieve route sharing across your cluster&rsquo;s outer router. The routers are all taught where the pod networks are.
Although this is a very powerful and scalable solution, it&rsquo;s also the most complex. It requires access to and expertise in configuring your physical network fabric (your routers and switches) to peer with your nodes, which may not be possible all the time.</p><h4 id=overlay-networks>Overlay Networks<a hidden class=anchor aria-hidden=true href=#overlay-networks>#</a></h4><p>An overlay network is a virtual network built on top of an existing physical network, like a cloud provider&rsquo;s network.
This concept isn&rsquo;t specific to Kubernetes but very useful in managed Kubernetes offerings,
because your cluster nodes (like VMs) might be on different underlying networks, such as in different cloud availability zones.</p><p>In modern data centers or clouds, your cluster nodes (VMs) are often in different locations.
They might be on different racks or even in different &ldquo;availability zones&rdquo;. They are not all plugged into one giant switch, instead they live on a complex, routed network.</p><img src=/images/cni-vxlan-problem.png alt="VXLAN flow" style="width:150%;max-width:900px;display:block;margin:10px auto"><p>This creates a problem: the physical network only knows how to send packets between nodes (using node IPs). It has no idea what a pod IP is or how to find it.
So, even though nodes are connected by L3 connectivity, each nodes get its own subnet.
Assume that Node 1 and Node 2 are VMs located in different servers and if Node 1 just sent a packet addressed to Pod IP located in Node 2, the physical network would drop it, not knowing where to send it even though Node 1 and Node 2 can communicate with each other (L3 connectivity).</p><p>To solve this, the overlay network hides the original pod packet.
It does this by &ldquo;wrapping&rdquo; it inside a new, &ldquo;outer&rdquo; packet. This process is called encapsulation.
The new outer packet is addressed to the node that the pod lives on (e.g., Node 2&rsquo;s IP), which the physical network does understand.</p><p>One of the most common overlay networking technique is VXLAN (Virtual Extensible LAN) which uses the similar idea of wrapping packets.
Each node has a virtual device called VTEP (Virtual Tunnel Endpoint), where packets are sent to this tunnel endpoint before they are going into other nodes.</p><p>For example, assume that we have Pod A on Node 1 and Pod B on Node 2.
When Pod A on Node 1 sends a packet to Pod B, the packet targets Pod B&rsquo;s IP address.
The kernel recognizes that this Pod is not in our subnet, so the packet is destined to VTEP, because since Pod B&rsquo;s IP address is local to Node 2, if the physical network receives it, it does not know how to send it to the actual place (similar to NAT problem).</p><p>The VTEP encapsulates this packet inside a new UDP packet, which is destined to Node 2&rsquo;s IP.</p><pre tabindex=0><code>+-----------------------------------------------------+
|   Outer IP Header (Source: Node 1, Dest: Node 2)    |
+-----------------------------------------------------+
|   Outer UDP Header (Dest Port: 4789 for VXLAN)      |
+-----------------------------------------------------+
|                 VXLAN Header (VNI)                  |
+-----------------------------------------------------+
|   +---------------------------------------------+   |
|   | Inner IP Header (Source: Pod A, Dest: Pod B)|   |
|   +---------------------------------------------+   |
|   |            Payload (Data)                   |   |
|   +---------------------------------------------+   |
+-----------------------------------------------------+
</code></pre><blockquote><p>Note: This is a simplified diagram. A real packet would also include L2 (Ethernet) headers for both the inner and outer packets, as well as a specific VXLAN header. But this illustrates the main idea of wrapping the pod-to-pod packet inside a node-to-node packet.</p></blockquote><p>The physical network sees a normal UDP packet from Node 1 to Node 2 and routes it.
Node 2&rsquo;s kernel receives the UDP packet, sees it&rsquo;s for VXLAN, and hands it to its VTEP.
The VTEP de-encapsulates it, removing the outer UDP packet and what&rsquo;s left is the original packet addressed to Pod B. Since Node 2 knows that the Pod B is on its system, it delivers it.</p><p>This is a popular approach in most cloud providers because it decouples the pod network from the physical network.
It allows Kubernetes to create its own flat, virtual network for pods without having to ask the cloud provider to make any special changes to its physical routers.
As long as the nodes can send UDP packets (L3 connectivity) to each other, the pod network just works.</p><h3 id=writing-cni-daemon-in-go>Writing CNI Daemon in Go<a hidden class=anchor aria-hidden=true href=#writing-cni-daemon-in-go>#</a></h3><p>Finally, in this section, we are going to write a simple daemon utilizing the principles defined in the
<a href=#simple-routing-in-same-subnet>Routing in Same Subnet</a> section. We&rsquo;ll create a Kubernetes controller
that modifies each node&rsquo;s route table to ensure networking works as expected.</p><p>Our controller will run as a DaemonSet, meaning one copy (one pod) will run on each node in the cluster.
This daemon watches for changes to Node objects.
Its job is to ensure that the correct routes are set up on the host it&rsquo;s running on.</p><p>It adds a route for every other node in the cluster, telling the host&rsquo;s kernel:
&ldquo;To reach pods on Node B (with PodCIDR 10.244.1.0/24), send the traffic directly to
Node B&rsquo;s internal IP (e.g., 192.168.1.101).&rdquo;</p><h4 id=how-can-a-pod-change-the-hosts-routes>How Can a Pod Change the Host&rsquo;s Routes?<a hidden class=anchor aria-hidden=true href=#how-can-a-pod-change-the-hosts-routes>#</a></h4><p>Before we look at the Go code, we should understand how a pod, which is normally an isolated sandbox, is allowed to modify its host.
Our DaemonSet&rsquo;s pod needs several high-privilege settings:</p><ul><li><code>hostNetwork: true</code>: This is the most important setting. It tells Kubernetes to not create a separate network namespace for this pod.
The pod will run directly in the host&rsquo;s network namespace.
This gives our controller access to the host&rsquo;s <code>eth0</code> and, crucially, its main routing table.</li><li><code>NET_ADMIN</code> Capability: Even in the host&rsquo;s namespace, a process needs permission to change network settings.
This capability grants our controller the right to add and delete routes.</li><li><code>priorityClassName: system-node-critical</code>: This is a best practice for CNIs.
It tells Kubernetes this pod is essential for the node to function,
so it should be scheduled first and be the last to get evicted if the node is under resource pressure.</li><li><code>tolerations: - operator: Exists</code>: This ensures our DaemonSet pod will run on all nodes, including control-plane nodes that might have taints.
This is necessary for full cluster connectivity.</li></ul><p>You may also see an <strong><code>initContainer</code></strong> in CNI DaemonSets.
This init container often has <code>privileged: true</code> and mounts <code>hostPath</code> volumes for <code>/opt/cni/bin</code> and <code>/etc/cni/net.d</code>.
Its job is different: it&rsquo;s responsible for installing the <em>CNI plugin binary</em> (like our <code>bridge</code> plugin) onto the host&rsquo;s filesystem, so the <code>kubelet</code> can call it.</p><h4 id=controller>Controller<a hidden class=anchor aria-hidden=true href=#controller>#</a></h4><blockquote><p>Note: The following is a simplified pseudocode representation.
It omits error handling, logging, and other production-level details to focus purely on the core logic and execution flow.
The source code can be found here for reference: <a href=https://github.com/buraksekili/hirs-cni/blob/main/cmd/controllers/node/main.go>https://github.com/buraksekili/hirs-cni/blob/main/cmd/controllers/node/main.go</a></p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>main</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>currentNodeName</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>os</span><span class=p>.</span><span class=nf>Getenv</span><span class=p>(</span><span class=s>&#34;NODE_NAME&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>mgr</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nf>CreateControllerManager</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>reconciler</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=o>&amp;</span><span class=nx>NodeReconciler</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Client</span><span class=p>:</span><span class=w>   </span><span class=nx>mgr</span><span class=p>.</span><span class=nf>GetClient</span><span class=p>(),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>NodeName</span><span class=p>:</span><span class=w> </span><span class=nx>currentNodeName</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Set up the controller with the manager.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// - We watch for &#34;Node&#34; objects.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// - We filter out events for our *own* node,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//    since we don&#39;t need to add a route to ourselves.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>isRemoteNode</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>predicate</span><span class=p>.</span><span class=nf>NewPredicateFuncs</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>obj</span><span class=w> </span><span class=nx>client</span><span class=p>.</span><span class=nx>Object</span><span class=p>)</span><span class=w> </span><span class=kt>bool</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>		  </span><span class=k>return</span><span class=w> </span><span class=nx>obj</span><span class=p>.</span><span class=nf>GetName</span><span class=p>()</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nx>NodeName</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	  </span><span class=p>})</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>ctrl</span><span class=p>.</span><span class=nf>NewControllerManagedBy</span><span class=p>(</span><span class=nx>mgr</span><span class=p>).</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nf>For</span><span class=p>(</span><span class=o>&amp;</span><span class=nx>corev1</span><span class=p>.</span><span class=nx>Node</span><span class=p>{}).</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nf>WithEventFilter</span><span class=p>(</span><span class=nx>predicate</span><span class=p>.</span><span class=nf>Filter</span><span class=p>(</span><span class=nx>isRemoteNode</span><span class=p>)).</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nf>Complete</span><span class=p>(</span><span class=nx>reconciler</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>mgr</span><span class=p>.</span><span class=nf>Start</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>func</span><span class=w> </span><span class=p>(</span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>NodeReconciler</span><span class=p>)</span><span class=w> </span><span class=nf>Reconcile</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>req</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// We already filtered out our own node, so this is always a remote node.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>node</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>Get</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>req</span><span class=p>.</span><span class=nx>NamespacedName</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>node</span><span class=p>.</span><span class=nf>IsBeingDeleted</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>if</span><span class=w> </span><span class=nx>controllerutil</span><span class=p>.</span><span class=nf>ContainsFinalizer</span><span class=p>(</span><span class=nx>node</span><span class=p>,</span><span class=w> </span><span class=s>&#34;hirscni.io/route-cleanup&#34;</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nx>r</span><span class=p>.</span><span class=nf>deleteRoute</span><span class=p>(</span><span class=nx>node</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nx>controllerutil</span><span class=p>.</span><span class=nf>RemoveFinalizer</span><span class=p>(</span><span class=nx>node</span><span class=p>,</span><span class=w> </span><span class=s>&#34;hirscni.io/route-cleanup&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>return</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>Update</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>node</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// add finalizer if it does not exist.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>controllerutil</span><span class=p>.</span><span class=nf>AddFinalizer</span><span class=p>(</span><span class=nx>node</span><span class=p>,</span><span class=w> </span><span class=s>&#34;hirscni.io/route-cleanup&#34;</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nx>r</span><span class=p>.</span><span class=nf>Update</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>node</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>return</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>reconcileRoute</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>node</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>func</span><span class=w> </span><span class=p>(</span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>NodeReconciler</span><span class=p>)</span><span class=w> </span><span class=nf>reconcileRoute</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>node</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>podCIDR</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>node</span><span class=p>.</span><span class=nx>Spec</span><span class=p>.</span><span class=nx>PodCIDR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>podCIDR</span><span class=w> </span><span class=o>==</span><span class=w> </span><span class=s>&#34;&#34;</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Get that node&#39;s internal IP (e.g., &#34;192.168.1.101&#34;).</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>nodeInternalIP</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nf>getNodeInternalIP</span><span class=p>(</span><span class=nx>node</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>nodeInternalIP</span><span class=w> </span><span class=o>==</span><span class=w> </span><span class=s>&#34;&#34;</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>r</span><span class=p>.</span><span class=nf>addOrUpdateRoute</span><span class=p>(</span><span class=nx>podCIDR</span><span class=p>,</span><span class=w> </span><span class=nx>nodeInternalIP</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>func</span><span class=w> </span><span class=p>(</span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>NodeReconciler</span><span class=p>)</span><span class=w> </span><span class=nf>addOrUpdateRoute</span><span class=p>(</span><span class=nx>podCIDR</span><span class=p>,</span><span class=w> </span><span class=nx>gatewayIP</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Parse the destination CIDR.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>dst</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>net</span><span class=p>.</span><span class=nf>ParseCIDR</span><span class=p>(</span><span class=nx>podCIDR</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Parse the gateway (the remote node&#39;s) IP.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>gw</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>net</span><span class=p>.</span><span class=nf>ParseIP</span><span class=p>(</span><span class=nx>gatewayIP</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Find the local interface (link) to reach that gateway.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// If you remember, we were using `ip route get &lt;address&gt;` command</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>//  to see which route the host takes while reaching &lt;address&gt;.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// `netlink.RouteGet` runs a similar request.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>routes</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>RouteGet</span><span class=p>(</span><span class=nx>gw</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>linkIndex</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>routes</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=nx>LinkIndex</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// The following route means that, to reach the address `dst`,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// forward the packet to `gw` through `linkIndex` device.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>route</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=o>&amp;</span><span class=nx>netlink</span><span class=p>.</span><span class=nx>Route</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>LinkIndex</span><span class=p>:</span><span class=w> </span><span class=nx>linkIndex</span><span class=p>,</span><span class=w> </span><span class=c1>// e.g., &#39;eth0&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Dst</span><span class=p>:</span><span class=w>       </span><span class=nx>dst</span><span class=p>,</span><span class=w>       </span><span class=c1>// e.g., 10.244.1.0/24</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Gw</span><span class=p>:</span><span class=w>        </span><span class=nx>gw</span><span class=p>,</span><span class=w>        </span><span class=c1>// e.g., 192.168.1.101</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// just a tip: `ip route replace` is the idempotent call</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// to manage routes.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>RouteReplace</span><span class=p>(</span><span class=nx>route</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>func</span><span class=w> </span><span class=p>(</span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>NodeReconciler</span><span class=p>)</span><span class=w> </span><span class=nf>deleteRoute</span><span class=p>(</span><span class=nx>node</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>podCIDR</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>node</span><span class=p>.</span><span class=nx>Spec</span><span class=p>.</span><span class=nx>PodCIDR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>nodeInternalIP</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nf>getNodeInternalIP</span><span class=p>(</span><span class=nx>node</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>dst</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>net</span><span class=p>.</span><span class=nf>ParseCIDR</span><span class=p>(</span><span class=nx>podCIDR</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>gw</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>net</span><span class=p>.</span><span class=nf>ParseIP</span><span class=p>(</span><span class=nx>nodeInternalIP</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>routes</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>RouteGet</span><span class=p>(</span><span class=nx>gw</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>linkIndex</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>routes</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=nx>LinkIndex</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>route</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=o>&amp;</span><span class=nx>netlink</span><span class=p>.</span><span class=nx>Route</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>LinkIndex</span><span class=p>:</span><span class=w> </span><span class=nx>linkIndex</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Dst</span><span class=p>:</span><span class=w>       </span><span class=nx>dst</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Gw</span><span class=p>:</span><span class=w>        </span><span class=nx>gw</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>netlink</span><span class=p>.</span><span class=nf>RouteDel</span><span class=p>(</span><span class=nx>route</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>func</span><span class=w> </span><span class=nf>getNodeInternalIP</span><span class=p>(</span><span class=nx>node</span><span class=p>)</span><span class=w> </span><span class=kt>string</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// Loop through the node&#39;s addresses and find the one labeled &#34;InternalIP&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>for</span><span class=w> </span><span class=nx>_</span><span class=p>,</span><span class=w> </span><span class=nx>addr</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=k>range</span><span class=w> </span><span class=nx>node</span><span class=p>.</span><span class=nx>Status</span><span class=p>.</span><span class=nx>Addresses</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>if</span><span class=w> </span><span class=nx>addr</span><span class=p>.</span><span class=nx>Type</span><span class=w> </span><span class=o>==</span><span class=w> </span><span class=nx>corev1</span><span class=p>.</span><span class=nx>NodeInternalIP</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>return</span><span class=w> </span><span class=nx>addr</span><span class=p>.</span><span class=nx>Address</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>return</span><span class=w> </span><span class=s>&#34;&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>We&rsquo;ve covered a lot of ground in this post. We started with the very basics of networking theory, then saw how those ideas are implemented in Linux with tools like network namespaces, veth pairs, and bridges.</p><p>From that foundation, we explored the CNI specification and wrote our own CNI plugin in Go, even tackling tricky concurrency issues like runtime.LockOSThread.
We also learned that a plugin is only half the story and went on to build a complete CNI daemon as a Kubernetes controller, teaching it to manage routes across multiple nodes.</p><p>While we&rsquo;ve built a functional CNI, the Kubernetes networking world is vast. We didn&rsquo;t get to critical features like Network Policy (for security), Service Routing (for ClusterIPs), or IP Address Management (IPAM).</p><p>I hope this journey has helped demystify what&rsquo;s happening under the hood.
The next time you deploy a pod and see it instantly get an IP, you&rsquo;ll know the whole story,
from the kubelet calling the plugin for local connectivity, to the CNI daemon ensuring that pod is reachable by the rest of the cluster.</p><hr><p>As I mentioned, this is all based on my personal notes. If you find any mistakes or have suggestions, feel free to edit this post through GitHub and raise a PR or contact me via <a href=https://x.com/buraksekili>X</a>.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://nghiant3223.github.io/2025/04/15/go-scheduler.html>Go Scheduler</a></li><li><a href="https://www.youtube.com/watch?v=Vn6KYkNevBQ&amp;t">Kubernetes and the CNI: Where We Are and What&rsquo;s Next - Casey Callendrello, CoreOS</a></li><li><a href=https://web.archive.org/web/20190226080835/https://www.weave.works/blog/linux-namespaces-and-go-don-t-mix>Linux Namespaces and Go Don&rsquo;t Mix</a></li><li><a href=https://ifeanyi.co/posts/linux-namespaces-part-4/>Linux Namespaces Part 4</a></li><li><a href=https://lwn.net/Articles/580893/>Namespaces in Operation, part 4: more on PID namespaces</a></li><li><a href=https://www.amazon.com/Networking-Kubernetes-Approach-James-Strong/dp/1492081655>Networking and Kubernetes: A Layered Approach</a></li><li><a href=https://pkg.go.dev/runtime>Runtime Package Documentation</a></li><li><a href="https://www.youtube.com/watch?v=InZVNuKY5GY">Tutorial: Communication Is Key - Understanding Kubernetes Networking - Jeff Poole, Vivint Smart Home</a></li></ul></div><footer class=post-footer><div class=social-icons><a class=edit-post-span href=https://github.com/buraksekili/buraksekili.github.io/blob/main/content/articles/cni.md rel="noopener noreferrer" target=_blank>Edit this page on <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div><ul class=post-tags><li><a href=https://buraksekili.github.io/tags/networking/>Networking</a></li><li><a href=https://buraksekili.github.io/tags/linux/>Linux</a></li><li><a href=https://buraksekili.github.io/tags/linux-networking/>Linux Networking</a></li><li><a href=https://buraksekili.github.io/tags/kubernetes/>Kubernetes</a></li><li><a href=https://buraksekili.github.io/tags/go/>Go</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Let's Build a CNI Plugin! From Linux Networking to CNI on twitter" href="https://twitter.com/intent/tweet/?text=Let%27s%20Build%20a%20CNI%20Plugin%21%20From%20Linux%20Networking%20to%20CNI&amp;url=https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f&amp;hashtags=Networking%2cLinux%2cLinuxNetworking%2cKubernetes%2cGo"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Let's Build a CNI Plugin! From Linux Networking to CNI on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f&amp;title=Let%27s%20Build%20a%20CNI%20Plugin%21%20From%20Linux%20Networking%20to%20CNI&amp;summary=Let%27s%20Build%20a%20CNI%20Plugin%21%20From%20Linux%20Networking%20to%20CNI&amp;source=https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Let's Build a CNI Plugin! From Linux Networking to CNI on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f&title=Let%27s%20Build%20a%20CNI%20Plugin%21%20From%20Linux%20Networking%20to%20CNI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Let's Build a CNI Plugin! From Linux Networking to CNI on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Let's Build a CNI Plugin! From Linux Networking to CNI on whatsapp" href="https://api.whatsapp.com/send?text=Let%27s%20Build%20a%20CNI%20Plugin%21%20From%20Linux%20Networking%20to%20CNI%20-%20https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Let's Build a CNI Plugin! From Linux Networking to CNI on telegram" href="https://telegram.me/share/url?text=Let%27s%20Build%20a%20CNI%20Plugin%21%20From%20Linux%20Networking%20to%20CNI&amp;url=https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Let's Build a CNI Plugin! From Linux Networking to CNI on ycombinator" href="https://news.ycombinator.com/submitlink?t=Let%27s%20Build%20a%20CNI%20Plugin%21%20From%20Linux%20Networking%20to%20CNI&u=https%3a%2f%2fburaksekili.github.io%2farticles%2fcni%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://buraksekili.github.io/>Burak Sekili</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>