[{"content":"I\u0026rsquo;ve always found the best way to learn something is to try and write it down. This post is the result of that process, a collection of my personal notes (zettelkasten) aimed at connecting the dots between three fundamental concepts in Linux: inodes, file descriptors, and sockets. So, this post is just a cleaned-up version of my personal notes, explaining how inodes (representing files on disk), file descriptors (used by programs), and sockets (for network communication) all fit together.\ni-nodes Every file and directory in a filesystem is represented by a data structure called an inode. The inode stores metadata about the object, such as its permissions, attributes, and the disk block locations of its data. In essence, an inode contains pointers to the actual data blocks on the disk, along with the file attributes.\nThis is an overly simplified reference of the inode table and data blocks. In real scenarios, since a file can be larger than a single disk block (typically 4KB or 8KB), the inode stores a series of pointers to all the data blocks that constitute the file. This allows the system to assemble the complete file from its scattered blocks.\nAn inode also includes a reference count, which tracks the number of links pointing to it. When you create a link to a file, the system does not create a new inode or copy the data. Instead, it creates a new name entry in a directory and increments the reference count of the existing inode. The file\u0026rsquo;s data blocks are only deallocated when this reference count drops to zero. Removing a file link, an operation known as \u0026ldquo;unlinking\u0026rdquo;, decrements this count.\nThe following example demonstrates this concept. First, let\u0026rsquo;s inspect a new file:\n1 2 3 $ ls -il total 0 3558217 -rw-rw-r-- 1 burak burak 0 Sep 6 17:54 document.md The first column in the output is the inode number (3558217), and the third column is the reference count (1). Now, let\u0026rsquo;s create a hard link to the file:\n1 2 3 4 5 $ ln document.md document-linked.md $ ls -il total 0 3558217 -rw-rw-r-- 2 burak burak 0 Sep 6 17:54 document-linked.md 3558217 -rw-rw-r-- 2 burak burak 0 Sep 6 17:54 document.md After creating a hard link named document-linked.md, the output shows that both filenames point to the same inode number. The reference count for that inode has now increased to 2.\nFinally, if one of the links is removed, the reference count is decremented.\n1 2 3 $ rm document-linked.md \u0026amp;\u0026amp; ls -il total 0 3558217 -rw-rw-r-- 1 burak burak 0 Sep 6 17:54 document.md The inode and its data persist as long as the reference count is greater than zero.\nIt\u0026rsquo;s good practice to monitor inode usage on your system. Even if your disk has sufficient storage space, you might still get \u0026ldquo;no space left on device\u0026rdquo; or similar errors. This can happen if a process creates a massive number of small files.\nThe issue, in this case, isn\u0026rsquo;t a lack of storage capacity, but rather a shortage of available inodes to track new files. Since Linux uses an inode to manage every file and directory, a faulty process that creates too many files can exhaust all available inodes. For instance, if your HTTP server creates a lot of temporary files, sessions, etc, then you might face such issues at scale.\nTherefore, monitoring inode usage is crucial for system health.\nFile Descriptors A file descriptor is a small, non-negative integer that acts as a handle for an open file. When your process opens or creates a file, the kernel returns this number, which your program then uses to perform all further operations, like reading from or writing to that file.\nThere are special file descriptors for processes (except daemons) in Linux: 0 (stdin), 1 (stdout), and 2 (stderr), called stdin, stdout, and stderr. By convention, the shell automatically opens these three standard file descriptors for every new process it starts:\nBecause these first three numbers are reserved, the first file you open in a program will typically be assigned the next available file descriptor, which is 3.\nThe open() system call manual provides a clear definition of a file descriptor:\n“The return value of open() is a file descriptor, a small, integer that is an index to an entry in the process\u0026rsquo;s table of open file descriptors. The file descriptor is used in subsequent system calls to refer to the open file. The file descriptor returned by a successful call will be the lowest-numbered file descriptor not currently open for the process.” ^1\nAs this description states, a file descriptor is an integer that serves as an index in a per-process table. The kernel maintains a separate file descriptor table for each process, and the number returned by open() is simply the lowest available index in that table. If the kernel cannot open a file, it will typically return -1 to indicate an error. Because each process has its own table, a file descriptor in one process is independent of those in another.\nThis per-process table, however, is only the first part of the structure. Each entry in the file descriptor table points to a second data structure: the system-wide open file table. An entry in this table, often called an \u0026ldquo;open file description,\u0026rdquo; records information like the current file offset and status flags (e.g., read-only, append-only). A file descriptor\u0026rsquo;s reference to an open file description is unaffected even if the original file path is later removed or changed.\nFinally, each entry in the system-wide open file table points to the file\u0026rsquo;s inode, which contains the metadata and pointers to the actual data on disk.\nWith that being said, we can extend our simplified drawing above as follows:\nThe relationship between these tables is important when a process is forked. When fork() is called, the child process receives a copy of the parent\u0026rsquo;s file descriptor table. Crucially, the corresponding file descriptors in both the parent and child tables point to the same entry in the system-wide open file table. This means that they share a file offset; if the child reads from the file, the offset advances for the parent as well.\nIn contrast, processes that are completely independent of each other have separate file descriptor tables that do not share these underlying open file descriptions.\nMonitoring file descriptors is a helpful practice for SREs and system administrators. It\u0026rsquo;s likely to face \u0026ldquo;too many open files\u0026rdquo; errors in day-to-day operations, even in staging environments. These errors often occur when an application forgets to close files after using them, causing a resource leak over time.\nA steady increase in the number of open file descriptors is a strong indicator of a leak in an application\u0026rsquo;s logic.\nYou can start by checking /proc/sys/fs/file-nr to get an overall picture of file descriptor usage on the system.\nThe three values in file-nr denote the number of allocated file handles, the number of allocated but unused file handles, and the maximum number of file handles, reference:Kernel.org.\nTo identify the specific processes consuming the most file descriptors, you can run the following command. Note that it may take some time to complete.\n1 sudo lsof -n | awk \u0026#39;{print $2}\u0026#39; | sort | uniq -c | sort -rn | head -n 10 After identifying the process IDs (PIDs), you can use ps -fn \u0026lt;pid\u0026gt; to understand what each process is doing.\nAlso, you can inspect the open files for a specific process with lsof -p \u0026lt;pid\u0026gt;. This helps identify the source of a leak, such as unclosed temporary files or TCP connections that aren\u0026rsquo;t terminating properly.\nSockets Sockets provide a standard way for processes to communicate with one another, a mechanism known as Inter-Process Communication (IPC). This communication can occur between processes on the same host using Unix Domain Sockets (UDS), or between processes on different hosts across a network. Sockets are the fundamental application programming interface (API) for network protocols like TCP and UDP within the TCP/IP suite.\nThe three most common types of sockets are Stream, Datagram, and Unix Domain sockets. Working with them involves a set of key system calls, such as socket(), bind(), listen(), accept(), and connect().\nThe process begins with the socket() system call, which creates a communication endpoint and returns a socket descriptor. This descriptor is implemented as a file descriptor, allowing programs to use many standard file-related calls like read() and write() to send and receive data. However, not all file operations apply to sockets; for example, a syscall lseek() has no meaning for a socket stream and cannot be used.\nPassive Socket A server follows a specific sequence of system calls to establish a listening socket capable of accepting client connections.\nsocket(): This call creates an endpoint for communication and returns a file descriptor that references it. This is the initial step for both clients and servers.\nbind(): This associates the socket file descriptor with a specific network address, which consists of an IP address and a port number. The kernel now knows where to send incoming packets destined for this socket.\nlisten(): This call marks the socket as a passive one, indicating that it will be used to accept incoming connection requests. The listen() call also takes a backlog parameter, which defines the maximum length of the queue for pending connections.\naccept(): This call extracts the first connection request from the queue of the listening socket, creates a new connected socket dedicated to that specific client, and returns a new file descriptor for it. The original listening socket remains open and continues to listen for other clients.\nread() / write(): All subsequent communication with the client, such as sending and receiving data, occurs on the new file descriptor returned by accept().\nclose(): When communication is finished, the socket file descriptor is closed.\nActive Socket The client-side setup is more straightforward.\nsocket(): Just like the server, the client first creates a socket endpoint.\nconnect(): This call establishes a connection from the client\u0026rsquo;s socket to the server\u0026rsquo;s listening address. Once this call succeeds, the two-way communication channel is established.\nA common best practice for handling connection errors is to completely close the socket file descriptor with close() and then create a new one before attempting to connect() again. Reusing the same file descriptor in subsequent connect() calls after an error can lead to unpredictable behavior across different operating systems. For example, some BSD-derived implementations may fail, so creating a new socket is the most portable approach.\n","permalink":"https://buraksekili.github.io/articles/inodes-filedescriptors-sockets/","summary":"I\u0026rsquo;ve always found the best way to learn something is to try and write it down. This post is the result of that process, a collection of my personal notes (zettelkasten) aimed at connecting the dots between three fundamental concepts in Linux: inodes, file descriptors, and sockets. So, this post is just a cleaned-up version of my personal notes, explaining how inodes (representing files on disk), file descriptors (used by programs), and sockets (for network communication) all fit together.","title":"Notes on i-nodes, File Descriptors, and Sockets"},{"content":"Kubernetes Client-Side Indexing This post is part of Kubernetes controller development. Check out the first part on Diving into controller-runtime | Manager if you are interested in controller-runtime.\nWhen working with Kubernetes Operators, read requests (get and list) to the Kubernetes API server are handled by an in-memory cache that is maintained by client-go to reduce the load on your API server. This cache can be enhanced with indexes to retrieve resources more efficiently.\nThis post explains the underlying indexing implementation in controller-runtime (and in client-go). We\u0026rsquo;ll explore how indexing works under the hood in these Kubernetes client libraries.\nIf you\u0026rsquo;re interested in practical examples of indexing, the Kubebuilder book and controller-runtime package documentation provide great practical examples and implementations.\nPurpose of Indexing Indexing in Kubernetes client libraries (client-go and controller-runtime) optimizes the performance of Kubernetes controllers by enabling efficient lookups of cached objects based on specific attributes or relationships. This is crucial for:\nEfficient Resource Retrieval: Finding resources matching certain criteria without scanning the entire cache. This is particularly useful when managing resources that depend on other resources.\nReduced API Server Load: Minimizing the number of API calls to the Kubernetes API server through caching. Most modern Kubernetes Operators utilize controller-runtime, where this behavior comes by default.\nResponsive Reconciliation: Enabling controllers to react to changes in related resources. This allows you to reconcile specific resources based on changes in their dependencies.\nLet\u0026rsquo;s explore these concepts through an example.\nScenario We\u0026rsquo;ll examine a simplified version of an example from the Kubebuilder book.\nNote: We\u0026rsquo;ll use controller-runtime in our examples. For general information about Kubernetes Operators and controller-runtime, please refer to my previous post.\nConsider a Custom Resource Definition (CRD) called MyApp that deploys your application on Kubernetes and uses a ConfigMap for application-specific configurations.\nThe MyApp spec references a ConfigMap which includes MyApp specific configuration. The MyApp controller reconciles the desired state based on the following MyApp custom resource:\n1 2 3 4 5 6 7 8 apiVersion: my.domain/v1alpha1 kind: MyApp metadata: name: myapp-staging spec: config: configMapRef: name: \u0026#34;myapp-staging-conf\u0026#34; When a ConfigMap specified via spec.config.configMapRef.name is updated, MyApp should reflect those changes. To achieve this, MyApp needs to watch ConfigMap resources.\nA basic initialization of the MyApp controller looks like this:\n1 2 3 ctrl.NewControllerManagedBy(mgr). For(\u0026amp;v1alpha1.MyApp{}). Owns(\u0026amp;corev1.ConfigMap{}) Here, the controller manages MyApp CR as the primary resource and ConfigMap as a secondary resource. The controller will reconcile on both MyApp CR and ConfigMap events (e.g., updates to either resource).\nWhile this approach works, its efficiency is arguable since the controller will reconcile on every change to MyApp and ConfigMap resources. For example, changes to ConfigMaps unrelated to MyApp configuration will trigger reconciliation. Since reconciliation logic is often costly, involving external API calls or write requests to the Kubernetes API, we should avoid unnecessary reconciliations.\nThere are various ways to filter reconciliation requests, but the idiomatic approach uses Predicates and Event handlers. Since I covered these in previous blog post, we\u0026rsquo;ll focus on scenarios where indexing is crucial.\nConsider three MyApp instances:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: my.domain/v1alpha1 kind: MyApp metadata: name: myapp-staging1 spec: config: configMapRef: name: \u0026#34;myapp-staging-conf\u0026#34; --- apiVersion: my.domain/v1alpha1 kind: MyApp metadata: name: myapp-staging2 spec: config: configMapRef: name: \u0026#34;myapp-staging-conf\u0026#34; --- apiVersion: my.domain/v1alpha1 kind: MyApp metadata: name: myapp-dev spec: config: configMapRef: name: \u0026#34;myapp-dev-conf\u0026#34; Here, myapp-staging1 and myapp-staging2 use a ConfigMap called myapp-staging-conf, while myapp-dev uses myapp-dev-conf.\nIn our reconciler, when a ConfigMap is updated, we want to update only the MyApp instances that use that particular ConfigMap. This is where indexing becomes handy, allowing us to somehow correlate MyApp instances with their corresponding ConfigMaps.\nIndexing Implementation To find dependent MyApp instances, we need to find all MyApp instances using a particular ConfigMap. For example, when myapp-staging-conf is updated, we want to trigger reconciliation for both myapp-staging1 and myapp-staging2.\nTo achieve that, we will add an index to the MyApp custom resource containing the name of its associated ConfigMap. This enables efficient retrieval of MyApp resources through this index.\nIn controller-runtime, indexers are configured through the Cache interface, which provides methods to index and retrieve Kubernetes objects efficiently. The cache wraps client-go informers and provides additional capabilities, including indexing.\nWhen creating a new manager with ctrl.NewManager(), it initializes a cache (cache.Cache) to hold informers and indexers.\nBefore starting the manager, add indexers via the FieldIndexer:\n1 2 3 4 5 6 7 8 9 10 mgr.GetFieldIndexer().IndexField( context.Background(), \u0026amp;v1alpha1.MyApp{}, \u0026#34;spec.config.configMapRef.name\u0026#34;, // Can be any string, not limited to field names func(obj client.Object) []string { myApp := obj.(*v1alpha1.MyApp) cmName := myApp.Spec.Config.ConfigMapRef.Name return []string{cmName} }, ) This indexes MyApp resources based on the myApp.Spec.Config.ConfigMapRef.Name field. When mgr.Start() is called, the cache initializes all informers and configures the indexers.\nIn the reconciler, we can use controller-runtime client with indexing capabilities to efficiently query objects:\n1 2 3 4 5 6 myAppList := \u0026amp;v1alpha1.MyAppList{} listOps := \u0026amp;client.ListOptions{ FieldSelector: fields.OneTermEqualSelector(\u0026#34;spec.config.configMapRef.name\u0026#34;, \u0026#34;myapp-staging-conf\u0026#34;), } r.List(ctx, myAppList, listOps) This lists all MyApp resources that use the ConfigMap \u0026ldquo;myapp-staging-conf\u0026rdquo;. The Kubernetes client searches the cache using the index key \u0026ldquo;spec.config.configMapRef.name\u0026rdquo;.\nFinishing the Controller initialization Now that we can list MyApp resources based on their associated ConfigMap, we can avoid unnecessary reconciliations. For example, when myapp-staging-conf is updated, we don\u0026rsquo;t need to reconcile the myapp-dev resource since it uses a different ConfigMap.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ctrl.NewControllerManagedBy(mgr). For(\u0026amp;v1alpha1.MyApp{}). Watches( \u0026amp;corev1.ConfigMap{}, handler.EnqueueRequestsFromMapFunc( func(ctx context.Context, configMap client.Object) []reconcile.Request { myAppList := \u0026amp;v1alpha1.MyAppList{} listOps := \u0026amp;client.ListOptions{ FieldSelector: fields.OneTermEqualSelector( \u0026#34;spec.config.configMapRef.name\u0026#34;, configMap.GetName(), ), } if err := r.List(ctx, myAppList, listOps); err != nil { return []reconcile.Request{} } requests := make([]reconcile.Request, len(myAppList.Items)) for i := range myAppList.Items { requests[i] = reconcile.Request{ NamespacedName: types.NamespacedName{ Name: myAppList.Items[i].Name, Namespace: myAppList.Items[i].Namespace, }, } } return requests }), builder.WithPredicates(predicate.NewPredicateFuncs( func(object client.Object) bool { // No ConfigMap filtering currently. // Add predicate functions here to filter ConfigMaps // For example, consider ConfigMaps with specific // annotations like `myapp/config` // The Enqueue function runs based on this return value return true }), ), ). Complete(r) Instead of owning all ConfigMap resources, the controller now watches ConfigMaps. With the help of predicate functions, we can filter ConfigMap resources events before enqueuing MyApp resources for reconciliation. After passing predicates, we specify which MyApp resources to reconcile within EnqueueRequestsFromMapFunc. Using our index, we can find MyApp instances using the updated ConfigMap, ensuring efficient reconciliation of only the relevant resources.\nHow While previous sections covered how to use controller-runtime for indexing resources, this section explores the underlying implementation details.\nKubernetes contains well-structured code patterns, and indexing is one of them. Although we used controller-runtime in our examples, it leverages client-go under the hood to handle the actual indexing operations.\nLet\u0026rsquo;s examine the implementation details that make indexing possible. While I can\u0026rsquo;t cover every implementation detail, we\u0026rsquo;ll focus on the most interesting aspects of controller-runtime and client-go\u0026rsquo;s indexing mechanisms.\nManager and Cache Setup The first step is instantiating a Manager from controller-runtime to handle the MyApp controller and its dependencies. We\u0026rsquo;ll use NewManager from controller-runtime/pkg/manager for manager instantiation.\nIf you\u0026rsquo;re new to Manager or want to understand its role in depth, check out my post on Diving into controller-runtime | Manager.\nAlthough Manager offers various configuration options, I\u0026rsquo;ll focus on the cache configuration, specifically the NewCache option. We typically use the default cache configuration for NewCache since it is a low-level primitive that rarely needs customization except in specific use cases. This is already mentioned in the docs, as follows:\n1 2 3 4 5 6 7 8 9 // NewCache is the function that will create the cache to be used // by the manager. If not set this will use the default new cache function. // // When using a custom NewCache, the Cache options will be passed to the // NewCache function. // // NOTE: LOW LEVEL PRIMITIVE! // Only use a custom NewCache if you know what you are doing. NewCache cache.NewCacheFunc By default, NewCache instantiates an informerCache instance from controller-runtime that uses NewSharedIndexInformer informer from client-go. The exception is when multi-namespace is configured for cache, in which case multiNamespaceCache from controller-runtime is used.\nIndex Field Implementation Now, our cache is instantiated and attached to Manager. What happens when we run following code piece to create an indexer for us?\n1 2 3 4 5 6 7 8 9 10 mgr.GetFieldIndexer().IndexField( context.Background(), \u0026amp;pkg.MyApp{}, \u0026#34;spec.config.configMapRef.name\u0026#34;, func(obj client.Object) []string { myApp := obj.(*pkg.MyApp) cmName := myApp.Spec.Config.ConfigMapRef.Name return []string{cmName} }, ) The function passed into IndexField will be referred as F later in the post.\nThe FieldIndexer here is the informerCache that we created while setting up the Manager, as mentioned above.\nIndexField gets informer of the given object, in our case MyAp, indexing key (spec.config.configMapRef.name), and a function F. In F, if you notice, we did not specify the namespace of the ConfigMap as it will be handled automatically by the controller-runtime under the hood.\ncontroller-runtime takes F and uses it in indexByField function (reference: here) and generates both namespaced and cluster-scoped variants of the extracted values.\nAll of these operations are wrapped in an anonymous function. This anonymous function actually runs F to get extracted values from each MyApp resources. The result of F will then be translated into special indexed value(s) as follows:\n__all_namespaces/\u0026lt;MyApp.metadata.name\u0026gt; \u0026lt;MyApp.metadata.namespace\u0026gt;/\u0026lt;MyApp.metadata.name\u0026gt; if resource (MyApp) is namespaced. In our case, since MyApp is namespaced resource (not cluster scoped resource such as ClusterRole), controller-runtime adds namespace for us. It also creates a special value with __all_namespaces/ prefix for our index key (spec.config.configMapRef.name) in order to allow listing regardless of the object namespace in cache.\nThen this anonymous function that wraps F is going to be passed into informer through adding an Indexer to the MyApp\u0026rsquo;s informer.\nUnderstanding Indexers and Indices We mentioned adding an Indexer to an informer but what is Indexer? What does it look like?\nIndexer is actual core component used in indexing logic. It keeps track of Indexer names and functions used to calculate indexed values, such as our F. Indexer names have a special format as field:\u0026lt;indexer_name\u0026gt;. So, our indexer name is going to be field:spec.config.configMapRef.name.\nIndexer and other related types are defined in client-go, as follows:\n1 2 3 4 5 6 7 8 9 10 11 // IndexFunc returns indexed values for objects, // such as ConfigMap names for MyApp resources like anonymous // function (that wraps F) mentioned above. type IndexFunc func(obj interface{}) ([]string, error) // Indexers maintain indexer names as keys and // their corresponding calculation functions as value. type Indexers map[string]IndexFunc // Indices stores the actual index data for each registered Indexer name. type Indices map[string]Index type Index map[string]sets.String For example, when we create a custom indexer named spec.config.configMapRef.name by calling IndexField, client-go adds it to the indexer with a special format:\nIndexers[field:spec.config.configMapRef.name]=\u0026lt;anonymous function generated in controller-runtime\u0026gt; Indices=nil Since we do not have any actual resource created at the time we call IndexField function, Indices is not yet populated; thus, nil.\nFor Indices and Index, assume that we already have a MyApp instance called myapptest.\nIndices will contain all indexes for the particular indexer. For our custom indexer field:spec.config.configMapRef.name, Indices is going to look like\n\u0026#34;field:spec.config.configMapRef.name\u0026#34; -\u0026gt; set(\u0026#34;default/myapptest\u0026#34;, \u0026#34;__all_namespaces/myapptest\u0026#34;) It contains all indexes calculated by the indexer called field:spec.config.configMapRef.name.\nCache Implementation Details Back to our actual work, recall that the anonymous function wrapping F needs to be passed to the informer by adding an Indexer to the MyApp\u0026rsquo;s informer. controller-runtime handles this here.\nThe MyApp\u0026rsquo;s Informer (of type sharedIndexInformer) takes this request and passes it to its indexer indexer field, which implements the Indexer interface.\nThe indexer field is implemented by cache that has a core field called cacheStorage serving as as the actual storage of the client-go cache. It implements ThreadSafeStore interface. Specifically, it uses threadSafeMap implementation.\nthreadSafeMap implementation uses items (map[string]interface{}) for objects in the cache, and index field which is a type of storeIndex.\n1 2 3 4 5 6 7 8 9 10 11 type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} index *storeIndex } // storeIndex implements the indexing functionality for Store interface type storeIndex struct { indexers Indexers indices Indices } The Indexers and Indices we discussed earlier are implemented here in the client-go cache.\nIndexer in action With the indexer registered, let\u0026rsquo;s create an instance of MyApp CR and examine what happens.\nWhen creating a MyApp instance, processDelta function handles cache updates; either removes or updates keys from the cache.\nSince the cache in client-go uses threadSafeMap, it checks our object in its storage using the object\u0026rsquo;s key. The key is generated by MetaNamespaceKeyFunc, with \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; format.\nSince we are creating the resource for the first time, this key does not exist in the cache (specifially in threadSafeMap). That\u0026rsquo;s why, client-go adds the object (MyApp instance) to its cache.\nclient-go updates underlying cache storage (threadSafeMap) by inserting the object into the key.\nkey: default/myapp-staging1 value: *pkg.MyApp At the moment, we have two indexers: the default \u0026ldquo;namespace\u0026rdquo; indexer that works with \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; metadata of objects, and our custom indexer called \u0026ldquo;field:spec.config.configMapRef.name\u0026rdquo;. client-go iterates through these indexers and updates them one by one.\nThe cool part here is that client-go actually runs Update method under the hood, as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // First we call Add, where key: default/myapp-staging1 and value is the actual // MyApp resource instance. func (c *threadSafeMap) Add(key string, obj interface{}) { c.Update(key, obj) } // Then Update method is called by Add method. It checks for the given key on // the threadSafeMap (which is an actual storage used in client-go cache) items. // And runs updateIndices. func (c *threadSafeMap) Update(key string, obj interface{}) { c.lock.Lock() defer c.lock.Unlock() oldObject := c.items[key] c.items[key] = obj c.index.updateIndices(oldObject, obj, key) } // This doc is directly taken from client-go code. // // updateIndices modifies the objects location in the managed indexes: // - for create you must provide only the newObj // - for update you must provide both the oldObj and the newObj // - for delete you must provide only the oldObj // updateIndices must be called from a function that already has a lock on the cache func (i *storeIndex) updateIndices(oldObj interface{}, newObj interface{}, key string) { for name := range i.indexers { i.updateSingleIndex(name, oldObj, newObj, key) } } Based on the oldObj and newObj values, updateSingleIndex determines which keys need to be added to or removed from the index of the given indexer.\nThe index update process follows this pattern:\n1 2 3 4 5 6 7 8 9 10 11 12 objectKey := \u0026#34;default/myapp-staging1\u0026#34; indexerName := \u0026#34;field:spec.config.configMapRef.name\u0026#34; index := threadSafeMap.index.Indices[indexerName] // set corresponds to Set that contains all MyApp // resources that use myapp-staging-conf ConfigMap. set := index[\u0026#34;__all_namespaces/myapp-staging-conf\u0026#34;] if myAppListUsingSpecificConfigMap == nil { set = sets.String{} index[\u0026#34;__all_namespaces/myapp-staging-conf\u0026#34;] = set } set.Insert(objectKey) Using the Indexer When we call r.List(), controller-runtime checks if the object is uncached (or unstructured). In our case, the objects are cached, controller-runtime client tries to List all MyApp instances from the cache, as follows:\n1 return c.cache.List(ctx, obj, opts...) As we initialized informerCache (recall Manager setup), controller-runtime client calls informerCache\u0026rsquo;s List method (defined here). Now, in informerCache\u0026rsquo;s List method, we first get MyApp object\u0026rsquo;s informer. If the informer has started, we try to run List method on MyApp informer\u0026rsquo;s cache, which delegates operation to client-go.\nRecall the following code piece:\n1 2 3 4 5 6 7 8 myAppList := \u0026amp;pkg.MyAppList{} listOps := \u0026amp;client.ListOptions{ FieldSelector: fields.OneTermEqualSelector( \u0026#34;spec.config.configMapRef.name\u0026#34;, \u0026#34;myapp-staging-conf\u0026#34;, ), } r.List(ctx, myAppList, listOps) In the list options passed to r.List, we specified our indexer key. When using a FieldSelector, the cache requires an exact match for the selector. Then, controller-runtime goes through each Indexers (in ourcase we have two, one field:spec.config.configMapRef.name and namespace one.)\nSince we haven\u0026rsquo;t specified a Namespace in ListOpts, controller-runtime will look for the __all_namespaces/myapp-staging-conf indexed value in the index called field:spec.config.configMapRef.name.\n- indexName: \u0026#34;field:spec.config.configMapRef.name\u0026#34; indexName is derived from ListOptions - indexedValue: \u0026#34;spec.config.configMapRef.name\u0026#34; indexedValue is derived from ListOptions After getting this informations, controller-runtime actually forwards our request to client-go\u0026rsquo;s Indexer interface\u0026rsquo;s ByIndex method which is documented as follows:\n1 2 3 // ByIndex returns the stored objects whose set of indexed values // for the named index includes the given indexed value ByIndex(indexName, indexedValue string) ([]interface{}, error) Remember how we call this client-go method from controller-runtime\n1 objs, err = indexer.ByIndex(\u0026#34;field:spec.config.configMapRef.name\u0026#34;, \u0026#34;__all_namespaces/myapp-staging-conf\u0026#34;) Based on its documentation, ByIndex looks for stored objects for key \u0026ldquo;__all_namespaces/myapp-staging-conf\u0026rdquo; in the index of \u0026ldquo;field:spec.config.configMapRef.name\u0026rdquo; indexer\nThis may sound complicated but to simplify this flow, let\u0026rsquo;s re iterate through the following types:\n1 2 3 4 5 6 7 8 9 10 11 12 13 // the key is indexer\u0026#39;s name, such as \u0026#34;field:spec.config.configMapRef.name\u0026#34; // the value is IndexFunc, such as the anonymous function mentioned previously. type Indexer map[string]IndexFunc // the key is indexer\u0026#39;s name, such as \u0026#34;field:spec.config.configMapRef.name\u0026#34; // the value is Index type Indices map[string]Index // the key is indexed value, such as __all_namespaces/myapp-staging-conf // the value is set of strings that corresponds to // \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; metadata of the objects. // For ex, default/myapp-staging1 type Index map[string]sets.String Now, we have set including / metadata of the MyApp resources. Then, threadSafeMap converts this Set to a list, and List operation is done by using the indexing functionality that we added.\nConclusion Throughout this post, we\u0026rsquo;ve walked through Kubernetes client-side indexing, exploring both its practical use and internal workings. While this post shares one approach to using indexing for improving controller performance, there are many other patterns and best practices in the Kubernetes ecosystem that might better suit your specific needs.\nIf you\u0026rsquo;re interested in learning more about controller development, you might find my earlier post about Controller Runtime Manager helpful, where I shared my experience with the core components of controllers. The Kubebuilder documentation is also an excellent resource that provides more comprehensive examples and different indexing use cases.\nI hope sharing these experiences with client-side indexing has been helpful for your journey :) Every controller and operator has unique requirements, and I\u0026rsquo;d be interested to hear about your approaches to similar challenges. Feel free to share your thoughts or ask questions about implementing indexing in your controllers.\nIf you notice any mistakes or have feedback, feel free to reach out to me on Twitter, LinkedIn, or GitHub.\nReferences https://book.kubebuilder.io/ https://pkg.go.dev/sigs.k8s.io/controller-runtime ","permalink":"https://buraksekili.github.io/articles/client-k8s-indexing/","summary":"Kubernetes Client-Side Indexing This post is part of Kubernetes controller development. Check out the first part on Diving into controller-runtime | Manager if you are interested in controller-runtime.\nWhen working with Kubernetes Operators, read requests (get and list) to the Kubernetes API server are handled by an in-memory cache that is maintained by client-go to reduce the load on your API server. This cache can be enhanced with indexes to retrieve resources more efficiently.","title":"Kubernetes Client-Side Indexing"},{"content":"A thread pool is a software design pattern where a set of worker threads is created to execute tasks concurrently. Instead of creating a new thread for each task, which can be resource-intensive, tasks are submitted to the pool and executed by available worker threads.\nThis blog post will go through a simple thread pool implementation - similar to the one in Rust book - with a couple of simple enhancements.\nUnlike Go\u0026rsquo;s lightweight goroutines, Rust\u0026rsquo;s threads map directly to OS threads, making thread pools widely used technique for resource management and performance optimization.\nAt its core, a thread pool is a collection of worker threads that are ready to execute given tasks. Instead of spawning a new thread for each task - potentially costly operation in Rust - tasks are submitted to the pool and executed by available workers (threads). This pattern is useful in various scenarios: web servers handling multiple client connections, task scheduling systems processing queued jobs, parallel data processing applications crunching large datasets, I/O-bound applications managing concurrent operations, and more.\nThere are various benefits of thread pools. They offer resource management by limiting the number of active threads, preventing system resource exhaustion. Performance may be optimized as the overhead of repeated thread creation and destruction is eliminated. Thread pools also allow for predictable resource consumption, enabling developers to control and forecast the maximum thread usage. Load balancing comes as a natural consequence, with tasks distributed evenly across available threads.\nHowever, thread pools are not without their challenges.\nIntroducing additional complexity to the codebase, requiring careful management of shared state and synchronization to avoid pitfalls like deadlocks and race conditions, For very short-lived tasks, the overhead of task submission might outweigh the benefits. This means that sometimes if your task is not long-running or not compute expensive, sending this execution to another thread might be longer than the time spent on preparation of the task and sending it to a thread. Let\u0026rsquo;s say a task takes 1 ms to execute directly. If the overhead of submitting to and retrieving from the thread pool takes 10 ms, you\u0026rsquo;re spending 11 ms total instead of just 1. Determining the optimal pool size can be a tricky process as well. Contention or starvation may happen. For instance, while workers are busy with long running tasks, shorter tasks are waiting in line for a long time. In the context of Rust, additional language-specific considerations arise. The ownership system and Rust\u0026rsquo;s memory safety guarantees require careful implementation - that\u0026rsquo;s why I wanted to implement the pooling from scratch. It helped me to practice concurrency practices in Rust while considering ownership model.\nNow, let\u0026rsquo;s start implementing a thread pool in Rust.\nImplementation Please read Rust book\u0026rsquo;s thread pool section if you haven\u0026rsquo;t read before: https://rust-book.cs.brown.edu/ch20-02-multithreaded.html\nOur thread pool, as explained above, will have already initialized threads which are ready to execute a task. Here, we will use Worker to represents an individual thread in our pool.\nWorker Each worker runs in its own thread, continuously polling for jobs. Its purpose is to process the given jobs.\nYou may ask who gives the task to worker. Worker will poll the jobs from a queue which will be shared among all other workers. If no jobs are available, the worker needs to wait until another task is submitted to queue.\n1 2 3 4 5 6 7 8 struct Worker { // id corresponds to the arbitrary id for the thread // useful while debugging :) id: usize, // thread is the actual thread which is going // to execute a real task. thread: Option\u0026lt;thread::JoinHandle\u0026lt;()\u0026gt;\u0026gt;, } So, the requirements that we expect from the worker:\nIt will execute the task in its thread, Thread is part of Worker struct. So, we need to somehow implement a logic to get the task. We cannot store the task in Worker struct as the same thread (or Worker) can be reused after executing its task - this is the idea of the thread pooling. Thread safe queue to pull the tasks. It will be part of the constructor of the Worker. So that, when we spawn the thread, we also start listening this queue. The queue needs to be shared among all workers. That\u0026rsquo;s why it needs to be thread-safe. While creating a Worker, we will use std::thread and spawn the thread. In the closure of the thread, we will run our logic to pull tasks from the queue.\nIf there is a task on the queue, the thread will execute the task, and then waits for a next task to be scheduled for itself. As Worker needs to re-run tasks after executing a single one, it needs to run continous loop in order not to miss any task pushed into queue.\nIf no task is provided, the queue will return None. In that case, the thread needs to wait for next task to be scheduled. Therefore, thread needs to wait within the loop.\nThere are various way to do that but most simple one is sleeping. However, sleeping leads to busy-waiting which wastes CPU cycles. Also, if there are new tasks registered while the thread is sleeping, the worker will run the thread after a duration of sleep which causes a latency.\nInstead of sleeping, we will use Condvar which is a synchronisation primitive to allow threads to wait for some condition to become true. It\u0026rsquo;s often used in conjunction with a Mutex to provide a way for threads to efficiently wait for changes in shared state. So, we will use Condvar to wait for new tasks to be available as it allows immediate response when the job is available which improves thread utilization.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 impl Worker { fn new( id: usize, job_queue: Arc\u0026lt;SegQueue\u0026lt;Job\u0026gt;\u0026gt;, job_signal: Arc\u0026lt;(Mutex\u0026lt;bool\u0026gt;, Condvar)\u0026gt;, running: Arc\u0026lt;AtomicBool\u0026gt;, ) -\u0026gt; Worker { let thread = thread::spawn(move || loop { match job_queue.pop() { Some(Job::Task(task)) =\u0026gt; if let Err(_) = task() {}, Some(Job::Shutdown) =\u0026gt; { break; } None =\u0026gt; { let (lock, cvar) = \u0026amp;*job_signal; let mut job_available = lock.lock().unwrap(); while !*job_available \u0026amp;\u0026amp; running.load(Ordering::Relaxed) { job_available = cvar .wait_timeout(job_available, Duration::from_millis(100)) .unwrap() .0; } *job_available = false; } } }); Worker { id, thread: Some(thread), } } } Our queue depends on crossbeam\u0026rsquo;s crossbeam_queue::SegQueue which is a concurrent queue implementation. It\u0026rsquo;s designed for high-performance concurrent scenarios. Also, it is lock-free which is an important aspect of SegQueue.\nOur queue is actually a linked list of Job which will look like following:\n1 2 3 4 pub enum Job { Task(Box\u0026lt;dyn FnOnce() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; + Send + \u0026#39;static\u0026gt;), Shutdown, } Job has two possible values, Task and Shutdown.\nThe Shutdown job type allows us to break the loop and terminate the thread gracefully. If a thread receives a Shutdown from the queue, it will stop executing.\nOkay, now check the Task. I know it looks too complicated at first glance, it still looks complicated to me. I\u0026rsquo;ll try to break it down.\nBox\u0026lt;dyn FnOnce() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; + Send + 'static\u0026gt; Here we have two main parts; the FnOnce() and the return type of this closure, Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; + Send + 'static\nFnOnce(): It is a closure without taking any arguments. It is same as thread::spawn(|| {}) closure. helps us to prevent accidentally call a task multiple times when it\u0026rsquo;s not safe to do so. -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; is the return type of this closure. On success, it returns () (unit, or void). On error, it returns a boxed trait object of std::error::Error. Trait object is one of the ways in Rust to write polymorphic code. Especially, dynamic dispatch uses trait objects to resolve generic function calls at runtime.\nBox\u0026lt;dyn ...\u0026gt;: Box is used for heap allocation. dyn indicates a trait object, allowing for dynamic dispatch. Send: is a trait which ensures the closure can be safely sent between threads. 'static: is a lifetime bound ensuring the closure doesn\u0026rsquo;t contain any non-static references. 'static bound ensures a type is safe to use without lifetime constraints. Without 'static, we might create closures that reference stack-local variables, leading to use-after-free bugs. In the arguments of Worker::new method, the use of Arc allows safe sharing of the job queue and signaling mechanism between threads. This is crucial in Rust\u0026rsquo;s ownership model for concurrent programming. Otherwise, shared data cannot be used safely among multiple threads as it will cause lots of critical bugs.\nThread Pool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 pub struct ThreadPool { // workers keep track of all worker threads. workers: Vec\u0026lt;Worker\u0026gt;, // job_queue corresponds to a shared queue for distributing jobs to workers. job_queue: Arc\u0026lt;SegQueue\u0026lt;Job\u0026gt;\u0026gt;, // job_signal is notifier for workers when new jobs are available. job_signal: Arc\u0026lt;(Mutex\u0026lt;bool\u0026gt;, Condvar)\u0026gt;, // running indicates whether the threadpool is actively running or not. // it is mainly checked by worker threads to understand the status // of the pool. running: Arc\u0026lt;AtomicBool\u0026gt;, } impl ThreadPool { pub fn new(size: usize) -\u0026gt; ThreadPool { assert!(size \u0026gt; 0); let job_queue = Arc::new(SegQueue::new()); let job_signal = Arc::new((Mutex::new(false), Condvar::new())); let mut workers = Vec::with_capacity(size); let running = Arc::new(AtomicBool::new(true)); for id in 0..size { workers.push(Worker::new( id, Arc::clone(\u0026amp;job_queue), Arc::clone(\u0026amp;job_signal), Arc::clone(\u0026amp;running), )); } ThreadPool { workers, job_queue, job_signal, running, } } pub fn execute\u0026lt;F\u0026gt;(\u0026amp;self, f: F) -\u0026gt; Result\u0026lt;(), ThreadPoolError\u0026gt; where F: FnOnce() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; + Send + \u0026#39;static, { // We create a new Job::Task, wrapping our closure \u0026#39;f\u0026#39; let job = Job::Task(Box::new(f)); // Push this job to our queue self.job_queue.push(job); // Signal that a new job is available let (lock, cvar) = \u0026amp;*self.job_signal; let mut job_available = lock.lock().unwrap(); *job_available = true; cvar.notify_all(); Ok(()) } } ThreadPool::new creates a fixed number of workers, each with shared access to the job queue and signaling mechanism.\nThreadPool::execute takes a generic parameter called F which implements FnOnce() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; + Send + 'static. This type is the same type used for Job::Task - so that we can use argument f as Job. The argument passed in execute is actually a task that needs to be run in any of the workers.\nFor example,\n1 2 3 4 5 let x = 3; pool.execute(move || { println!(\u0026#34;the task is running with value {}\u0026#34;, x); Ok(()) }) The execute will take closure function, f: F, as an argument and it is used as Job::Task wrapped in Box. Some of the reasons behind this are:\nallowing us to send different types of jobs through the same queue. Workers can distinguish between actual tasks and shutdown signals. providing a uniform type (or interface if you are familiar with interfaces in other languages) for our job queue. All items in the queue are of type Job, regardless of the closure they contain. But, why do we use Box? The Box is crucial here for several reasons:\nWe are using dyn FnOnce() which is a trait object. In Rust, trait objects must be behind a pointer, and Box provides this. Closures can capture variables from their environment - as we did in the example above - which makes their size unknown at compile time. Box puts the closure on the heap, giving it a known size (the size of a pointer) at compile time. Box allows us to take ownership of the closure and move it into the Job enum, which is necessary because the closure will be executed in a different thread. This design actually allows us to:\nHandle different types of jobs (tasks and shutdown signals) uniformly. Move closures between threads safely, respecting Rust\u0026rsquo;s ownership rules. Deal with closures of different sizes and types in a unified manner. Graceful Shutdown 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 impl ThreadPool { pub fn shutdown(\u0026amp;mut self, timeout: Duration) -\u0026gt; Result\u0026lt;(), ThreadPoolError\u0026gt; { let start = Instant::now(); // Step 1: Signal all workers to stop self.running.store(false, Ordering::SeqCst); // Step 2: Wake up all waiting threads let (lock, cvar) = \u0026amp;*self.job_signal; match lock.try_lock() { Ok(mut job_available) =\u0026gt; { *job_available = true; cvar.notify_all(); } Err(_) =\u0026gt; { // We couldn\u0026#39;t acquire the lock, but we\u0026#39;ve set running to false, // so workers will eventually notice println!(\u0026#34;Warning: Couldn\u0026#39;t acquire lock to notify workers. They will exit on their next timeout check.\u0026#34;); } } // Step 3: Wait for all workers to finish for worker in \u0026amp;mut self.workers { if let Some(thread) = worker.thread.take() { // Step 4: Calculate remaining time let remaining = timeout .checked_sub(start.elapsed()) .unwrap_or(Duration::ZERO); // Step 5: Check if we\u0026#39;ve exceeded the timeout if remaining.is_zero() { return Err(ThreadPoolError::ShutdownTimeout); } // Step 6: Wait for the worker to finish if thread.join().is_err() { return Err(ThreadPoolError::ThreadJoinError(format!( \u0026#34;Worker {} failed to join\u0026#34;, worker.id ))); } } } // Step 7: Final timeout check if start.elapsed() \u0026gt; timeout { Err(ThreadPoolError::ShutdownTimeout) } else { Ok(()) } } } To notify workers polling the queue, we set the running flag to false. This is an atomic operation that immediately signals all workers that a shutdown is in progress. Since all threads need to finish executing the task they assigned to or stop waiting for next job (through cvar.wait_timeout method in Worker::new method) for proper shutdown, job_available is set to true, which triggers all threads and we notify all threads waiting on the condition variable.\nWe use try_lock() instead of lock() while notifying the threads. This attempts to acquire the lock but returns immediately if it can\u0026rsquo;t, rather than waiting until acquiring the lock.\nIf the lock is acquired, we proceed as before: set job_available to true and notify all waiting threads. By doing that, we can ensure that idle workers that were waiting on cvar.wait_timeout will wake up and notice the shutdown signal.\nIf the lock is not acquire successfully, the shutdown process continues. As running is already set to false, which all workers check periodically and wait_timeout in Worker\u0026rsquo;s main loop will expire - so they\u0026rsquo;ll wake up eventually and notice that running is false.\nAfter sending signals to notify threads, then we iterate through all workers, attempting to join each thread. Before joining each thread, the remaining time is calculated based on our timeout. This ensures we respect the overall timeout even if a worker is stuck (e.g., in an infinite loop), the timeout ensures we don\u0026rsquo;t wait forever.\nTo use shutdown method explicitly, the Drop can be implemented where the shutdown method can be triggered whenver ThreadPool is dropped, such as ThreadPool variable going out of scope.\n1 2 3 4 5 6 7 impl Drop for ThreadPool { fn drop(\u0026amp;mut self) { if !self.workers.is_empty() { let _ = self.shutdown(Duration::from_secs(2)); } } } Implementing a thread pool in Rust offers a great opportunity to practice Rust\u0026rsquo;s concurrency and memory safety paradigms. Throughout this blog post, I\u0026rsquo;ve tried to explain some concepts such as atomic operations, condition variables, and Rust\u0026rsquo;s ownership system in a practical context. While the implementation provides a solid foundation, there\u0026rsquo;s always room for improvement and optimization - so, ofc not use it on anywhere :) There are plenty of great crates including crossbeam\u0026rsquo;s. I just developed this thread pooling to practice concepts aforementioned. Consider exploring advanced features like work stealing algorithms or dynamic pool sizing to further enhance performance.\nIf you notice any mistakes or have feedback, feel free to reach out to me on Twitter, LinkedIn, or GitHub.\nReferences https://doc.rust-lang.org/book/ch20-02-multithreaded.html https://github.com/crossbeam-rs/crossbeam ","permalink":"https://buraksekili.github.io/articles/thread-pooling-rs/","summary":"A thread pool is a software design pattern where a set of worker threads is created to execute tasks concurrently. Instead of creating a new thread for each task, which can be resource-intensive, tasks are submitted to the pool and executed by available worker threads.\nThis blog post will go through a simple thread pool implementation - similar to the one in Rust book - with a couple of simple enhancements.","title":"Thread Pooling in Rust "},{"content":"Working with Custom Data Format in Rust using serde If you need to perform serialization or deserialization in Rust, you’ve most likely used the serde before. I’m currently learning Rust, and I found myself needing similar thing.\nTo get familiar with the Rust ecosystem, I decided to develop a simple key-value store. Initially, the engine for this key-value store was designed to work with JSON objects, as JSON is a widely-used format that’s straightforward to use with web clients. Also, as most of the languages and platforms already support JSON, it is a good choice to start with.\nWhat I wanted to achieve was that users send JSON objects to indicate their request, such as \u0026ldquo;give me the value of key \u0026rdquo;, which can be represented as { \u0026quot;key\u0026quot;: \u0026quot;some_key\u0026quot; } in JSON. The response is also formatted as a JSON object, like {\u0026quot;value\u0026quot;: \u0026quot;value_of_the_key\u0026quot;}. The concept and requirements of this server are quite simple and clear.\nI generally use Go for my projects, where the JSON serialization/deserialization process is straightforward. You can manage it with some tags on your struct, and the rest is handled by Go itself. In Rust, achieving the same thing is also straightforward with serde. You only need to add a few macros, and serde will automatically implement serialization and deserialization methods for you. As JSON is one of the most popular data formats, you can set this up with just a few lines of code. I am not going to give details about how to achieve this as serde documentation has handful examples regarding this.\nAfter checking similar projects, I realized that most of the engines do not use these widely adopted data formats in place with their storage engine. Of course, the reasons of this may vary but the fundamental concerns are more or less the same: performance and simplicity. In my case, users only get, set or rm the key. So, using JSON is a bit overkill here. I totally agree that it is quite straightforward to implement and get start with JSON but what happens if I want to introduce a simple - and to be honest useless and dumbest - data format to interact with my key-value storage engine? Considering that my learning purpose of Rust, the idea looks okay to me.\nThen, I started to focus on how to work with custom data format with serde? So that I can serialize Rust data types to my data format and deserialize some byte sequence such as strings back to Rust types?\nConcepts Before jumping into implementation details, it\u0026rsquo;d be better to get familiar with some core concepts that we are going to be using.\nData Format First things first, let’s try to understand what the data format is. Simply put, a data format defines how the data is stored and structured. For example, well-known data formats such as JSON, CSV, and TOML can store the same data but in different ways.\nYou probably use these formats on a daily basis as they are widely adopted and used. Almost every language support these data formats more or less. Although these formats use different syntax while representing the data, the underlying data they represent is the same.\nIn my case, I wanted to introduce a simple custom data format, which looks like this:\n+:\u0026lt;cmd_name\u0026gt; \u0026lt;key\u0026gt; \u0026lt;optional_value\u0026gt;: This format allows me to store a sequence of commands - essentially the requests that users send to the key-value engine, such as fetching a key - in a log file. This format is of course not complicated and maybe even not quite helpful compared to JSON but it is of course cleaner and easier for me to work with. For instance, using this format makes it easy for my parser to determine where a command request starts in the file, as each command is prefixed with +: and ends with a colon :.\nThis data format of course is NOT a JSON (or YAML, CSV and whatever) which means I cannot use existing deserializer of already knowns data formats. This means I need to implement my own basic serializer and deserializer to convert Rust data structures, such as enum or struct, into this custom format and vice versa.\nFor simple use cases like mine, you do not even need to write a full Serializer and Deserializer using serde. You can implement a custom deserialize method and a visitor to handle most tasks. My primary goal in creating these custom serializers and deserializers is to learn and understand the process better.\nData Type The data types refer to the way data is stored in memory and classified in the language that we use. In Rust and in many other languages, data types can be simple types such as integers, floating number, booleans and composite ones like structs, enums, and classes.\nOn the other hand, data format is how this data type is stored and structured in the storage.\nFor example, the following Get struct in Rust corresponds to the data type:\n1 2 3 struct Get { key: String, } where, storing this data type as a JSON string as {\u0026quot;key\u0026quot;: \u0026quot;some_key\u0026quot;} is the data format.\nSerialization and Deserialization With that being said, serialization is the process of converting or transforming these data types into a specific data format. For example, if you use a JSON serializer, it will convert your Rust data types into a JSON compatible string representation.\nThe deserialization is kind of the reverse process of the serialization where it takes your input (like as a string, byte sequence or in binary) and converts this input back into the data types, which might be struct or vector etc.\nSerialization As clearly mentioned in serde documentation, serde is NOT a parsing library. So, i am not going to dive into how to parse the stream.\nIn our case, the request that users send is represented as enum type, as follows:\n1 2 3 4 5 pub enum Request { Get { key: String }, Set { key: String, val: String }, Rm { key: String }, } First start with serializing this data type into our custom data format:\n+:\u0026lt;cmd_name\u0026gt; \u0026lt;key\u0026gt; \u0026lt;optional_value\u0026gt;: In order to serialize our enum, we need to implement Serialize trait for our Request type. This trait only has one required method called serialize. For most of the time, you do not need to implement the trait from scratch. There is a helper procedural macro called serde_derive to implement the trait for you. So, let\u0026rsquo;s add this macro to our type.\n1 2 3 4 5 6 7 8 use serde::Serialize; #[derive(Serialize)] pub enum Request { Get { key: String }, Set { key: String, val: String }, Rm { key: String }, } Eventually, this macro generates the following code (which shows the generated code in simplified manner) for Request struct.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // for more, please check https://docs.rs/serde/1.0.208/serde/ser/trait.Serialize.html impl Serialize for Request { fn serialize\u0026lt;S\u0026gt;(\u0026amp;self, serializer: S) -\u0026gt; Result\u0026lt;S::Ok, S::Error\u0026gt; where S: Serializer, { match *self { Request::Get { ref key } =\u0026gt; { let mut s = serializer.serialize_struct_variant(\u0026#34;Request\u0026#34;, 0, \u0026#34;Get\u0026#34;, 1)?; s.serialize_field(\u0026#34;key\u0026#34;, key)?; s.end() } Request::Set { ref key, ref val } =\u0026gt; { let mut s = serializer.serialize_struct_variant(\u0026#34;Request\u0026#34;, 1, \u0026#34;Set\u0026#34;, 2)?; s.serialize_field(\u0026#34;key\u0026#34;, key)?; s.serialize_field(\u0026#34;val\u0026#34;, val)?; s.end() } Request::Rm { ref key } =\u0026gt; { let mut s = s.serialize_struct_variant(\u0026#34;Request\u0026#34;, 2, \u0026#34;Rm\u0026#34;, 1)?; s.serialize_field(\u0026#34;key\u0026#34;, key)?; s.end(s) } } } } Instead of manually running serialize_field on each field of our enum, the macro will automatically generate the necessary code for us. This makes it easier to use in most cases.\nTo understand the flow, let’s break down what the serialize method does. It knows how to instruct Serializer to serialize the Request struct. Then, for each field in the enum, it calls serialize_field by using Serializer that is passed to serialize method.\nIf you want to use an existing serializer, like a JSON serializer, you can pass that serializer into the serialize method. The serializer will then handle the serialization process for you, converting your Rust data types into a JSON compatible format.\nHowever, in our case, we want to serialize the Request data type into our custom data format, which is not compatible with JSON or any other standard data formats. This means we need to implement our own custom serialization logic to handle this specific format - at least as scope of this blog post :).\nTo write our own Serializer, we need to implement serde::ser::Serializer trait. If you check the trait, it includes lots of required methods. But of course, not all of these methods need to be implemented in every case. Most of the methods are related to serializing specific types, like structs or i32.\nIn our case, we need to serialize a struct type. Based on the code generated by Serialize macro, we call serialize_struct_variant method. Therefore, we\u0026rsquo;ll definitely need to implement this method.\nAlso, we need to specify some required associated types of Serializer trait: Ok, Error and SerializeStructVariant in the Serializer trait.\ntype Ok corresponds to the output type that we generate after serialization. In our case, we can use () for Ok value since we are going to store the the serialization result in-memory and then write it to io::Write.\ntype Error corresponds to the error type that we may face during serialization. For the error type, we can define a custom Error type by following conventions here.\ntype SerializeStructVariant corresponds to the type returned from the serialize_struct_variant method. We can set SerializeStructVariant to Self, meaning our Serializer will be returned as the result of the serialize_struct_variant method. This allows us to use the serialization methods that we define within our custom Serializer.\nHere’s a simplified version of what our Serializer implementation will look like (omitting other methods that we don\u0026rsquo;t need to implement):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 impl\u0026lt;\u0026#39;a\u0026gt; ser::Serializer for \u0026amp;\u0026#39;a mut KvRequestSerializer { type Ok = (); type Error = Error; type SerializeStructVariant = Self; fn serialize_struct_variant( self, _name: \u0026amp;\u0026#39;static str, _variant_index: u32, variant: \u0026amp;\u0026#39;static str, _len: usize, ) -\u0026gt; std::result::Result\u0026lt;Self::SerializeStructVariant, Self::Error\u0026gt; { let req_type = match variant { \u0026#34;Set\u0026#34; =\u0026gt; Ok(\u0026#34;set\u0026#34;), \u0026#34;Get\u0026#34; =\u0026gt; Ok(\u0026#34;get\u0026#34;), \u0026#34;Rm\u0026#34; =\u0026gt; Ok(\u0026#34;rm\u0026#34;), _ =\u0026gt; Err(Error::InvalidData(String::from(\u0026#34;invalid request provided\u0026#34;))), }?; self.output += \u0026#34;+:\u0026#34;; self.output += req_type; Ok(self) } // and other required methods and types... } impl\u0026lt;\u0026#39;a\u0026gt; ser::SerializeStructVariant for \u0026amp;\u0026#39;a mut KvRequestSerializer { type Ok = (); type Error = Error; fn serialize_field\u0026lt;T\u0026gt;( \u0026amp;mut self, _key: \u0026amp;\u0026#39;static str, value: \u0026amp;T, ) -\u0026gt; std::result::Result\u0026lt;(), Self::Error\u0026gt; where T: ?Sized + Serialize, { value.serialize(\u0026amp;mut **self)?; Ok(()) } fn end(self) -\u0026gt; std::result::Result\u0026lt;Self::Ok, Self::Error\u0026gt; { self.output += \u0026#34;:\\n\u0026#34;; Ok(()) } } So, for example in order to serialize Request::Get {key: \u0026quot;abc\u0026quot;}, based on the serialize method (which is generated by Serialize macro)\n1 2 3 4 5 6 7 8 9 10 11 // serialize_struct_variant returns Result\u0026lt;Self::SerializeStructVariant, Self::Error\u0026gt; // which means succesfull results yield Self::SerializeStructVariant. // In our case, we defined `type SerializeStructVariant` as Self again. // So, the result of `serialize_struct_variant` method will be `KvRequestSerializer`. // // In the following code, `s` is type of KvRequestSerializer // which implements SerializeStructVariant trait. So that we can serialize the structs. let mut s = serializer.serialize_struct_variant(\u0026#34;Request\u0026#34;, 0, \u0026#34;Get\u0026#34;, 1)?; // now, s points to KvRequestSerializer and we already implement SerializeStructVariant trait. s.serialize_field(\u0026#34;key\u0026#34;, key)?; s.end() This flow will be resulted through following process:\nKvRequestSerializer\u0026rsquo;s serialize_struct_variant method. Here, we have information about how our Rust data type look like. KvRequestSerializer\u0026rsquo;s serialize_field method in SerializeStructVariant trait implementation. Here, we know each key of the Request::Get enum and its values KvRequestSerializer\u0026rsquo;s serialize_str. Then we pass the value of key field to serialize_str. So that we can form our desired data format Lastly we call end method from SerializeStructVariant During the process, we store the result of each operation in the output field of our Serializer. At the end, we can pass this output to our desired output location.\nNow, we implemented our serializer. Let\u0026rsquo;s write a simple test to verify the result. Before jumping into test cases, let\u0026rsquo;s define a function that eases usage of our serializer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pub fn serialize\u0026lt;T: ser::Serialize\u0026gt;(request: \u0026amp;T) -\u0026gt; String { let mut serializer = KvRequestSerializer { output: String::new(), }; request.serialize(\u0026amp;mut serializer).expect(\u0026#34;failed to serialize\u0026#34;); serializer.output } #[test] fn test_serialization_request_struct() { use crate::request::Request; let get_request = Request::Get { key: \u0026#34;get_key_testing\u0026#34;.to_owned(), }; let expected_get = \u0026#34;+:get get_key_testing:\\n\u0026#34;; assert_eq!(serialize(\u0026amp;get_request), expected_get); } Now that our serializer works as expected, let\u0026rsquo;s move on to deserializing our custom data format into the Request type by implementing our own deserializer.\nDeserialization Writing a custom deserializer can be more complicated than writing a serializer. I found myself confused quite often while working on the implementation at first. However, I\u0026rsquo;ll try to simplify the explanation as much as possible.\nIn serde, deserialization is a two-phase process that involves both a Deserializer and a Visitor. Let\u0026rsquo;s start with the Deserializer.\nDeserializer is responsible for interpreting the input, which is a data format in form of string, byte, binary etc., and matching this input data format to serde data model, such as integer, sequence and so on. Here, the serde data model refers to the data types defined within serde, which correspond closely to Rust\u0026rsquo;s type system. For example, bool in serde corresponds to the boolean type in Rust. The serde documentation provides a clear explanation of the data model, including an example using OsString, which is highly recommended if you\u0026rsquo;re not already familiar with serde\u0026rsquo;s data model. Please refer to the docs https://serde.rs/data-model.html.\nOnce the Deserializer matches input data to the appropriate serde data model, a Visitor is then used to analyze this generic data and convert it into the specific data type we want to achieve.\nAlthough this process may sound complicated, let\u0026rsquo;s break it down using a concrete example based on our case. Suppose we have a string like get abc:. Our deserialize method will call deserialize_str on our custom deserializer. This flow is similar to how the Serializer calls serialize_struct_variant, knowing that the data type is a struct. In our case, we know that our data format is a string that contains a single request. Thus, we will call deserialize_str. As opposed to serialize method, now we do not need to use macro to autogenerate deserialize method for Request type. As we already know that the input data format will be a string, we will implement deserialize method in a way that it will call deserialize_str method of our custom deserializer.\nWhen the deserializer\u0026rsquo;s deserialize_str method is called, it will, in turn, call the visit_str method on a visitor that we provide. This means we need to implement our own Visitor to handle string representation of our custom data format. Finally, within the visit_str method of our Visitor, we will parse the string and create the corresponding Request enum type based on the input.\nLet\u0026rsquo;s try to implement this deserialization process. As we did with the Serializer, we\u0026rsquo;ll start with the deserialize method for our data type - Request enum - which will instruct our custom deserializer.\n1 2 3 4 5 6 7 8 impl\u0026lt;\u0026#39;de\u0026gt; de::Deserialize\u0026lt;\u0026#39;de\u0026gt; for Request { fn deserialize\u0026lt;D\u0026gt;(deserializer: D) -\u0026gt; Result\u0026lt;Self, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { deserializer.deserialize_str(RequestVisitor) } } If we look at the deserialize method, you\u0026rsquo;ll notice that we pass our deserializer to this method (similar to how we did it with the serialize method). Since we know our custom data format is a string, we\u0026rsquo;ll directly call deserialize_str. This is actually suitable for our simple data format. However, we also need to pass a RequestVisitor as an argument to deserialize_str. This requires us to create a visitor that implements the de::Visitor trait. The RequestVisitor will process the string input and try to generate the appropriate Request enum.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 pub struct RequestVisitor; impl\u0026lt;\u0026#39;de\u0026gt; de::Visitor\u0026lt;\u0026#39;de\u0026gt; for RequestVisitor { type Value = Request; fn expecting(\u0026amp;self, formatter: \u0026amp;mut std::fmt::Formatter) -\u0026gt; std::fmt::Result { formatter.write_str(\u0026#34;a command in the format \u0026#39;+:\u0026lt;cmd\u0026gt; \u0026lt;required_key\u0026gt; \u0026lt;optional_value\u0026gt;\u0026#39;\u0026#34;) } fn visit_str\u0026lt;E\u0026gt;(self, cmd_str: \u0026amp;str) -\u0026gt; std::result::Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { let inputs = cmd_str.split_whitespace().collect::\u0026lt;Vec\u0026lt;\u0026amp;str\u0026gt;\u0026gt;(); let len = inputs.len(); if len != 2 \u0026amp;\u0026amp; len != 3 { return Err(de::Error::custom( \u0026#34;invalid command request provided, valid commands are \u0026#39;get\u0026#39;, \u0026#39;set\u0026#39; and \u0026#39;rm\u0026#39;\u0026#34;, )); } let cmd_name = inputs[0]; let get_key = |k: \u0026amp;str| -\u0026gt; String { if k.ends_with(\u0026#34;:\u0026#34;) { return k.get(0..k.len() - 1).unwrap_or(k).to_owned(); } else if k.ends_with(\u0026#34;:\\n\u0026#34;) { return k.get(0..k.len() - 2).unwrap_or(k).to_owned(); } return k.to_owned(); }; let mut cmd_name = inputs[0]; if cmd_name.starts_with(\u0026#34;+:\u0026#34;) { cmd_name = cmd_name.trim().get(2..cmd_name.len()).unwrap_or(cmd_name); } let key = inputs[1]; match cmd_name { \u0026#34;get\u0026#34; =\u0026gt; Ok(Request::Get { key: get_key(key) }), \u0026#34;rm\u0026#34; =\u0026gt; Ok(Request::Rm { key: get_key(key) }), \u0026#34;set\u0026#34; =\u0026gt; { let val = inputs[2]; Ok(Request::Set { key: key.to_owned(), val: get_key(val), }) } _ =\u0026gt; Err(de::Error::custom( \u0026#34;invalid command is provided, valid commands are \u0026#39;get\u0026#39;, \u0026#39;set\u0026#39; and \u0026#39;rm\u0026#39;\u0026#34;, )), } } } The Visitor trait has only one required method: expecting which will be used in error messages. While other methods like visit_str have default implementations, we need to override and implement the methods necessary for our use case. In this case, since we are working with string values, we\u0026rsquo;ll implement visit_str, which will handle parsing the given string.\nFor our example, visit_str will expect strings like get abc:, set key value:, or rm key:. The visit_str method will attempt to convert these strings into the corresponding Request enum variants.\nFinally we can implement our deserializer which will simply call the visitor\u0026rsquo;s visit_str method, as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 pub struct Deserializer\u0026lt;\u0026#39;de\u0026gt; { input: \u0026amp;\u0026#39;de str, } impl\u0026lt;\u0026#39;de, \u0026#39;a\u0026gt; de::Deserializer\u0026lt;\u0026#39;de\u0026gt; for \u0026amp;\u0026#39;a mut Deserializer\u0026lt;\u0026#39;de\u0026gt; { type Error = Error; fn deserialize_str\u0026lt;V\u0026gt;(self, visitor: V) -\u0026gt; std::result::Result\u0026lt;V::Value, Self::Error\u0026gt; where V: de::Visitor\u0026lt;\u0026#39;de\u0026gt;, { self.input = self .input .strip_suffix(\u0026#34;\\r\\n\u0026#34;) .or(self.input.strip_suffix(\u0026#34;\\n\u0026#34;)) .unwrap_or(self.input); visitor.visit_str::\u0026lt;Self::Error\u0026gt;(\u0026amp;self.input) } // Rest of the deserialize_* methods ... // // As Serializer, i do not list them all here as there is a deserialize_* // method for almost all type. // // For more detail about it, please refer to the: // https://docs.rs/serde/1.0.208/serde/trait.Deserializer.html# } This approach keeps the deserialization process straightforward and clean. Now, let\u0026rsquo;s write a simple test scenario to verify our implementation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 fn deserialize\u0026lt;\u0026#39;a, T: de::Deserialize\u0026lt;\u0026#39;a\u0026gt;\u0026gt;(input: \u0026amp;\u0026#39;a str) -\u0026gt; Result\u0026lt;T\u0026gt; { let mut deserializer = Deserializer::from_str(input); let t = T::deserialize(\u0026amp;mut deserializer)?; Ok(t) } #[test] fn test_deserialize_set() { let data = r\u0026#34;set burak 123:\u0026#34;; let expected = Request::Set { key: \u0026#34;burak\u0026#34;.to_string(), val: \u0026#34;123\u0026#34;.to_string(), }; let result: Request = deserialize(data).expect(\u0026#34;failed to deserialize\u0026#34;); } I hope this post helps you get started with your own serde implementations. Since I am also a beginner in Rust, I encourage you to always refer to the official serde documentation as the primary source of truth.\nAlso, the source code is available on GitHub: https://github.com/buraksekili/kvs_protocol/\nIf you notice any mistakes or have feedback, feel free to reach out to me on Twitter, LinkedIn, or GitHub.\nReferences https://serde.rs/ https://owengage.com/writing/2021-08-14-exploring-serdes-data-model-with-a-toy-deserializer/ ","permalink":"https://buraksekili.github.io/articles/rust-serde/","summary":"Working with Custom Data Format in Rust using serde If you need to perform serialization or deserialization in Rust, you’ve most likely used the serde before. I’m currently learning Rust, and I found myself needing similar thing.\nTo get familiar with the Rust ecosystem, I decided to develop a simple key-value store. Initially, the engine for this key-value store was designed to work with JSON objects, as JSON is a widely-used format that’s straightforward to use with web clients.","title":"Working with Custom Data Format in Rust using serde"},{"content":" This guide is a summary of AWS EKS Best practices documentation to help me to skim through some concepts that i usually refer to. For details, please have a look to official EKS docs mentioned on Resources section.\nEKS (Amazon Elastic Kubernetes Service) It manages Kubernetes control-plane on behalf of you, ensures that each cluster has its own control plane. So, as an end-user, you only need to handle your workloads in worker nodes. All control plane related stuff will be handled by AWS.\nControl plane provisions at least two Kubernetes API Servers and three etcd instances across three AZs in the AWS.\nEKS will monitor your control-plane and if it fails or falls down, EKS handles this situations so that you will have control planes at high availability.\nNetworking The control plane is deployed in a VPC managed by AWS. So, this VPC is not visible to customers. And in order to deploy EKS cluster (workloads, nodes etc\u0026hellip;), you need to deploy them into a VPC again which is different than the one managed by AWS.\nSo, this means that you have two VPCs - one is managed by AWS for control plane and the other is managed by customer for data plane (workloads).\nIn that case, you need to configure your VPC configuration carefully. Your VPC needs to connect AWS\u0026rsquo;s VPC where control plane is created in order to talk with API Server. Otherwise - if you VPC cannot communicate with AWS\u0026rsquo;s VPC -, your nodes cannot register Kubernetes control plane which prevents scheduler from scheduling pods to worker nodes.\nYour API Server\u0026rsquo;s endpoint can be public - which is default - or private. As a default option, EKS bootstraps an endpooint for API server. Optionally, you can enable private access to the Kubernetes API server. This means that the communication between the nodes and API server happens within the VPC.\nFor more details, please advise to EKS docs: https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html\nEKS both supports IPv4 and IPv6 but by default, it uses IPv4 and recommends having at least two subnets created in different AZs while creating the cluster.\ncluster subnets: subnets specified during cluster creation.\nVPC Your VPC must have sufficient number of IP addresses so that resources on the cluster can be assigned to an IP address. Your VPC must support DNS hostname and resolution support to allow nodes to register to the cluster. Subnet Two subnets in different AZs - in at least two AZs. Subnets must have at least six IP addresses - ideally at least 16 IP addresses for EKS. Subnets can be public or private but the recommend way is private subnets. If you need an ingress from internet to your Pods, make sure that you have at least 1 public subnet to deploy loadbalancers. In this case, loadbalancers can be deployed into public or private subnets based on your use case but the nodes should be deployed in private subnets if possible. If you want to deploy LB into a subnet, subnet needs specific tags. If the LB needs to be private, add kubernetes.io/role/internal-elb: 1, otherwise kubernetes.io/role/elb: 1. Service of LoadBalancer type To load balance the network traffic at L4, deploy kubernetes Service with type of LoadBalancer. In AWS, EKS provisions ALB (AWS Application Load Balancer) for you.\nMake sure that your VPC and Subnet requirements are met. ALB chooses one subnet from each AZ. If you tag kubernetes.io/cluster/my-cluster=shared public subnet(s), it enables creating Service of LoadBalancer\nSecurity Groups SGs are used to control communication between the control plane and worker nodes. When you provision a cluster, EKS creates a default SG and associates this SG to all nodes. The default rules allow all traffic communication between nodes and all outbound traffic to any destination.\nNode communication The recommended way is having a both private and public subnets with minimum of two public subnets and two private subnets where private subnets are in two AZs.\nYou can provision load balancers in public subnets which forwards traffic to the workloads (Pods) created on private subnets.\nCluster endpoint As mentioned above, EKS provisions the cluster with public-only cluster endpoint. The recommended way is to have public-private mode where Kubernetes API calls within your cluster (from control plane to worker nodes or vice versa) uses private VPC.\nCNI When you provision EKS cluster, Amazon Virtual Private Cloud (VPC) CNI is enabled by default\nLoad Balancing Choose ALB if your workloads runs HTTP/HTTPS. If your workloads run TCP, use NLB.\nFor EKS, there are two targets for the target group of your load balancer; instance and ip.\nTarget group forwards requests to the registered targets such as EC2 instance as per docs\nInstance target This target will be the node IP of the worker node.\nSo, the incoming traffic will be routed to worker node through NodePort, and then the Service of ClusterIP selects a pod to forward the traffic.\nThis process has extra network hops and also sometimes ClusterIP service might select a Pod which runs on different node (different AZ) which will increase the latency.\nIP target Now, the traffic will be directly forwarded to the Pods. So, it significantly reduces the latency as the process does not include hops between NodePort and Service.\nResources https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html https://aws.github.io/aws-eks-best-practices/ ","permalink":"https://buraksekili.github.io/articles/eks/","summary":"This guide is a summary of AWS EKS Best practices documentation to help me to skim through some concepts that i usually refer to. For details, please have a look to official EKS docs mentioned on Resources section.\nEKS (Amazon Elastic Kubernetes Service) It manages Kubernetes control-plane on behalf of you, ensures that each cluster has its own control plane. So, as an end-user, you only need to handle your workloads in worker nodes.","title":"Fundamental EKS requirements"},{"content":"Concurrency Channels Unbuffered channel If channel is unbuffered\n1 ch := make(chan struct{}) sending a data to channel will block the goroutine as the channel is nil.\n1 2 3 4 5 6 7 package main func main() { ch := make(chan struct{}) ch \u0026lt;- struct{}{} } Output of this program is:\n1 2 3 4 5 6 7 $ go run main.go fatal error: all goroutines are asleep - deadlock! goroutine 1 [chan send]: main.main() /Users/buraksekili/projects/concur/main.go:35 +0x30 exit status 2 As reading from non-nil channel blocks goroutine and since there is no other goroutine reading from this channel exists, the program panics.\nSo, to simply put, we have 1 goroutine in this program and it is blocked while sending the channel, all goroutines in this simple application stucked as blocked. Hence, the program fails as all goroutines are asleep - deadlock!\nSo, if we have another goroutine that helps main goroutine by receiving a data from the channel, the program won\u0026rsquo;t be paniced.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan struct{}) go func() { \u0026lt;-ch }() ch \u0026lt;- struct{}{} fmt.Println(\u0026#34;Done\u0026#34;) } Output:\n1 2 $ go run main.go Done First listen channel in another goroutine, then write data to it. In the example above, even the goroutine that receives a data from the channel executes the statement \u0026lt;-ch after main goroutine sends data to channel ch \u0026lt;- struct{}{}, as long as the goroutines are running, the program won\u0026rsquo;t panic.\n1 2 3 4 5 6 7 8 9 10 11 12 func main() { ch := make(chan struct{}) go func() { time.Sleep(3 * time.Second) \u0026lt;-ch }() fmt.Println(\u0026#34;hangs here for 3 secs\u0026#34;) ch \u0026lt;- struct{}{} fmt.Println(\u0026#34;done\u0026#34;) } Main goroutine hangs while sending data to channel for 3 seconds while other goroutine was sleeping. This won\u0026rsquo;t cause deadlock, as sleeping or doing other computations like fetching data from database, waiting a response from HTTP server do not cause goroutine to fall into blocking state from running state.\nBuffered Channel Again, if we have the same example as above, but with buffered channels:\n1 2 3 4 5 6 7 8 9 10 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan struct{}, 1) ch \u0026lt;- struct{}{} fmt.Println(\u0026#34;done\u0026#34;) } Output:\n1 2 $ go run main.go done the program will not panic due to deadlock, as we specify capacity (buffer) for the channel. Main goroutine will write data to the buffer and continues to the execution.\nRegardless of whether the channel is buffered or unbuffered, receiving from a closed channel will NOT block your goroutine.\n1 2 3 4 5 6 7 8 9 10 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan struct{}) close(ch) v, ok := \u0026lt;-ch fmt.Printf(\u0026#34;v: %v, ok: %v\\n\u0026#34;, v, ok) } Output:\n1 2 $ go run main.go v: {}, ok: false Of course, you cannot send any value to closed channel, which will cause panic.\nThe second parameter ok corresponds to boolean value showing that if the value is sent to the channel before closing the channel. Hence, there was no value in the channel before closing it, the ok is false.\n1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan int, 1) ch \u0026lt;- 3 close(ch) v, ok := \u0026lt;-ch fmt.Printf(\u0026#34;v: %v, ok: %v\\n\u0026#34;, v, ok) } Output:\n1 2 $ go run main.go v: 3, ok: true ","permalink":"https://buraksekili.github.io/articles/concurrency/","summary":"Concurrency Channels Unbuffered channel If channel is unbuffered\n1 ch := make(chan struct{}) sending a data to channel will block the goroutine as the channel is nil.\n1 2 3 4 5 6 7 package main func main() { ch := make(chan struct{}) ch \u0026lt;- struct{}{} } Output of this program is:\n1 2 3 4 5 6 7 $ go run main.go fatal error: all goroutines are asleep - deadlock!","title":"Concurrency Notes in Go"},{"content":"Introduction controller-runtime package has become a fundamental tool for most Kubernetes controllers, simplifying the creation of controllers to manage resources within a Kubernetes environment efficiently. Users tend to prefer it over client-go.\nThe increased adoption of projects like Kubebuilder or Operator SDK has facilitated the creation of Kubernetes Operator projects. Users need to implement minimal requirements to start with Kubernetes controllers, thanks to these projects.\nAs a developer working on Kubernetes projects, I inevitably touch code pieces utilizing controller-runtime Whenever I dive into code base, I always learn something new about the underlying mechanism of Kubernetes.\nThrough this blog series, I aim to share my learning regarding controller-runtime consolidating my notes spread across various notebooks.\nThis article will specifically dive into the role of controller-runtime Manager.\nWhat are Controllers and Operators? controller-runtime has emerged as the go-to package for building Kubernetes controllers. However, it is essential to understand what these controllers - or Kubernetes Operators - are.\nIn Kubernetes, controllers observe resources, such as Deployments, in a control loop to ensure the cluster resources conform to the desired state specified in the resource specification (e.g., YAML files) 1.\nOn the other hand, according to Redhat, a Kubernetes Operator is an application-specific controller 2. For instance, the Prometheus Operator manages the lifecycle of a Prometheus instance in the cluster, including managing configurations and updating Kubernetes resources, such as ConfigMaps.\nRoughly both are quite similar. They provide a control loop to ensure the current state meets the desired state.\nThe Architecture of Controllers Since controllers are in charge of meeting the desired state of the resources in Kubernetes, they somehow need to be informed about the changes on the resources and perform certain operations if needed. For this, controllers follow a special architecture to\nobserve the resources, inform any events (updating, deleting, adding) done on the resources, keep a local cache to decrease the load on API Server, keep a work queue to pick up events, run workers to perform reconciliation on resources picked up from work queue. This architecture is clearly pictured in client-go documentation:\nreference: client-go documentation Most end-users typically do not need to interact with the sections outlined in blue in the architecture. The controller-runtime effectively manages these elements. The subsequent section will explain these components in simple terms.\nTo simply put, controllers use\ncache to prevent sending each getter request to API server, workqueue which includes the key of the object that needs to be reconciled, workers to process items reconciliation. Informer Informers watch Kubernetes API server to detect changes in resources that we want to. It keeps a local cache - in-memory cache implementing Store interface - including the objects observed through Kubernetes API. Then controllers and operators use this cache for all getter requests - GET and LIST - to prevent load on Kubernetes API server. Moreover, Informers invoke controllers by sending objects to the controllers (registering Event Handlers).\nInformers leverage certain components like Reflector, Queue and Indexer, as shown in the above diagram.\nReflector According to godocs:\nReflector watches a specified resource and causes all changes to be reflected in the given store.\nThe store is actually a cache - with two options; simple one and FIFO. Reflector pushes objects to Delta Fifo queue.\nBy monitoring the server (Kubernetes API Server), the Reflector maintains a local cache of the resources. Upon any event occurring on the watched resource, implying a new operation on the Kubernetes resource, the Reflector updates the cache (Delta FIFO queue, as illustrated in the diagram). Subsequently, the Informer reads objects from this Delta FIFO queue, indexes them for future retrievals, and dispatches the object to the controller.\nIndexer Indexer saves objects into thread-safe Store by indexing the objects. This approach facilitates efficient querying of objects from the cache.\nCustom indexers, based on specific needs, can be created. For example, a custom indexer can be generated to retrieve all objects based on certain fields, such as Annotations.\nMore details about how Kubernetes indexing works, check Kubernetes Client-Side Indexing.\nManager According to godocs\nmanager is required to create controllers and provides shared dependencies such as clients, caches, schemes, etc. Controllers must be started by calling Manager.Start.\nThe Manager serves as a crucial component for controllers by managing their operations. To put it simply, the manager oversees one or more controllers that watch the resources (e.g., Pods) of interest.\nEach operator requires a Manager to operate, as the Manager controls the controllers, webhooks, metric servers, logs, leader elections, caches, and other components.\nFor all dependencies managed by the Manager, please refer to the Manager interface\nController Dependencies As godocs mentioned, Manager provides shared dependencies such as clients, caches, schemes etc. These dependencies are shared among the controllers managed by the Manager. If you have registered two controllers with the Manager, these controllers will share common resources.\nReconciliation, or the reconcile loop, involves the operators or controllers executing the business logic for the watched resources. For example, a Deployment controller might create a specific number of Pods as specified in the Deployment spec.\nThe Client package exposes functionalities to communicate with the Kubernetes API 3. Controllers, registered with a specific Manager, utilize the same client to interact with the Kubernetes API. The main operations of the client include reading and writing.\nReading operations mostly utilize the cache to access the Kubernetes API, rather than accessing it directly, to reduce the load on the Kubernetes API. In contrast, write operations directly communicate with the Kubernetes API. However, this behavior can be modified so that read requests are directed to the Kubernetes API. Nevertheless, this is generally not recommended unless there is a compelling reason to do so.\nThe cache is also shared across controllers, ensuring optimal performance. Consider a scenario where there are n controllers observing multiple resources in a cluster. If a separate cache is maintained for each controller, n caches will attempt to synchronize with the API Server, increasing the load on API Server. Instead, controller-runtime utilizes a shared cache called NewSharedIndexInformer for all controllers registered within a manager.\nIn the diagram above, two controllers maintain separate caches where both send ListAndWatch requests to API Server. However, controller-runtime utilizes a shared cache, reducing the need for multiple ListAndWatch operations.\nreference: controller-runtime/pkg/cache/internal/informers.go Code Whether you use Kubebuilder, Operator SDK, or controller-runtime directly, operators necessitate a Manager to function. The NewManager from controller-runtime facilitates the creation of a new manager.\n1 2 3 4 5 6 var ( // Refer to godocs for details // .... // NewManager returns a new Manager for creating Controllers. NewManager = manager.New\t) Under the hood, NewManager calls New from the manager package.\n1 func New(config *rest.Config, options Options) (Manager, error) For a simple setup, we can create a new manager as follows\n1 2 3 4 5 6 7 8 9 10 11 12 13 import ( \u0026#34;log\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/manager\u0026#34; ) func main() { mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), manager.Options{}) if err != nil { log.Fatal(err) } } Though this code piece is sufficient to create a Manager, the crucial part involves configuring the Manager using manager.Options{}.\nManager Options manager.Options{} configures various dependencies, such as webhooks, clients, or leader elections under the hood.\nScheme As mentioned in the godocs:\nScheme is the scheme used to resolve runtime.Objects to GroupVersionKinds / Resources.\nSo, scheme helps us to register your objects Go type into GVK. If you are building operators, you will realize following code block in your operator:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; runtimeschema \u0026#34;k8s.io/apimachinery/pkg/runtime/schema\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/scheme\u0026#34; ) var ( myscheme = runtime.NewScheme() // SchemeBuilder is used to add go types to the GroupVersionKind scheme SchemeBuilder = \u0026amp;scheme.Builder{ GroupVersion: runtimeschema.GroupVersion{ Group: \u0026#34;your.group\u0026#34;, Version: \u0026#34;v1alpha1\u0026#34;, }, } ) func init() { // Adds your GVK to the scheme that you provided, which is myscheme in our case. utilruntime.Must(SchemeBuilder.AddToScheme(myscheme)) } The scheme is responsible for registering the Go type declaration of your Kubernetes object into a GVK. This is significant as RESTMapper then translates GVK to GVR, establishing a distinct HTTP path for your Kubernetes resource. Consequently, this empowers the Kubernetes client to know the relevant endpoint for your resource.\nCache I mentioned cache a lot, but it is one of the most crucial piece of operators and controllers, where you can see its effect directly. As mentioned Controller Dependencies section, controller-runtime initializes NewSharedIndexInformer for our controllers under the hood. In order to configure cache, cache.Options{} needs to be utilized. There are again a couple of possible configurations possible but be careful while configuring your cache since it has an impact on performance and resource consumption of your operator.\nI specifically want to emphasize SyncPeriod and DefaultNamespaces\nSyncPeriod triggers reconciliation again for every object in the cache once the duration passes. By default, this is configured as 10 hours or so with some jitter across all controllers. Since running a reconciliation over all objects is quite expensive, be careful while adjusting this configuration.\nDefaultNamespaces configures caching objects in specified namespaces. For instance, to watch objects in prod namespace:\n1 2 3 4 5 manager.Options{ Cache: cache.Options{ DefaultNamespaces: map[string]cache.Config{\u0026#34;prod\u0026#34;: cache.Config{}}, }, } Controller The Controller field, in manager.Options{}, configures essential options for controllers registered to this Manager. These options are set using controller.Options{}.\nNotably, the MaxConcurrentReconciles attribute within this configuration governs the number of concurrent reconciles allowed. As detailed in the Architecture of Controllers section, controllers run workers to execute reconciliation tasks. These workers operate as goroutines. By default, a controller uses only one goroutine, but this can be adjusted using the MaxConcurrentReconciles attribute.\nAfter configuring the Manager\u0026rsquo;s options, the NewManager function generates the controllerManager structure, which implements the Runnable interface.\nDuring the creation of the controllerManager structure, controller-runtime initializes the Cluster to handle all necessary operations to interact with your cluster, including managing clients and caches.\nAll the settings provided within manager.Options{} are transferred to cluster.New() to create the cluster. This process calls the private function newCache(restConfig *rest.Config, opts Options) newCacheFunc, initiating the NewInformer, which uses the type SharedIndexInformer as referenced in the Controller Dependencies section.\nThe next step involves registering controllers to the Manager.\n1 2 3 ctrl.NewControllerManagedBy(mgr). // \u0026#39;mgr\u0026#39; refers to the controller-runtime Manager we\u0026#39;ve set up For(\u0026amp;your.Object{}). // The \u0026#39;For()\u0026#39; function takes the object your controller will reconcile Complete(r) // \u0026#39;Complete\u0026#39; builds the controller and starts watching. I will dive into the detailed explanation of the controller registration process in my future writings to avoid making this post excessively long.\nStarting Manager 1 2 3 4 5 // mgr corresponds to manager.Manager{} if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \u0026#34;problem running manager\u0026#34;) os.Exit(1) } Once the manager starts, all required runnables in the manager will start, in the order of\ninternal HTTP servers; health probes, metrics and profiling if enabled. webhooks, cache, controllers, leader election. For reference, check Start(context.Context) method of controllerManager struct.\nFeel free to suggest improvements on GitHub or through my Twitter\nReferences https://kubernetes.io/docs/concepts/architecture/controller/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/client\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://buraksekili.github.io/articles/controller-runtime-1/","summary":"Introduction controller-runtime package has become a fundamental tool for most Kubernetes controllers, simplifying the creation of controllers to manage resources within a Kubernetes environment efficiently. Users tend to prefer it over client-go.\nThe increased adoption of projects like Kubebuilder or Operator SDK has facilitated the creation of Kubernetes Operator projects. Users need to implement minimal requirements to start with Kubernetes controllers, thanks to these projects.\nAs a developer working on Kubernetes projects, I inevitably touch code pieces utilizing controller-runtime Whenever I dive into code base, I always learn something new about the underlying mechanism of Kubernetes.","title":"Diving into controller-runtime | Manager"}]